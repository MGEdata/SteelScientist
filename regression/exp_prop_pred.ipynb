{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae93c8da-3895-4104-ac62-42d9f0397ccf",
   "metadata": {},
   "source": [
    "## load module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7d4dd3-18e8-480b-b5c9-0d654169a8a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:52:59.577372Z",
     "iopub.status.busy": "2023-10-07T13:52:59.576375Z",
     "iopub.status.idle": "2023-10-07T13:53:10.309693Z",
     "shell.execute_reply": "2023-10-07T13:53:10.309693Z",
     "shell.execute_reply.started": "2023-10-07T13:52:59.577372Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "############### sklearn ###############\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, KFold, cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import Dataset\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "############### pytorch ###############\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "############### sklearn ###############\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, AutoAdapterModel, AdapterTrainer\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers.adapters import BertAdapterModel, AutoAdapterModel, AdapterArguments\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(123)\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    AdamW\n",
    ")\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a09118c-d5c6-4004-8d0b-3746a24ac0dd",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4572a4a4-2c1a-486b-9700-e21991cf05f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:53:10.326647Z",
     "iopub.status.busy": "2023-10-07T13:53:10.326647Z",
     "iopub.status.idle": "2023-10-07T13:53:10.341609Z",
     "shell.execute_reply": "2023-10-07T13:53:10.341609Z",
     "shell.execute_reply.started": "2023-10-07T13:53:10.326647Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_text_seq(df):\n",
    "    text_li=[]\n",
    "    for i in range(df.shape[0]):\n",
    "        li = list(df.iloc[i, 3:8])\n",
    "        # print(li)\n",
    "        text = f\"solution treatment temperature/°C |{li[0]}| solution treatment time/min |{li[1]}| cold rolling/% |{li[2]}| annealing temperature/°C |{li[3]}| annealing time/min|{li[4]}\"\n",
    "        text_li.append(text)   \n",
    "    return text_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f327418-cc0f-4d59-8fd1-7d0d2b1d2e75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:44:14.752868Z",
     "iopub.status.busy": "2023-10-07T13:44:14.752868Z",
     "iopub.status.idle": "2023-10-07T13:44:14.767827Z",
     "shell.execute_reply": "2023-10-07T13:44:14.767827Z",
     "shell.execute_reply.started": "2023-10-07T13:44:14.752868Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_old_data(pred_prop='all', split_ratio=0.75, seed=666):\n",
    "    \"\"\"\n",
    "    Load experiment dataset with different features combinations.\n",
    "    ------------------------------    \n",
    "    prop\n",
    "        ['Tensile_value', 'Yield_value', 'Elongation_value', 'all']\n",
    "    fes\n",
    "        ['com', 'actions', 'text', 'embeddings']\n",
    "    \"\"\"\n",
    "    df = pd.read_excel('./dataset/experiment_data/old_text_data.xlsx')\n",
    "    df.fillna('', inplace=True)\n",
    "    df['Text'] = df['Text'] + ' ' + df['Text_addition']\n",
    "    \n",
    "    # property selection\n",
    "    prop_all = ['Tensile_value', 'Yield_value', 'Elongation_value']\n",
    "    if pred_prop == 'all':\n",
    "        drop_col = []\n",
    "    else:\n",
    "        drop_col = list(set(prop_all) - set([pred_prop]))\n",
    "        \n",
    "    data = df.drop(columns=drop_col+['Text_addition']).copy()\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for _ in range(50):\n",
    "        data = data.sample(frac=1.0, random_state=seed)\n",
    "    \n",
    "    # df_train, df_test = np.split(data.sample(frac=1, random_state=seed, ignore_index=True), [int(split_ratio*len(data))])\n",
    "    # df_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8279f7-6a29-4df0-aaa8-fb2ba6e3bd30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:44:14.769822Z",
     "iopub.status.busy": "2023-10-07T13:44:14.768825Z",
     "iopub.status.idle": "2023-10-07T13:44:15.383004Z",
     "shell.execute_reply": "2023-10-07T13:44:15.383004Z",
     "shell.execute_reply.started": "2023-10-07T13:44:14.769822Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tensile_value</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>784.0</td>\n",
       "      <td>Six experimental interstitially-alloyed Cr-Mn austenitic stainless steels with chemical compositions given in Table 1 were ingot-cast in a laboratory vacuum induction furnace under a protective n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>989.0</td>\n",
       "      <td>The high-nitrogen austenitic stainless steel studied in the present investigation was supplied by M/s Jindal Steel Ltd. (JSL), Hisar, India, as forged steel block of 30 cm × 5 cm × 3.5 cm size. Fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>955.0</td>\n",
       "      <td>As-cast billets were homogenized at 1100 °C for 3 h and subsequently machined into cubic block specimens with a size of 70 mm × 70 mm × 70 mm. Before forging, the specimen was heated to 1100 °C an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1630.2</td>\n",
       "      <td>To perform the thermomechanical treatment, samples were cut into 10*50*100 mm plates. Steel plates were heated at 850 °C for 60 min in a furnace with a heating rate of about 14 °C/min, before quen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1112.0</td>\n",
       "      <td>Experimental slabs were hot rolled and directly quenched using a laboratory hot rolling mill. Based on the austenite grain growth studies described below, 55 mm thick slabs were reheated at 1225 °...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tensile_value  \\\n",
       "142          784.0   \n",
       "861          989.0   \n",
       "558          955.0   \n",
       "191         1630.2   \n",
       "181         1112.0   \n",
       "\n",
       "                                                                                                                                                                                                        Text  \n",
       "142   Six experimental interstitially-alloyed Cr-Mn austenitic stainless steels with chemical compositions given in Table 1 were ingot-cast in a laboratory vacuum induction furnace under a protective n...  \n",
       "861  The high-nitrogen austenitic stainless steel studied in the present investigation was supplied by M/s Jindal Steel Ltd. (JSL), Hisar, India, as forged steel block of 30 cm × 5 cm × 3.5 cm size. Fl...  \n",
       "558  As-cast billets were homogenized at 1100 °C for 3 h and subsequently machined into cubic block specimens with a size of 70 mm × 70 mm × 70 mm. Before forging, the specimen was heated to 1100 °C an...  \n",
       "191  To perform the thermomechanical treatment, samples were cut into 10*50*100 mm plates. Steel plates were heated at 850 °C for 60 min in a furnace with a heating rate of about 14 °C/min, before quen...  \n",
       "181  Experimental slabs were hot rolled and directly quenched using a laboratory hot rolling mill. Based on the austenite grain growth studies described below, 55 mm thick slabs were reheated at 1225 °...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_old_data(pred_prop='Tensile_value', split_ratio=0.75, seed=666)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac97d68-03f3-4640-8549-17c1fc097afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903110b-c7da-49b3-a184-b301aaf204c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55d6000d-9a7a-4152-a022-f99190c5429a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:44:15.384970Z",
     "iopub.status.busy": "2023-10-07T13:44:15.383972Z",
     "iopub.status.idle": "2023-10-07T13:44:15.397935Z",
     "shell.execute_reply": "2023-10-07T13:44:15.397935Z",
     "shell.execute_reply.started": "2023-10-07T13:44:15.384970Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auto_reg(label, data_train, data_test, metric='r2', time_min=1):\n",
    "    predictor = TabularPredictor(label=label, eval_metric=metric).fit(data_train, \n",
    "                                time_limit=time_min*60, presets='best_quality')\n",
    "    # add learderboard\n",
    "    print(predictor.leaderboard(data_test, extra_metrics=['r2'], silent=True))\n",
    "    \n",
    "    # performance on training data\n",
    "    train_performance = predictor.evaluate(data_train, auxiliary_metrics=True)\n",
    "\n",
    "    train_r2, train_rmse = round(train_performance['r2'],4), round(train_performance['root_mean_squared_error'], 4)\n",
    "    y_train = data_train.loc[:, label]\n",
    "    y_train_pred = predictor.predict(data_train)\n",
    "    \n",
    "    # performance on test data\n",
    "    test_performance = predictor.evaluate(data_test, auxiliary_metrics=True)\n",
    "    test_r2, test_rmse = round(test_performance['r2'], 4), round(test_performance['root_mean_squared_error'], 4)\n",
    "    y_test = data_test.loc[:, label]\n",
    "    y_test_pred = predictor.predict(data_test)\n",
    "    \n",
    "    ############## plot #########################\n",
    "    fig = plt.figure(figsize=(10,5), dpi=150)\n",
    "    \n",
    "    # on training data\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(y_train, y_train_pred, s=15, color='#1F4B73')\n",
    "    plt.plot(np.arange(0,int(max(y_test)*1.2)), np.arange(0,int(max(y_test)*1.2)), '-', color='#A2555A')\n",
    "    plt.title(f'Train: $R^2$ = {train_r2}, RMSE={train_rmse}',fontdict={'family':'Times New Roman', 'size': 10})\n",
    "    plt.ylabel('prediction', fontdict={'family':'Times New Roman', 'size': 14})\n",
    "    plt.xlabel('True', fontdict={'family':'Times New Roman', 'size': 14})\n",
    "    plt.xlim(0,int(max(y_test)*1.2))\n",
    "    plt.xlim(0,int(max(y_test)*1.2))\n",
    "    plt.legend(labels=['Train data','Y=X'])\n",
    "    plt.grid()\n",
    "\n",
    "    # on test data\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(y_test, y_test_pred,s=15 , color='#1F4B73')\n",
    "    plt.plot(np.arange(0,int(max(y_test)*1.2)), np.arange(0,int(max(y_test)*1.2)), '-', color='#A2555A')\n",
    "    plt.title(f'Test: $R^2$ = {test_r2}, RMSE={test_rmse}',fontdict={'family':'Times New Roman', 'size': 10})\n",
    "    plt.ylabel('prediction', fontdict={'family':'Times New Roman', 'size': 14})\n",
    "    plt.xlabel('True', fontdict={'family':'Times New Roman', 'size': 14})\n",
    "    plt.xlim(0,int(max(y_test)*1.2))\n",
    "    plt.xlim(0,int(max(y_test)*1.2))\n",
    "    plt.legend(labels=['Test data','Y=X'])\n",
    "    plt.grid()\n",
    "    # plt.savefig(f'./mlp_fig/{name}.png')\n",
    "    return train_performance, test_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba1b474-91ec-4e99-9c3b-1a317bbd51f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:53:32.399049Z",
     "iopub.status.busy": "2023-10-07T13:53:32.399049Z",
     "iopub.status.idle": "2023-10-07T13:53:34.427624Z",
     "shell.execute_reply": "2023-10-07T13:53:34.427624Z",
     "shell.execute_reply.started": "2023-10-07T13:53:32.399049Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./../model_saved/checkpoint-140000 were not used when initializing DebertaV2Model: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# model_name = 'bert-base-uncased'\n",
    "# model_name = 'microsoft/deberta-v3-base'\n",
    "# model_name = 'm3rg-iitd/matscibert'\n",
    "model_name = './../model_saved/checkpoint-140000'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddinngs(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output).detach().cpu().numpy()[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706c7bdf-572f-4e5e-a1d0-da1ed6003a8f",
   "metadata": {},
   "source": [
    "## embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f671d378-5417-4b38-bd76-9a0c9ad8fc8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:53:11.044726Z",
     "iopub.status.busy": "2023-10-07T13:53:11.044726Z",
     "iopub.status.idle": "2023-10-07T13:53:11.065669Z",
     "shell.execute_reply": "2023-10-07T13:53:11.065669Z",
     "shell.execute_reply.started": "2023-10-07T13:53:11.044726Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed_exp_data():\n",
    "\n",
    "    bert_df = pd.read_excel('./dataset/experiment_data/zhangcheng_exp_data.xlsx')\n",
    "    cols_li = ['Tensile_name', 'Material', 'Tensile_unit', 'Yield_name', 'Yield_unit'] + \\\n",
    "        ['Elongation_name', 'Elongation_unit', 'H', 'F', 'Na', 'Mg', 'Cl', 'Ca', 'Zn', 'As', 'Zr', 'Y'] + \\\n",
    "        ['Bi', 'Pb', 'Ta', 'Ce', 'La', 'Sb', 'Zr', 'Fe', 'Sn']\n",
    "\n",
    "    bert_df.drop(columns=list(set(cols_li)&set(bert_df.columns)), inplace=True)\n",
    "    bert_df.reset_index(drop=True, inplace=True)\n",
    "    # print(bert_df.shape)\n",
    "    # print(bert_df.columns[:])\n",
    "    \n",
    "    tqdm.pandas(desc='Progress bar!')\n",
    "    # bert_df['process_emb'] = bert_df['Text'].progress_apply(get_embeddinngs)\n",
    "    bert_df['process_emb'] = bert_df['Text'].apply(get_embeddinngs)\n",
    "\n",
    "    temp_bert_df = pd.DataFrame(pd.Series(bert_df['process_emb'][0])).T\n",
    "    for i in range(1, bert_df.shape[0]):\n",
    "        new_row = pd.DataFrame(pd.Series(bert_df['process_emb'][i])).T\n",
    "        temp_bert_df = pd.concat([temp_bert_df, new_row], ignore_index=True, axis=0)\n",
    "\n",
    "    df = pd.concat([bert_df, temp_bert_df], axis=1)\n",
    "    df.drop(columns=['process_emb'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b8cc15-14a6-4348-9cba-679dd2f70c83",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99a5d7fd-261c-4e57-9167-e5311b3a6b6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T14:20:01.695325Z",
     "iopub.status.busy": "2023-10-07T14:20:01.695325Z",
     "iopub.status.idle": "2023-10-07T14:20:01.715272Z",
     "shell.execute_reply": "2023-10-07T14:20:01.715272Z",
     "shell.execute_reply.started": "2023-10-07T14:20:01.695325Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_exp_data(pred_prop='all', fes=['com', 'actions'], split_ratio=0.75, seed=666):\n",
    "    \"\"\"\n",
    "    Load experiment dataset with different features combinations.\n",
    "    ------------------------------    \n",
    "    prop\n",
    "        ['Tensile_value', 'Yield_value', 'Elongation_value', 'all']\n",
    "    fes\n",
    "        ['com', 'actions', 'text', 'embeddings']\n",
    "    \"\"\"\n",
    "    df = embed_exp_data()\n",
    "    df['Text'] = gen_text_seq(df) \n",
    "    \n",
    "    # property selection\n",
    "    prop_all = ['Tensile_value', 'Yield_value', 'Elongation_value']\n",
    "    if pred_prop == 'all':\n",
    "        drop_col = []\n",
    "    else:\n",
    "        drop_col = list(set(prop_all) - set([pred_prop]))\n",
    "    \n",
    "    # features selection\n",
    "    if 'com' not in fes:\n",
    "        drop_col += list(df.columns)[8:-769]\n",
    "    if 'actions' not in fes:\n",
    "        drop_col += list(df.columns)[3:8]\n",
    "    if 'text' not in fes:\n",
    "        drop_col += ['Text']\n",
    "    if 'embeddings' not in fes:\n",
    "        drop_col += list(df.columns)[-768:]\n",
    "        \n",
    "    data = df.drop(columns=drop_col).copy()\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for _ in range(50):\n",
    "        data = data.sample(frac=1.0, random_state=seed)\n",
    "    \n",
    "    df_train, df_test = np.split(data.sample(frac=1, random_state=seed, ignore_index=True), [int(split_ratio*len(data))])\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb3f20a-6401-4700-9c46-7a2061762cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0afc2eb3-0f78-4757-96d6-e6b1aa24e4da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:53:35.639116Z",
     "iopub.status.busy": "2023-10-07T13:53:35.639116Z",
     "iopub.status.idle": "2023-10-07T13:53:39.356206Z",
     "shell.execute_reply": "2023-10-07T13:53:39.356206Z",
     "shell.execute_reply.started": "2023-10-07T13:53:35.639116Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 769) (26, 769)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elongation_value</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>415</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>459</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "      <th>514</th>\n",
       "      <th>515</th>\n",
       "      <th>516</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "      <th>527</th>\n",
       "      <th>528</th>\n",
       "      <th>529</th>\n",
       "      <th>530</th>\n",
       "      <th>531</th>\n",
       "      <th>532</th>\n",
       "      <th>533</th>\n",
       "      <th>534</th>\n",
       "      <th>535</th>\n",
       "      <th>536</th>\n",
       "      <th>537</th>\n",
       "      <th>538</th>\n",
       "      <th>539</th>\n",
       "      <th>540</th>\n",
       "      <th>541</th>\n",
       "      <th>542</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "      <th>545</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>563</th>\n",
       "      <th>564</th>\n",
       "      <th>565</th>\n",
       "      <th>566</th>\n",
       "      <th>567</th>\n",
       "      <th>568</th>\n",
       "      <th>569</th>\n",
       "      <th>570</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>573</th>\n",
       "      <th>574</th>\n",
       "      <th>575</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>578</th>\n",
       "      <th>579</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "      <th>591</th>\n",
       "      <th>592</th>\n",
       "      <th>593</th>\n",
       "      <th>594</th>\n",
       "      <th>595</th>\n",
       "      <th>596</th>\n",
       "      <th>597</th>\n",
       "      <th>598</th>\n",
       "      <th>599</th>\n",
       "      <th>600</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "      <th>611</th>\n",
       "      <th>612</th>\n",
       "      <th>613</th>\n",
       "      <th>614</th>\n",
       "      <th>615</th>\n",
       "      <th>616</th>\n",
       "      <th>617</th>\n",
       "      <th>618</th>\n",
       "      <th>619</th>\n",
       "      <th>620</th>\n",
       "      <th>621</th>\n",
       "      <th>622</th>\n",
       "      <th>623</th>\n",
       "      <th>624</th>\n",
       "      <th>625</th>\n",
       "      <th>626</th>\n",
       "      <th>627</th>\n",
       "      <th>628</th>\n",
       "      <th>629</th>\n",
       "      <th>630</th>\n",
       "      <th>631</th>\n",
       "      <th>632</th>\n",
       "      <th>633</th>\n",
       "      <th>634</th>\n",
       "      <th>635</th>\n",
       "      <th>636</th>\n",
       "      <th>637</th>\n",
       "      <th>638</th>\n",
       "      <th>639</th>\n",
       "      <th>640</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "      <th>644</th>\n",
       "      <th>645</th>\n",
       "      <th>646</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "      <th>650</th>\n",
       "      <th>651</th>\n",
       "      <th>652</th>\n",
       "      <th>653</th>\n",
       "      <th>654</th>\n",
       "      <th>655</th>\n",
       "      <th>656</th>\n",
       "      <th>657</th>\n",
       "      <th>658</th>\n",
       "      <th>659</th>\n",
       "      <th>660</th>\n",
       "      <th>661</th>\n",
       "      <th>662</th>\n",
       "      <th>663</th>\n",
       "      <th>664</th>\n",
       "      <th>665</th>\n",
       "      <th>666</th>\n",
       "      <th>667</th>\n",
       "      <th>668</th>\n",
       "      <th>669</th>\n",
       "      <th>670</th>\n",
       "      <th>671</th>\n",
       "      <th>672</th>\n",
       "      <th>673</th>\n",
       "      <th>674</th>\n",
       "      <th>675</th>\n",
       "      <th>676</th>\n",
       "      <th>677</th>\n",
       "      <th>678</th>\n",
       "      <th>679</th>\n",
       "      <th>680</th>\n",
       "      <th>681</th>\n",
       "      <th>682</th>\n",
       "      <th>683</th>\n",
       "      <th>684</th>\n",
       "      <th>685</th>\n",
       "      <th>686</th>\n",
       "      <th>687</th>\n",
       "      <th>688</th>\n",
       "      <th>689</th>\n",
       "      <th>690</th>\n",
       "      <th>691</th>\n",
       "      <th>692</th>\n",
       "      <th>693</th>\n",
       "      <th>694</th>\n",
       "      <th>695</th>\n",
       "      <th>696</th>\n",
       "      <th>697</th>\n",
       "      <th>698</th>\n",
       "      <th>699</th>\n",
       "      <th>700</th>\n",
       "      <th>701</th>\n",
       "      <th>702</th>\n",
       "      <th>703</th>\n",
       "      <th>704</th>\n",
       "      <th>705</th>\n",
       "      <th>706</th>\n",
       "      <th>707</th>\n",
       "      <th>708</th>\n",
       "      <th>709</th>\n",
       "      <th>710</th>\n",
       "      <th>711</th>\n",
       "      <th>712</th>\n",
       "      <th>713</th>\n",
       "      <th>714</th>\n",
       "      <th>715</th>\n",
       "      <th>716</th>\n",
       "      <th>717</th>\n",
       "      <th>718</th>\n",
       "      <th>719</th>\n",
       "      <th>720</th>\n",
       "      <th>721</th>\n",
       "      <th>722</th>\n",
       "      <th>723</th>\n",
       "      <th>724</th>\n",
       "      <th>725</th>\n",
       "      <th>726</th>\n",
       "      <th>727</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.73</td>\n",
       "      <td>-0.916031</td>\n",
       "      <td>-0.127834</td>\n",
       "      <td>-0.412525</td>\n",
       "      <td>-1.148250</td>\n",
       "      <td>0.397523</td>\n",
       "      <td>-1.339542</td>\n",
       "      <td>0.413916</td>\n",
       "      <td>-0.331999</td>\n",
       "      <td>-0.839937</td>\n",
       "      <td>-0.017726</td>\n",
       "      <td>-0.237142</td>\n",
       "      <td>0.552550</td>\n",
       "      <td>-0.703550</td>\n",
       "      <td>-0.196383</td>\n",
       "      <td>0.964938</td>\n",
       "      <td>0.763047</td>\n",
       "      <td>0.408478</td>\n",
       "      <td>-1.437083</td>\n",
       "      <td>0.081418</td>\n",
       "      <td>-1.040954</td>\n",
       "      <td>-0.242027</td>\n",
       "      <td>0.237640</td>\n",
       "      <td>-0.416142</td>\n",
       "      <td>-1.184805</td>\n",
       "      <td>0.471824</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-1.178648</td>\n",
       "      <td>-0.339284</td>\n",
       "      <td>0.987294</td>\n",
       "      <td>0.131147</td>\n",
       "      <td>0.404637</td>\n",
       "      <td>-0.291238</td>\n",
       "      <td>-0.534806</td>\n",
       "      <td>-0.136035</td>\n",
       "      <td>-0.561124</td>\n",
       "      <td>-0.430609</td>\n",
       "      <td>-1.161139</td>\n",
       "      <td>0.540728</td>\n",
       "      <td>0.158976</td>\n",
       "      <td>0.127382</td>\n",
       "      <td>-0.515667</td>\n",
       "      <td>-0.194996</td>\n",
       "      <td>-0.083271</td>\n",
       "      <td>0.051068</td>\n",
       "      <td>0.357276</td>\n",
       "      <td>-0.439148</td>\n",
       "      <td>0.012219</td>\n",
       "      <td>0.885475</td>\n",
       "      <td>0.116124</td>\n",
       "      <td>-0.836317</td>\n",
       "      <td>0.382456</td>\n",
       "      <td>-0.985915</td>\n",
       "      <td>-0.827866</td>\n",
       "      <td>0.248264</td>\n",
       "      <td>-0.176879</td>\n",
       "      <td>0.748900</td>\n",
       "      <td>-0.294604</td>\n",
       "      <td>0.388489</td>\n",
       "      <td>0.880481</td>\n",
       "      <td>0.315506</td>\n",
       "      <td>1.552602</td>\n",
       "      <td>-0.475215</td>\n",
       "      <td>0.805777</td>\n",
       "      <td>-0.029898</td>\n",
       "      <td>-1.077119</td>\n",
       "      <td>-0.866457</td>\n",
       "      <td>0.293282</td>\n",
       "      <td>-0.007556</td>\n",
       "      <td>0.541936</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>0.378152</td>\n",
       "      <td>0.142043</td>\n",
       "      <td>0.456180</td>\n",
       "      <td>-0.084118</td>\n",
       "      <td>0.352744</td>\n",
       "      <td>-0.443961</td>\n",
       "      <td>0.123427</td>\n",
       "      <td>-0.109332</td>\n",
       "      <td>-0.707241</td>\n",
       "      <td>0.336127</td>\n",
       "      <td>0.325942</td>\n",
       "      <td>0.590749</td>\n",
       "      <td>1.251289</td>\n",
       "      <td>-1.884276</td>\n",
       "      <td>-0.172657</td>\n",
       "      <td>1.881408</td>\n",
       "      <td>-1.107557</td>\n",
       "      <td>-0.589994</td>\n",
       "      <td>0.271838</td>\n",
       "      <td>-0.179294</td>\n",
       "      <td>-2.616996</td>\n",
       "      <td>-0.034784</td>\n",
       "      <td>0.754296</td>\n",
       "      <td>-0.333799</td>\n",
       "      <td>0.478040</td>\n",
       "      <td>-0.462951</td>\n",
       "      <td>-0.434281</td>\n",
       "      <td>-1.663120</td>\n",
       "      <td>-0.357730</td>\n",
       "      <td>0.856380</td>\n",
       "      <td>-0.111549</td>\n",
       "      <td>0.935525</td>\n",
       "      <td>0.180116</td>\n",
       "      <td>0.325657</td>\n",
       "      <td>0.052699</td>\n",
       "      <td>-0.351309</td>\n",
       "      <td>-0.472753</td>\n",
       "      <td>-1.405189</td>\n",
       "      <td>1.926005</td>\n",
       "      <td>0.610800</td>\n",
       "      <td>-0.034712</td>\n",
       "      <td>0.864820</td>\n",
       "      <td>0.068866</td>\n",
       "      <td>-0.338850</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.259941</td>\n",
       "      <td>1.278410</td>\n",
       "      <td>0.794152</td>\n",
       "      <td>-1.114567</td>\n",
       "      <td>-1.409564</td>\n",
       "      <td>0.410061</td>\n",
       "      <td>-0.541645</td>\n",
       "      <td>-1.011619</td>\n",
       "      <td>0.224822</td>\n",
       "      <td>-0.347563</td>\n",
       "      <td>0.756954</td>\n",
       "      <td>-0.383644</td>\n",
       "      <td>0.156032</td>\n",
       "      <td>-0.375340</td>\n",
       "      <td>1.037328</td>\n",
       "      <td>-0.471992</td>\n",
       "      <td>0.772341</td>\n",
       "      <td>0.096003</td>\n",
       "      <td>-0.950419</td>\n",
       "      <td>-0.046104</td>\n",
       "      <td>-0.171486</td>\n",
       "      <td>-0.393246</td>\n",
       "      <td>-1.251333</td>\n",
       "      <td>-0.877932</td>\n",
       "      <td>0.965971</td>\n",
       "      <td>0.209952</td>\n",
       "      <td>0.803545</td>\n",
       "      <td>-0.279004</td>\n",
       "      <td>0.416956</td>\n",
       "      <td>0.488927</td>\n",
       "      <td>-0.368015</td>\n",
       "      <td>0.556807</td>\n",
       "      <td>-0.499697</td>\n",
       "      <td>-0.001963</td>\n",
       "      <td>0.471412</td>\n",
       "      <td>0.764955</td>\n",
       "      <td>0.172411</td>\n",
       "      <td>-0.077837</td>\n",
       "      <td>0.619746</td>\n",
       "      <td>-0.104510</td>\n",
       "      <td>-0.279822</td>\n",
       "      <td>-0.446809</td>\n",
       "      <td>-1.110106</td>\n",
       "      <td>0.099923</td>\n",
       "      <td>-0.774132</td>\n",
       "      <td>-0.616423</td>\n",
       "      <td>-0.429239</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>-0.701345</td>\n",
       "      <td>0.038776</td>\n",
       "      <td>0.685462</td>\n",
       "      <td>0.514711</td>\n",
       "      <td>-0.315413</td>\n",
       "      <td>0.729795</td>\n",
       "      <td>-0.894981</td>\n",
       "      <td>-0.838126</td>\n",
       "      <td>-1.053222</td>\n",
       "      <td>-0.674418</td>\n",
       "      <td>-0.048791</td>\n",
       "      <td>-0.641334</td>\n",
       "      <td>1.288083</td>\n",
       "      <td>0.290813</td>\n",
       "      <td>-0.590914</td>\n",
       "      <td>0.936593</td>\n",
       "      <td>1.334001</td>\n",
       "      <td>0.530370</td>\n",
       "      <td>0.796496</td>\n",
       "      <td>-0.193131</td>\n",
       "      <td>0.246324</td>\n",
       "      <td>-0.305265</td>\n",
       "      <td>0.067173</td>\n",
       "      <td>0.584022</td>\n",
       "      <td>0.266881</td>\n",
       "      <td>-0.530092</td>\n",
       "      <td>-0.365654</td>\n",
       "      <td>-1.346749</td>\n",
       "      <td>1.413864</td>\n",
       "      <td>0.873827</td>\n",
       "      <td>0.206865</td>\n",
       "      <td>0.781309</td>\n",
       "      <td>0.919980</td>\n",
       "      <td>-0.130631</td>\n",
       "      <td>0.789748</td>\n",
       "      <td>-0.290507</td>\n",
       "      <td>-0.037331</td>\n",
       "      <td>-0.623846</td>\n",
       "      <td>-0.222425</td>\n",
       "      <td>-0.700611</td>\n",
       "      <td>-0.265118</td>\n",
       "      <td>-0.805169</td>\n",
       "      <td>-1.019531</td>\n",
       "      <td>0.276073</td>\n",
       "      <td>1.237971</td>\n",
       "      <td>0.510868</td>\n",
       "      <td>0.639456</td>\n",
       "      <td>-0.772032</td>\n",
       "      <td>-0.498545</td>\n",
       "      <td>-0.070208</td>\n",
       "      <td>0.586046</td>\n",
       "      <td>1.138827</td>\n",
       "      <td>-0.435073</td>\n",
       "      <td>-1.419451</td>\n",
       "      <td>-0.213396</td>\n",
       "      <td>-0.491136</td>\n",
       "      <td>-0.631334</td>\n",
       "      <td>-0.511933</td>\n",
       "      <td>-1.402461</td>\n",
       "      <td>-0.135511</td>\n",
       "      <td>0.057072</td>\n",
       "      <td>-0.531335</td>\n",
       "      <td>0.572108</td>\n",
       "      <td>-0.911924</td>\n",
       "      <td>-0.690377</td>\n",
       "      <td>-0.255268</td>\n",
       "      <td>-0.198094</td>\n",
       "      <td>-0.191315</td>\n",
       "      <td>0.432752</td>\n",
       "      <td>0.301143</td>\n",
       "      <td>-0.751887</td>\n",
       "      <td>-0.945049</td>\n",
       "      <td>-1.429686</td>\n",
       "      <td>-0.049388</td>\n",
       "      <td>0.066865</td>\n",
       "      <td>-0.282562</td>\n",
       "      <td>-0.674942</td>\n",
       "      <td>-1.291429</td>\n",
       "      <td>0.413257</td>\n",
       "      <td>1.328818</td>\n",
       "      <td>-1.339934</td>\n",
       "      <td>0.138815</td>\n",
       "      <td>-0.768350</td>\n",
       "      <td>0.424794</td>\n",
       "      <td>-0.130532</td>\n",
       "      <td>0.110422</td>\n",
       "      <td>-0.818007</td>\n",
       "      <td>-0.111718</td>\n",
       "      <td>-0.222480</td>\n",
       "      <td>0.471001</td>\n",
       "      <td>2.025520</td>\n",
       "      <td>0.220927</td>\n",
       "      <td>0.105805</td>\n",
       "      <td>0.146110</td>\n",
       "      <td>1.269073</td>\n",
       "      <td>-0.140321</td>\n",
       "      <td>0.531963</td>\n",
       "      <td>0.788315</td>\n",
       "      <td>0.576997</td>\n",
       "      <td>-0.545387</td>\n",
       "      <td>0.475949</td>\n",
       "      <td>-0.894918</td>\n",
       "      <td>-0.860079</td>\n",
       "      <td>0.102825</td>\n",
       "      <td>-0.095659</td>\n",
       "      <td>-0.507907</td>\n",
       "      <td>1.222366</td>\n",
       "      <td>0.398551</td>\n",
       "      <td>-1.468970</td>\n",
       "      <td>-0.884805</td>\n",
       "      <td>-0.613770</td>\n",
       "      <td>-1.238994</td>\n",
       "      <td>-0.103361</td>\n",
       "      <td>-0.684106</td>\n",
       "      <td>-0.115385</td>\n",
       "      <td>1.404014</td>\n",
       "      <td>-0.497012</td>\n",
       "      <td>0.441565</td>\n",
       "      <td>-0.594046</td>\n",
       "      <td>-0.747068</td>\n",
       "      <td>-1.084847</td>\n",
       "      <td>0.666250</td>\n",
       "      <td>0.213535</td>\n",
       "      <td>0.080009</td>\n",
       "      <td>1.397216</td>\n",
       "      <td>0.944891</td>\n",
       "      <td>0.390578</td>\n",
       "      <td>-0.060897</td>\n",
       "      <td>0.607301</td>\n",
       "      <td>-0.633116</td>\n",
       "      <td>0.749693</td>\n",
       "      <td>-1.190474</td>\n",
       "      <td>1.135499</td>\n",
       "      <td>0.246069</td>\n",
       "      <td>-1.088464</td>\n",
       "      <td>0.841257</td>\n",
       "      <td>1.666158</td>\n",
       "      <td>-0.033129</td>\n",
       "      <td>0.449991</td>\n",
       "      <td>0.225496</td>\n",
       "      <td>0.843186</td>\n",
       "      <td>0.301087</td>\n",
       "      <td>-0.876974</td>\n",
       "      <td>0.184274</td>\n",
       "      <td>0.272972</td>\n",
       "      <td>0.071353</td>\n",
       "      <td>-0.503152</td>\n",
       "      <td>-1.050257</td>\n",
       "      <td>0.035765</td>\n",
       "      <td>1.039412</td>\n",
       "      <td>-1.039289</td>\n",
       "      <td>-0.416652</td>\n",
       "      <td>-0.862202</td>\n",
       "      <td>-1.245696</td>\n",
       "      <td>0.361685</td>\n",
       "      <td>0.666548</td>\n",
       "      <td>0.697989</td>\n",
       "      <td>-0.266058</td>\n",
       "      <td>-0.210500</td>\n",
       "      <td>-1.571464</td>\n",
       "      <td>0.865381</td>\n",
       "      <td>0.274597</td>\n",
       "      <td>0.124707</td>\n",
       "      <td>-0.676559</td>\n",
       "      <td>-0.667969</td>\n",
       "      <td>0.206986</td>\n",
       "      <td>-0.170810</td>\n",
       "      <td>1.665514</td>\n",
       "      <td>1.224472</td>\n",
       "      <td>0.805410</td>\n",
       "      <td>0.966988</td>\n",
       "      <td>-0.228395</td>\n",
       "      <td>-0.133413</td>\n",
       "      <td>-0.482085</td>\n",
       "      <td>-0.110164</td>\n",
       "      <td>0.434618</td>\n",
       "      <td>-0.308112</td>\n",
       "      <td>-0.498381</td>\n",
       "      <td>0.811527</td>\n",
       "      <td>0.802559</td>\n",
       "      <td>-0.212064</td>\n",
       "      <td>-1.015529</td>\n",
       "      <td>-0.293165</td>\n",
       "      <td>-0.660605</td>\n",
       "      <td>-0.298868</td>\n",
       "      <td>-0.153508</td>\n",
       "      <td>0.022591</td>\n",
       "      <td>-0.103366</td>\n",
       "      <td>-0.438295</td>\n",
       "      <td>0.454937</td>\n",
       "      <td>-0.212763</td>\n",
       "      <td>-0.351217</td>\n",
       "      <td>-0.391846</td>\n",
       "      <td>-0.090670</td>\n",
       "      <td>1.201962</td>\n",
       "      <td>0.087204</td>\n",
       "      <td>0.361371</td>\n",
       "      <td>-0.241886</td>\n",
       "      <td>-0.849253</td>\n",
       "      <td>0.305798</td>\n",
       "      <td>0.937777</td>\n",
       "      <td>1.382287</td>\n",
       "      <td>1.204304</td>\n",
       "      <td>-0.419141</td>\n",
       "      <td>-0.563582</td>\n",
       "      <td>0.505697</td>\n",
       "      <td>0.208627</td>\n",
       "      <td>0.761790</td>\n",
       "      <td>-0.002871</td>\n",
       "      <td>0.303617</td>\n",
       "      <td>-0.697072</td>\n",
       "      <td>-0.131776</td>\n",
       "      <td>-0.536411</td>\n",
       "      <td>-0.844009</td>\n",
       "      <td>-0.792295</td>\n",
       "      <td>-0.247314</td>\n",
       "      <td>-0.595349</td>\n",
       "      <td>0.590092</td>\n",
       "      <td>-1.869353</td>\n",
       "      <td>0.188370</td>\n",
       "      <td>-0.149342</td>\n",
       "      <td>-0.424916</td>\n",
       "      <td>0.260395</td>\n",
       "      <td>-0.403129</td>\n",
       "      <td>-0.619658</td>\n",
       "      <td>-0.722321</td>\n",
       "      <td>-0.538546</td>\n",
       "      <td>-1.297676</td>\n",
       "      <td>0.115491</td>\n",
       "      <td>-0.066180</td>\n",
       "      <td>0.236784</td>\n",
       "      <td>0.415496</td>\n",
       "      <td>-0.458166</td>\n",
       "      <td>-0.462058</td>\n",
       "      <td>1.221647</td>\n",
       "      <td>0.445055</td>\n",
       "      <td>-0.227902</td>\n",
       "      <td>-1.135463</td>\n",
       "      <td>0.163360</td>\n",
       "      <td>-0.193833</td>\n",
       "      <td>-1.298360</td>\n",
       "      <td>-0.535846</td>\n",
       "      <td>-0.838075</td>\n",
       "      <td>0.104269</td>\n",
       "      <td>0.359346</td>\n",
       "      <td>0.459879</td>\n",
       "      <td>-0.845323</td>\n",
       "      <td>-0.473110</td>\n",
       "      <td>-0.738906</td>\n",
       "      <td>0.098208</td>\n",
       "      <td>0.634441</td>\n",
       "      <td>0.979829</td>\n",
       "      <td>-0.759346</td>\n",
       "      <td>-0.059345</td>\n",
       "      <td>0.817776</td>\n",
       "      <td>-0.983007</td>\n",
       "      <td>0.203411</td>\n",
       "      <td>-2.159915</td>\n",
       "      <td>0.082703</td>\n",
       "      <td>0.267683</td>\n",
       "      <td>-0.920031</td>\n",
       "      <td>1.404773</td>\n",
       "      <td>0.251037</td>\n",
       "      <td>0.232732</td>\n",
       "      <td>-0.032009</td>\n",
       "      <td>-0.220980</td>\n",
       "      <td>-0.446403</td>\n",
       "      <td>-0.452745</td>\n",
       "      <td>-0.820232</td>\n",
       "      <td>1.125852</td>\n",
       "      <td>-0.381940</td>\n",
       "      <td>-0.465709</td>\n",
       "      <td>1.790494</td>\n",
       "      <td>-0.339537</td>\n",
       "      <td>-0.053865</td>\n",
       "      <td>-0.511411</td>\n",
       "      <td>0.585588</td>\n",
       "      <td>0.433298</td>\n",
       "      <td>1.116106</td>\n",
       "      <td>-0.550744</td>\n",
       "      <td>1.050105</td>\n",
       "      <td>0.518447</td>\n",
       "      <td>0.202749</td>\n",
       "      <td>-0.542480</td>\n",
       "      <td>-0.088704</td>\n",
       "      <td>0.131593</td>\n",
       "      <td>-1.613492</td>\n",
       "      <td>-1.141693</td>\n",
       "      <td>-0.732665</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.386457</td>\n",
       "      <td>0.174765</td>\n",
       "      <td>0.514060</td>\n",
       "      <td>0.166606</td>\n",
       "      <td>-0.521211</td>\n",
       "      <td>0.464471</td>\n",
       "      <td>-0.112905</td>\n",
       "      <td>0.905570</td>\n",
       "      <td>0.160346</td>\n",
       "      <td>-1.562244</td>\n",
       "      <td>0.467454</td>\n",
       "      <td>0.778058</td>\n",
       "      <td>-0.327656</td>\n",
       "      <td>-0.583430</td>\n",
       "      <td>0.372757</td>\n",
       "      <td>-0.825826</td>\n",
       "      <td>-0.913649</td>\n",
       "      <td>-1.754187</td>\n",
       "      <td>1.064206</td>\n",
       "      <td>-0.191573</td>\n",
       "      <td>0.070641</td>\n",
       "      <td>0.507003</td>\n",
       "      <td>-0.309522</td>\n",
       "      <td>0.488213</td>\n",
       "      <td>-0.121472</td>\n",
       "      <td>-1.256016</td>\n",
       "      <td>0.158666</td>\n",
       "      <td>-0.351518</td>\n",
       "      <td>0.449145</td>\n",
       "      <td>-0.167377</td>\n",
       "      <td>-0.780235</td>\n",
       "      <td>0.966628</td>\n",
       "      <td>0.080630</td>\n",
       "      <td>0.068091</td>\n",
       "      <td>-0.685602</td>\n",
       "      <td>0.208218</td>\n",
       "      <td>-0.048632</td>\n",
       "      <td>0.485111</td>\n",
       "      <td>-0.168559</td>\n",
       "      <td>-0.784368</td>\n",
       "      <td>-0.237996</td>\n",
       "      <td>-0.587028</td>\n",
       "      <td>-0.269412</td>\n",
       "      <td>0.361229</td>\n",
       "      <td>-0.062331</td>\n",
       "      <td>-0.342411</td>\n",
       "      <td>-0.683894</td>\n",
       "      <td>-0.093147</td>\n",
       "      <td>-0.290296</td>\n",
       "      <td>-0.621593</td>\n",
       "      <td>0.017103</td>\n",
       "      <td>0.344595</td>\n",
       "      <td>-0.449510</td>\n",
       "      <td>0.455677</td>\n",
       "      <td>1.485255</td>\n",
       "      <td>-1.178215</td>\n",
       "      <td>0.439544</td>\n",
       "      <td>-0.130356</td>\n",
       "      <td>0.240152</td>\n",
       "      <td>1.002190</td>\n",
       "      <td>0.385913</td>\n",
       "      <td>0.449242</td>\n",
       "      <td>-0.988080</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>-0.263088</td>\n",
       "      <td>-0.772724</td>\n",
       "      <td>1.516419</td>\n",
       "      <td>0.534272</td>\n",
       "      <td>-0.034968</td>\n",
       "      <td>0.282683</td>\n",
       "      <td>0.308866</td>\n",
       "      <td>0.257303</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.922770</td>\n",
       "      <td>13.691738</td>\n",
       "      <td>0.192573</td>\n",
       "      <td>0.510660</td>\n",
       "      <td>-0.188680</td>\n",
       "      <td>0.650560</td>\n",
       "      <td>-0.250861</td>\n",
       "      <td>-0.452052</td>\n",
       "      <td>-0.228531</td>\n",
       "      <td>-0.320912</td>\n",
       "      <td>0.281924</td>\n",
       "      <td>-0.671686</td>\n",
       "      <td>0.128511</td>\n",
       "      <td>-0.248603</td>\n",
       "      <td>0.883401</td>\n",
       "      <td>-0.509352</td>\n",
       "      <td>-0.723377</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.965522</td>\n",
       "      <td>0.064517</td>\n",
       "      <td>-1.063517</td>\n",
       "      <td>0.328387</td>\n",
       "      <td>-0.436082</td>\n",
       "      <td>0.095997</td>\n",
       "      <td>-0.473636</td>\n",
       "      <td>-0.651135</td>\n",
       "      <td>-0.830119</td>\n",
       "      <td>-0.580499</td>\n",
       "      <td>-0.267841</td>\n",
       "      <td>-0.452461</td>\n",
       "      <td>0.503747</td>\n",
       "      <td>0.786223</td>\n",
       "      <td>0.420469</td>\n",
       "      <td>-0.973155</td>\n",
       "      <td>0.459810</td>\n",
       "      <td>-0.148301</td>\n",
       "      <td>0.152842</td>\n",
       "      <td>-0.173274</td>\n",
       "      <td>-0.254788</td>\n",
       "      <td>-0.289856</td>\n",
       "      <td>-0.108738</td>\n",
       "      <td>-0.556000</td>\n",
       "      <td>0.392114</td>\n",
       "      <td>-0.837225</td>\n",
       "      <td>0.068129</td>\n",
       "      <td>-1.171508</td>\n",
       "      <td>0.110569</td>\n",
       "      <td>-0.041277</td>\n",
       "      <td>0.116804</td>\n",
       "      <td>-0.248698</td>\n",
       "      <td>1.166849</td>\n",
       "      <td>-1.129025</td>\n",
       "      <td>-0.253690</td>\n",
       "      <td>-0.017193</td>\n",
       "      <td>0.800823</td>\n",
       "      <td>1.375973</td>\n",
       "      <td>-0.047102</td>\n",
       "      <td>-1.018535</td>\n",
       "      <td>0.102864</td>\n",
       "      <td>0.049689</td>\n",
       "      <td>-0.237023</td>\n",
       "      <td>0.519906</td>\n",
       "      <td>-0.185900</td>\n",
       "      <td>0.519788</td>\n",
       "      <td>0.215782</td>\n",
       "      <td>-0.430854</td>\n",
       "      <td>-0.915275</td>\n",
       "      <td>0.838968</td>\n",
       "      <td>0.814252</td>\n",
       "      <td>-1.847867</td>\n",
       "      <td>-0.664324</td>\n",
       "      <td>0.348469</td>\n",
       "      <td>1.322639</td>\n",
       "      <td>1.097698</td>\n",
       "      <td>0.237241</td>\n",
       "      <td>-0.224808</td>\n",
       "      <td>-0.041211</td>\n",
       "      <td>-0.058312</td>\n",
       "      <td>0.027385</td>\n",
       "      <td>1.010817</td>\n",
       "      <td>0.910949</td>\n",
       "      <td>0.413210</td>\n",
       "      <td>-0.312327</td>\n",
       "      <td>-1.012775</td>\n",
       "      <td>0.550531</td>\n",
       "      <td>-0.248307</td>\n",
       "      <td>0.016721</td>\n",
       "      <td>-1.514807</td>\n",
       "      <td>0.496081</td>\n",
       "      <td>0.372792</td>\n",
       "      <td>1.370030</td>\n",
       "      <td>0.141930</td>\n",
       "      <td>-0.329672</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.076919</td>\n",
       "      <td>0.799148</td>\n",
       "      <td>-0.326141</td>\n",
       "      <td>-0.362494</td>\n",
       "      <td>-1.473805</td>\n",
       "      <td>0.478124</td>\n",
       "      <td>0.986631</td>\n",
       "      <td>0.054843</td>\n",
       "      <td>0.782568</td>\n",
       "      <td>0.209285</td>\n",
       "      <td>0.091113</td>\n",
       "      <td>-0.189220</td>\n",
       "      <td>-0.718675</td>\n",
       "      <td>0.601441</td>\n",
       "      <td>0.282407</td>\n",
       "      <td>-0.210989</td>\n",
       "      <td>0.434217</td>\n",
       "      <td>-0.159499</td>\n",
       "      <td>1.016381</td>\n",
       "      <td>0.122543</td>\n",
       "      <td>0.843650</td>\n",
       "      <td>0.714568</td>\n",
       "      <td>-0.292461</td>\n",
       "      <td>0.375786</td>\n",
       "      <td>-0.017425</td>\n",
       "      <td>0.312415</td>\n",
       "      <td>0.117133</td>\n",
       "      <td>0.218766</td>\n",
       "      <td>-0.157159</td>\n",
       "      <td>-0.020092</td>\n",
       "      <td>0.248823</td>\n",
       "      <td>0.275047</td>\n",
       "      <td>-0.059181</td>\n",
       "      <td>0.095104</td>\n",
       "      <td>-0.751459</td>\n",
       "      <td>-1.037601</td>\n",
       "      <td>0.293187</td>\n",
       "      <td>-0.890594</td>\n",
       "      <td>0.206552</td>\n",
       "      <td>1.019548</td>\n",
       "      <td>-0.293066</td>\n",
       "      <td>-0.053301</td>\n",
       "      <td>-0.471833</td>\n",
       "      <td>-0.515374</td>\n",
       "      <td>0.820705</td>\n",
       "      <td>0.508655</td>\n",
       "      <td>-0.206545</td>\n",
       "      <td>-0.616110</td>\n",
       "      <td>0.389341</td>\n",
       "      <td>-1.095838</td>\n",
       "      <td>-0.964378</td>\n",
       "      <td>-0.095803</td>\n",
       "      <td>1.038210</td>\n",
       "      <td>-0.507393</td>\n",
       "      <td>-0.056993</td>\n",
       "      <td>0.850198</td>\n",
       "      <td>-1.490931</td>\n",
       "      <td>2.412739</td>\n",
       "      <td>-0.268311</td>\n",
       "      <td>-0.456288</td>\n",
       "      <td>0.394387</td>\n",
       "      <td>-0.013602</td>\n",
       "      <td>-1.086184</td>\n",
       "      <td>0.230685</td>\n",
       "      <td>-0.522678</td>\n",
       "      <td>0.920389</td>\n",
       "      <td>-1.407173</td>\n",
       "      <td>-0.286586</td>\n",
       "      <td>1.165661</td>\n",
       "      <td>1.206922</td>\n",
       "      <td>0.380767</td>\n",
       "      <td>0.468275</td>\n",
       "      <td>0.375186</td>\n",
       "      <td>0.571283</td>\n",
       "      <td>0.216194</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>-0.155045</td>\n",
       "      <td>-0.092205</td>\n",
       "      <td>0.311307</td>\n",
       "      <td>-0.081987</td>\n",
       "      <td>-0.547525</td>\n",
       "      <td>0.480314</td>\n",
       "      <td>-1.272792</td>\n",
       "      <td>0.368472</td>\n",
       "      <td>-0.351340</td>\n",
       "      <td>0.470269</td>\n",
       "      <td>1.709743</td>\n",
       "      <td>0.781073</td>\n",
       "      <td>0.444272</td>\n",
       "      <td>0.081989</td>\n",
       "      <td>-1.170666</td>\n",
       "      <td>0.959817</td>\n",
       "      <td>1.008016</td>\n",
       "      <td>0.708834</td>\n",
       "      <td>0.809033</td>\n",
       "      <td>-0.359720</td>\n",
       "      <td>0.237015</td>\n",
       "      <td>-0.831460</td>\n",
       "      <td>0.810471</td>\n",
       "      <td>1.055264</td>\n",
       "      <td>0.407583</td>\n",
       "      <td>-0.347130</td>\n",
       "      <td>-0.879955</td>\n",
       "      <td>0.054611</td>\n",
       "      <td>-1.294139</td>\n",
       "      <td>-0.795184</td>\n",
       "      <td>0.129581</td>\n",
       "      <td>-1.226594</td>\n",
       "      <td>-0.223163</td>\n",
       "      <td>0.609400</td>\n",
       "      <td>-0.022221</td>\n",
       "      <td>-0.733969</td>\n",
       "      <td>-0.034820</td>\n",
       "      <td>0.313441</td>\n",
       "      <td>0.183914</td>\n",
       "      <td>-0.549012</td>\n",
       "      <td>-1.346349</td>\n",
       "      <td>0.704137</td>\n",
       "      <td>0.329678</td>\n",
       "      <td>1.018111</td>\n",
       "      <td>0.340090</td>\n",
       "      <td>0.799143</td>\n",
       "      <td>-0.126810</td>\n",
       "      <td>-0.326063</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>-0.198835</td>\n",
       "      <td>0.311579</td>\n",
       "      <td>-0.047607</td>\n",
       "      <td>-0.019473</td>\n",
       "      <td>-0.386535</td>\n",
       "      <td>-1.092652</td>\n",
       "      <td>-0.932427</td>\n",
       "      <td>-0.356115</td>\n",
       "      <td>-0.496315</td>\n",
       "      <td>0.951529</td>\n",
       "      <td>1.134090</td>\n",
       "      <td>-0.301524</td>\n",
       "      <td>-0.653371</td>\n",
       "      <td>1.149359</td>\n",
       "      <td>0.122159</td>\n",
       "      <td>0.432706</td>\n",
       "      <td>-1.536304</td>\n",
       "      <td>1.144165</td>\n",
       "      <td>0.402096</td>\n",
       "      <td>0.735742</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>0.615236</td>\n",
       "      <td>0.555950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.58</td>\n",
       "      <td>-0.769254</td>\n",
       "      <td>-0.110520</td>\n",
       "      <td>-0.435552</td>\n",
       "      <td>-1.213306</td>\n",
       "      <td>0.333992</td>\n",
       "      <td>-1.318727</td>\n",
       "      <td>0.145431</td>\n",
       "      <td>-0.358220</td>\n",
       "      <td>-0.788955</td>\n",
       "      <td>0.019995</td>\n",
       "      <td>-0.265530</td>\n",
       "      <td>0.570121</td>\n",
       "      <td>-0.785721</td>\n",
       "      <td>-0.091056</td>\n",
       "      <td>1.174088</td>\n",
       "      <td>0.734770</td>\n",
       "      <td>0.395986</td>\n",
       "      <td>-1.373323</td>\n",
       "      <td>0.114723</td>\n",
       "      <td>-1.023738</td>\n",
       "      <td>-0.200627</td>\n",
       "      <td>0.218253</td>\n",
       "      <td>-0.401422</td>\n",
       "      <td>-1.050738</td>\n",
       "      <td>0.424351</td>\n",
       "      <td>0.935047</td>\n",
       "      <td>-1.177028</td>\n",
       "      <td>-0.371224</td>\n",
       "      <td>0.908881</td>\n",
       "      <td>0.170098</td>\n",
       "      <td>0.186938</td>\n",
       "      <td>-0.317118</td>\n",
       "      <td>-0.452783</td>\n",
       "      <td>-0.103247</td>\n",
       "      <td>-0.510493</td>\n",
       "      <td>-0.389673</td>\n",
       "      <td>-1.214257</td>\n",
       "      <td>0.490864</td>\n",
       "      <td>0.273634</td>\n",
       "      <td>0.087987</td>\n",
       "      <td>-0.438402</td>\n",
       "      <td>-0.125063</td>\n",
       "      <td>0.043757</td>\n",
       "      <td>-0.132434</td>\n",
       "      <td>0.417061</td>\n",
       "      <td>-0.381706</td>\n",
       "      <td>-0.023056</td>\n",
       "      <td>0.911881</td>\n",
       "      <td>0.028548</td>\n",
       "      <td>-0.842582</td>\n",
       "      <td>0.326267</td>\n",
       "      <td>-0.870367</td>\n",
       "      <td>-0.874450</td>\n",
       "      <td>0.291878</td>\n",
       "      <td>-0.271213</td>\n",
       "      <td>0.588776</td>\n",
       "      <td>-0.399369</td>\n",
       "      <td>0.460631</td>\n",
       "      <td>0.911604</td>\n",
       "      <td>0.279139</td>\n",
       "      <td>1.530984</td>\n",
       "      <td>-0.494210</td>\n",
       "      <td>0.867013</td>\n",
       "      <td>-0.091208</td>\n",
       "      <td>-1.103582</td>\n",
       "      <td>-0.940434</td>\n",
       "      <td>0.380958</td>\n",
       "      <td>0.080520</td>\n",
       "      <td>0.490296</td>\n",
       "      <td>-0.088894</td>\n",
       "      <td>0.577815</td>\n",
       "      <td>0.054718</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>-0.104318</td>\n",
       "      <td>0.331916</td>\n",
       "      <td>-0.399748</td>\n",
       "      <td>0.254990</td>\n",
       "      <td>-0.105435</td>\n",
       "      <td>-0.622705</td>\n",
       "      <td>0.362436</td>\n",
       "      <td>0.303355</td>\n",
       "      <td>0.485840</td>\n",
       "      <td>1.052350</td>\n",
       "      <td>-1.925727</td>\n",
       "      <td>-0.018693</td>\n",
       "      <td>1.878029</td>\n",
       "      <td>-1.045059</td>\n",
       "      <td>-0.622814</td>\n",
       "      <td>0.314003</td>\n",
       "      <td>-0.157367</td>\n",
       "      <td>-2.626469</td>\n",
       "      <td>-0.014078</td>\n",
       "      <td>0.751633</td>\n",
       "      <td>-0.309571</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>-0.659410</td>\n",
       "      <td>-0.434214</td>\n",
       "      <td>-1.617741</td>\n",
       "      <td>-0.295857</td>\n",
       "      <td>0.893189</td>\n",
       "      <td>-0.029646</td>\n",
       "      <td>0.856420</td>\n",
       "      <td>0.063499</td>\n",
       "      <td>0.351936</td>\n",
       "      <td>0.032566</td>\n",
       "      <td>-0.247557</td>\n",
       "      <td>-0.478003</td>\n",
       "      <td>-1.229001</td>\n",
       "      <td>1.917317</td>\n",
       "      <td>0.598785</td>\n",
       "      <td>-0.094997</td>\n",
       "      <td>0.943426</td>\n",
       "      <td>-0.009482</td>\n",
       "      <td>-0.250427</td>\n",
       "      <td>0.553012</td>\n",
       "      <td>0.099468</td>\n",
       "      <td>1.290062</td>\n",
       "      <td>0.771183</td>\n",
       "      <td>-0.946937</td>\n",
       "      <td>-1.310587</td>\n",
       "      <td>0.289362</td>\n",
       "      <td>-0.556137</td>\n",
       "      <td>-0.916540</td>\n",
       "      <td>0.178835</td>\n",
       "      <td>-0.241169</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>-0.351088</td>\n",
       "      <td>0.207845</td>\n",
       "      <td>-0.331350</td>\n",
       "      <td>1.061864</td>\n",
       "      <td>-0.432438</td>\n",
       "      <td>0.614693</td>\n",
       "      <td>0.166565</td>\n",
       "      <td>-0.969748</td>\n",
       "      <td>-0.090778</td>\n",
       "      <td>-0.246744</td>\n",
       "      <td>-0.317533</td>\n",
       "      <td>-1.176315</td>\n",
       "      <td>-0.874115</td>\n",
       "      <td>1.039821</td>\n",
       "      <td>0.197554</td>\n",
       "      <td>0.923907</td>\n",
       "      <td>-0.375042</td>\n",
       "      <td>0.438759</td>\n",
       "      <td>0.306860</td>\n",
       "      <td>-0.358336</td>\n",
       "      <td>0.469697</td>\n",
       "      <td>-0.664448</td>\n",
       "      <td>-0.017813</td>\n",
       "      <td>0.663424</td>\n",
       "      <td>0.849352</td>\n",
       "      <td>0.285440</td>\n",
       "      <td>-0.255188</td>\n",
       "      <td>0.544138</td>\n",
       "      <td>-0.138618</td>\n",
       "      <td>-0.428508</td>\n",
       "      <td>-0.296978</td>\n",
       "      <td>-1.145255</td>\n",
       "      <td>0.132171</td>\n",
       "      <td>-0.771986</td>\n",
       "      <td>-0.653468</td>\n",
       "      <td>-0.304016</td>\n",
       "      <td>1.058321</td>\n",
       "      <td>-0.700630</td>\n",
       "      <td>-0.116372</td>\n",
       "      <td>0.740907</td>\n",
       "      <td>0.435352</td>\n",
       "      <td>-0.259140</td>\n",
       "      <td>0.864672</td>\n",
       "      <td>-0.906711</td>\n",
       "      <td>-1.065525</td>\n",
       "      <td>-0.916127</td>\n",
       "      <td>-0.655933</td>\n",
       "      <td>0.122071</td>\n",
       "      <td>-0.639361</td>\n",
       "      <td>1.283238</td>\n",
       "      <td>0.300518</td>\n",
       "      <td>-0.726053</td>\n",
       "      <td>0.953515</td>\n",
       "      <td>1.213310</td>\n",
       "      <td>0.491617</td>\n",
       "      <td>0.754326</td>\n",
       "      <td>-0.140990</td>\n",
       "      <td>0.369727</td>\n",
       "      <td>-0.336232</td>\n",
       "      <td>0.226733</td>\n",
       "      <td>0.576442</td>\n",
       "      <td>0.446121</td>\n",
       "      <td>-0.500488</td>\n",
       "      <td>-0.196406</td>\n",
       "      <td>-1.352761</td>\n",
       "      <td>1.316477</td>\n",
       "      <td>0.800528</td>\n",
       "      <td>0.305253</td>\n",
       "      <td>0.788092</td>\n",
       "      <td>0.795825</td>\n",
       "      <td>-0.009118</td>\n",
       "      <td>0.905062</td>\n",
       "      <td>-0.285423</td>\n",
       "      <td>0.042176</td>\n",
       "      <td>-0.684889</td>\n",
       "      <td>-0.174024</td>\n",
       "      <td>-0.669468</td>\n",
       "      <td>-0.252664</td>\n",
       "      <td>-0.754569</td>\n",
       "      <td>-0.917132</td>\n",
       "      <td>0.291402</td>\n",
       "      <td>1.193981</td>\n",
       "      <td>0.584014</td>\n",
       "      <td>0.585057</td>\n",
       "      <td>-0.867754</td>\n",
       "      <td>-0.621958</td>\n",
       "      <td>-0.114463</td>\n",
       "      <td>0.635479</td>\n",
       "      <td>1.200061</td>\n",
       "      <td>-0.331071</td>\n",
       "      <td>-1.387399</td>\n",
       "      <td>-0.272733</td>\n",
       "      <td>-0.406333</td>\n",
       "      <td>-0.732201</td>\n",
       "      <td>-0.498863</td>\n",
       "      <td>-1.469920</td>\n",
       "      <td>-0.141468</td>\n",
       "      <td>0.065462</td>\n",
       "      <td>-0.470045</td>\n",
       "      <td>0.564274</td>\n",
       "      <td>-0.897157</td>\n",
       "      <td>-0.695762</td>\n",
       "      <td>-0.202772</td>\n",
       "      <td>-0.032059</td>\n",
       "      <td>0.109280</td>\n",
       "      <td>0.381726</td>\n",
       "      <td>0.328952</td>\n",
       "      <td>-0.673152</td>\n",
       "      <td>-0.922260</td>\n",
       "      <td>-1.523873</td>\n",
       "      <td>-0.087670</td>\n",
       "      <td>-0.092140</td>\n",
       "      <td>-0.234189</td>\n",
       "      <td>-0.774250</td>\n",
       "      <td>-1.304067</td>\n",
       "      <td>0.548550</td>\n",
       "      <td>1.151371</td>\n",
       "      <td>-1.343521</td>\n",
       "      <td>0.191375</td>\n",
       "      <td>-0.741978</td>\n",
       "      <td>0.558551</td>\n",
       "      <td>-0.080042</td>\n",
       "      <td>0.079785</td>\n",
       "      <td>-0.840949</td>\n",
       "      <td>0.012454</td>\n",
       "      <td>-0.201479</td>\n",
       "      <td>0.459524</td>\n",
       "      <td>2.159117</td>\n",
       "      <td>0.284523</td>\n",
       "      <td>0.164675</td>\n",
       "      <td>0.126171</td>\n",
       "      <td>1.270731</td>\n",
       "      <td>-0.168872</td>\n",
       "      <td>0.522522</td>\n",
       "      <td>0.733846</td>\n",
       "      <td>0.569694</td>\n",
       "      <td>-0.520281</td>\n",
       "      <td>0.377859</td>\n",
       "      <td>-0.783319</td>\n",
       "      <td>-0.931253</td>\n",
       "      <td>0.271501</td>\n",
       "      <td>-0.197730</td>\n",
       "      <td>-0.680632</td>\n",
       "      <td>1.172317</td>\n",
       "      <td>0.487209</td>\n",
       "      <td>-1.462741</td>\n",
       "      <td>-0.878852</td>\n",
       "      <td>-0.638744</td>\n",
       "      <td>-1.108701</td>\n",
       "      <td>-0.037423</td>\n",
       "      <td>-0.741607</td>\n",
       "      <td>-0.064968</td>\n",
       "      <td>1.391426</td>\n",
       "      <td>-0.549247</td>\n",
       "      <td>0.262075</td>\n",
       "      <td>-0.550730</td>\n",
       "      <td>-0.728045</td>\n",
       "      <td>-1.035685</td>\n",
       "      <td>0.662205</td>\n",
       "      <td>0.071736</td>\n",
       "      <td>0.033617</td>\n",
       "      <td>1.220232</td>\n",
       "      <td>0.923134</td>\n",
       "      <td>0.308076</td>\n",
       "      <td>-0.042995</td>\n",
       "      <td>0.615008</td>\n",
       "      <td>-0.607901</td>\n",
       "      <td>0.795418</td>\n",
       "      <td>-1.134020</td>\n",
       "      <td>1.164500</td>\n",
       "      <td>0.084182</td>\n",
       "      <td>-1.144427</td>\n",
       "      <td>0.897849</td>\n",
       "      <td>1.755242</td>\n",
       "      <td>-0.061617</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.129344</td>\n",
       "      <td>0.754704</td>\n",
       "      <td>0.310985</td>\n",
       "      <td>-1.041284</td>\n",
       "      <td>0.157351</td>\n",
       "      <td>0.425053</td>\n",
       "      <td>0.131695</td>\n",
       "      <td>-0.474184</td>\n",
       "      <td>-0.967048</td>\n",
       "      <td>-0.045287</td>\n",
       "      <td>0.930147</td>\n",
       "      <td>-0.942065</td>\n",
       "      <td>-0.427162</td>\n",
       "      <td>-0.923421</td>\n",
       "      <td>-1.236298</td>\n",
       "      <td>0.451016</td>\n",
       "      <td>0.677714</td>\n",
       "      <td>0.578491</td>\n",
       "      <td>-0.285586</td>\n",
       "      <td>-0.137903</td>\n",
       "      <td>-1.592543</td>\n",
       "      <td>0.941437</td>\n",
       "      <td>0.319753</td>\n",
       "      <td>0.130286</td>\n",
       "      <td>-0.522547</td>\n",
       "      <td>-0.579336</td>\n",
       "      <td>0.124565</td>\n",
       "      <td>-0.219538</td>\n",
       "      <td>1.640512</td>\n",
       "      <td>1.093547</td>\n",
       "      <td>0.660521</td>\n",
       "      <td>1.132050</td>\n",
       "      <td>-0.212897</td>\n",
       "      <td>-0.083036</td>\n",
       "      <td>-0.449174</td>\n",
       "      <td>-0.254495</td>\n",
       "      <td>0.483878</td>\n",
       "      <td>-0.251386</td>\n",
       "      <td>-0.545303</td>\n",
       "      <td>0.803491</td>\n",
       "      <td>0.833312</td>\n",
       "      <td>-0.211865</td>\n",
       "      <td>-0.982234</td>\n",
       "      <td>-0.377478</td>\n",
       "      <td>-0.658292</td>\n",
       "      <td>-0.374307</td>\n",
       "      <td>-0.092113</td>\n",
       "      <td>0.131410</td>\n",
       "      <td>-0.127149</td>\n",
       "      <td>-0.445892</td>\n",
       "      <td>0.601887</td>\n",
       "      <td>-0.119428</td>\n",
       "      <td>-0.218186</td>\n",
       "      <td>-0.345315</td>\n",
       "      <td>-0.131767</td>\n",
       "      <td>1.128863</td>\n",
       "      <td>0.055303</td>\n",
       "      <td>0.182105</td>\n",
       "      <td>-0.261048</td>\n",
       "      <td>-0.915739</td>\n",
       "      <td>0.212881</td>\n",
       "      <td>0.735336</td>\n",
       "      <td>1.378469</td>\n",
       "      <td>1.292215</td>\n",
       "      <td>-0.426474</td>\n",
       "      <td>-0.569086</td>\n",
       "      <td>0.460583</td>\n",
       "      <td>0.192373</td>\n",
       "      <td>0.693539</td>\n",
       "      <td>-0.068297</td>\n",
       "      <td>0.259698</td>\n",
       "      <td>-0.648445</td>\n",
       "      <td>-0.071153</td>\n",
       "      <td>-0.617521</td>\n",
       "      <td>-0.802165</td>\n",
       "      <td>-0.869009</td>\n",
       "      <td>-0.216430</td>\n",
       "      <td>-0.515363</td>\n",
       "      <td>0.585334</td>\n",
       "      <td>-1.772227</td>\n",
       "      <td>0.192960</td>\n",
       "      <td>-0.081374</td>\n",
       "      <td>-0.384915</td>\n",
       "      <td>0.202669</td>\n",
       "      <td>-0.318162</td>\n",
       "      <td>-0.583692</td>\n",
       "      <td>-0.685025</td>\n",
       "      <td>-0.493772</td>\n",
       "      <td>-1.375381</td>\n",
       "      <td>0.034264</td>\n",
       "      <td>-0.037577</td>\n",
       "      <td>0.264094</td>\n",
       "      <td>0.507684</td>\n",
       "      <td>-0.538325</td>\n",
       "      <td>-0.351766</td>\n",
       "      <td>1.216479</td>\n",
       "      <td>0.357146</td>\n",
       "      <td>-0.213810</td>\n",
       "      <td>-1.028683</td>\n",
       "      <td>0.131457</td>\n",
       "      <td>-0.298029</td>\n",
       "      <td>-1.235245</td>\n",
       "      <td>-0.668961</td>\n",
       "      <td>-0.892707</td>\n",
       "      <td>0.195993</td>\n",
       "      <td>0.289603</td>\n",
       "      <td>0.483021</td>\n",
       "      <td>-0.896739</td>\n",
       "      <td>-0.520981</td>\n",
       "      <td>-0.730229</td>\n",
       "      <td>0.088305</td>\n",
       "      <td>0.607742</td>\n",
       "      <td>0.995007</td>\n",
       "      <td>-0.798895</td>\n",
       "      <td>-0.049450</td>\n",
       "      <td>0.829252</td>\n",
       "      <td>-1.047841</td>\n",
       "      <td>0.163200</td>\n",
       "      <td>-2.152037</td>\n",
       "      <td>0.034751</td>\n",
       "      <td>0.296912</td>\n",
       "      <td>-0.972056</td>\n",
       "      <td>1.520006</td>\n",
       "      <td>0.387377</td>\n",
       "      <td>0.293731</td>\n",
       "      <td>0.047997</td>\n",
       "      <td>-0.336874</td>\n",
       "      <td>-0.408052</td>\n",
       "      <td>-0.488919</td>\n",
       "      <td>-0.972460</td>\n",
       "      <td>1.158347</td>\n",
       "      <td>-0.306049</td>\n",
       "      <td>-0.385948</td>\n",
       "      <td>1.809788</td>\n",
       "      <td>-0.399892</td>\n",
       "      <td>-0.213527</td>\n",
       "      <td>-0.582929</td>\n",
       "      <td>0.648600</td>\n",
       "      <td>0.623755</td>\n",
       "      <td>1.045417</td>\n",
       "      <td>-0.773080</td>\n",
       "      <td>0.937119</td>\n",
       "      <td>0.663291</td>\n",
       "      <td>0.261242</td>\n",
       "      <td>-0.604901</td>\n",
       "      <td>-0.192325</td>\n",
       "      <td>0.113959</td>\n",
       "      <td>-1.520509</td>\n",
       "      <td>-1.137753</td>\n",
       "      <td>-0.818679</td>\n",
       "      <td>-0.075022</td>\n",
       "      <td>0.352320</td>\n",
       "      <td>0.168844</td>\n",
       "      <td>0.506019</td>\n",
       "      <td>0.207167</td>\n",
       "      <td>-0.540203</td>\n",
       "      <td>0.548214</td>\n",
       "      <td>0.025663</td>\n",
       "      <td>0.808421</td>\n",
       "      <td>0.026290</td>\n",
       "      <td>-1.540848</td>\n",
       "      <td>0.451329</td>\n",
       "      <td>0.781690</td>\n",
       "      <td>-0.370763</td>\n",
       "      <td>-0.549507</td>\n",
       "      <td>0.316583</td>\n",
       "      <td>-0.911931</td>\n",
       "      <td>-0.936688</td>\n",
       "      <td>-1.797986</td>\n",
       "      <td>0.995487</td>\n",
       "      <td>-0.139737</td>\n",
       "      <td>-0.059015</td>\n",
       "      <td>0.605686</td>\n",
       "      <td>-0.217134</td>\n",
       "      <td>0.404120</td>\n",
       "      <td>-0.039379</td>\n",
       "      <td>-1.154441</td>\n",
       "      <td>0.116717</td>\n",
       "      <td>-0.551561</td>\n",
       "      <td>0.516555</td>\n",
       "      <td>-0.145332</td>\n",
       "      <td>-0.828297</td>\n",
       "      <td>0.995400</td>\n",
       "      <td>0.169654</td>\n",
       "      <td>0.185134</td>\n",
       "      <td>-0.641524</td>\n",
       "      <td>0.236970</td>\n",
       "      <td>-0.168315</td>\n",
       "      <td>0.429707</td>\n",
       "      <td>-0.097793</td>\n",
       "      <td>-0.738991</td>\n",
       "      <td>-0.140453</td>\n",
       "      <td>-0.329424</td>\n",
       "      <td>-0.173847</td>\n",
       "      <td>0.354213</td>\n",
       "      <td>0.009764</td>\n",
       "      <td>-0.416106</td>\n",
       "      <td>-0.700126</td>\n",
       "      <td>-0.079852</td>\n",
       "      <td>-0.427185</td>\n",
       "      <td>-0.773300</td>\n",
       "      <td>-0.112472</td>\n",
       "      <td>0.274777</td>\n",
       "      <td>-0.332992</td>\n",
       "      <td>0.551611</td>\n",
       "      <td>1.530572</td>\n",
       "      <td>-1.192895</td>\n",
       "      <td>0.418921</td>\n",
       "      <td>-0.150063</td>\n",
       "      <td>0.373168</td>\n",
       "      <td>1.092209</td>\n",
       "      <td>0.343963</td>\n",
       "      <td>0.671086</td>\n",
       "      <td>-1.067681</td>\n",
       "      <td>0.157845</td>\n",
       "      <td>-0.203760</td>\n",
       "      <td>-0.728330</td>\n",
       "      <td>1.416629</td>\n",
       "      <td>0.558890</td>\n",
       "      <td>-0.078986</td>\n",
       "      <td>0.259909</td>\n",
       "      <td>0.280206</td>\n",
       "      <td>0.136865</td>\n",
       "      <td>0.535399</td>\n",
       "      <td>0.836058</td>\n",
       "      <td>13.755681</td>\n",
       "      <td>0.185654</td>\n",
       "      <td>0.420050</td>\n",
       "      <td>-0.249418</td>\n",
       "      <td>0.579467</td>\n",
       "      <td>-0.209460</td>\n",
       "      <td>-0.544391</td>\n",
       "      <td>-0.237524</td>\n",
       "      <td>-0.309772</td>\n",
       "      <td>0.310431</td>\n",
       "      <td>-0.550121</td>\n",
       "      <td>0.138718</td>\n",
       "      <td>-0.358635</td>\n",
       "      <td>0.975893</td>\n",
       "      <td>-0.705316</td>\n",
       "      <td>-0.779524</td>\n",
       "      <td>0.535575</td>\n",
       "      <td>0.868967</td>\n",
       "      <td>-0.005817</td>\n",
       "      <td>-0.957481</td>\n",
       "      <td>0.345602</td>\n",
       "      <td>-0.456151</td>\n",
       "      <td>0.205090</td>\n",
       "      <td>-0.477702</td>\n",
       "      <td>-0.706677</td>\n",
       "      <td>-0.839750</td>\n",
       "      <td>-0.656065</td>\n",
       "      <td>-0.255254</td>\n",
       "      <td>-0.575219</td>\n",
       "      <td>0.534209</td>\n",
       "      <td>0.721351</td>\n",
       "      <td>0.356078</td>\n",
       "      <td>-0.894912</td>\n",
       "      <td>0.475254</td>\n",
       "      <td>-0.203818</td>\n",
       "      <td>0.135967</td>\n",
       "      <td>-0.162353</td>\n",
       "      <td>-0.338342</td>\n",
       "      <td>-0.270852</td>\n",
       "      <td>-0.057112</td>\n",
       "      <td>-0.419428</td>\n",
       "      <td>0.319831</td>\n",
       "      <td>-0.778240</td>\n",
       "      <td>0.100756</td>\n",
       "      <td>-1.252283</td>\n",
       "      <td>0.214270</td>\n",
       "      <td>-0.107290</td>\n",
       "      <td>0.146275</td>\n",
       "      <td>-0.228278</td>\n",
       "      <td>1.099355</td>\n",
       "      <td>-1.125695</td>\n",
       "      <td>-0.292137</td>\n",
       "      <td>0.128582</td>\n",
       "      <td>0.777553</td>\n",
       "      <td>1.387327</td>\n",
       "      <td>-0.014129</td>\n",
       "      <td>-1.020147</td>\n",
       "      <td>0.112671</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>-0.205237</td>\n",
       "      <td>0.464534</td>\n",
       "      <td>-0.145499</td>\n",
       "      <td>0.643182</td>\n",
       "      <td>0.158840</td>\n",
       "      <td>-0.376464</td>\n",
       "      <td>-0.871322</td>\n",
       "      <td>0.895346</td>\n",
       "      <td>0.938654</td>\n",
       "      <td>-1.890932</td>\n",
       "      <td>-0.644186</td>\n",
       "      <td>0.292456</td>\n",
       "      <td>1.214450</td>\n",
       "      <td>1.147751</td>\n",
       "      <td>0.188052</td>\n",
       "      <td>-0.259500</td>\n",
       "      <td>-0.040903</td>\n",
       "      <td>-0.088932</td>\n",
       "      <td>0.081010</td>\n",
       "      <td>0.955893</td>\n",
       "      <td>0.926698</td>\n",
       "      <td>0.379206</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-1.025534</td>\n",
       "      <td>0.466301</td>\n",
       "      <td>-0.318006</td>\n",
       "      <td>0.110879</td>\n",
       "      <td>-1.479651</td>\n",
       "      <td>0.474943</td>\n",
       "      <td>0.288888</td>\n",
       "      <td>1.347598</td>\n",
       "      <td>-0.008139</td>\n",
       "      <td>-0.372184</td>\n",
       "      <td>0.151988</td>\n",
       "      <td>-0.031868</td>\n",
       "      <td>0.879389</td>\n",
       "      <td>-0.177461</td>\n",
       "      <td>-0.285828</td>\n",
       "      <td>-1.547795</td>\n",
       "      <td>0.418664</td>\n",
       "      <td>1.061381</td>\n",
       "      <td>0.049695</td>\n",
       "      <td>0.702683</td>\n",
       "      <td>0.165721</td>\n",
       "      <td>0.105098</td>\n",
       "      <td>-0.253561</td>\n",
       "      <td>-0.748394</td>\n",
       "      <td>0.583135</td>\n",
       "      <td>0.362687</td>\n",
       "      <td>-0.193986</td>\n",
       "      <td>0.372298</td>\n",
       "      <td>-0.138735</td>\n",
       "      <td>0.862881</td>\n",
       "      <td>0.127770</td>\n",
       "      <td>0.841560</td>\n",
       "      <td>0.671501</td>\n",
       "      <td>-0.442474</td>\n",
       "      <td>0.338687</td>\n",
       "      <td>0.018475</td>\n",
       "      <td>0.194239</td>\n",
       "      <td>0.252680</td>\n",
       "      <td>0.299385</td>\n",
       "      <td>-0.152371</td>\n",
       "      <td>-0.172670</td>\n",
       "      <td>0.327163</td>\n",
       "      <td>0.279736</td>\n",
       "      <td>0.031042</td>\n",
       "      <td>0.071036</td>\n",
       "      <td>-0.689128</td>\n",
       "      <td>-1.041822</td>\n",
       "      <td>0.259532</td>\n",
       "      <td>-0.961228</td>\n",
       "      <td>0.242013</td>\n",
       "      <td>0.963285</td>\n",
       "      <td>-0.365468</td>\n",
       "      <td>-0.200003</td>\n",
       "      <td>-0.385047</td>\n",
       "      <td>-0.607396</td>\n",
       "      <td>0.685862</td>\n",
       "      <td>0.407754</td>\n",
       "      <td>-0.322496</td>\n",
       "      <td>-0.570370</td>\n",
       "      <td>0.388105</td>\n",
       "      <td>-1.039391</td>\n",
       "      <td>-0.946039</td>\n",
       "      <td>0.123374</td>\n",
       "      <td>0.939547</td>\n",
       "      <td>-0.354201</td>\n",
       "      <td>-0.129019</td>\n",
       "      <td>0.860494</td>\n",
       "      <td>-1.518184</td>\n",
       "      <td>2.376690</td>\n",
       "      <td>-0.151651</td>\n",
       "      <td>-0.584268</td>\n",
       "      <td>0.447015</td>\n",
       "      <td>-0.127059</td>\n",
       "      <td>-1.072402</td>\n",
       "      <td>0.276638</td>\n",
       "      <td>-0.455084</td>\n",
       "      <td>0.823231</td>\n",
       "      <td>-1.364535</td>\n",
       "      <td>-0.263401</td>\n",
       "      <td>1.176278</td>\n",
       "      <td>1.089824</td>\n",
       "      <td>0.364919</td>\n",
       "      <td>0.361851</td>\n",
       "      <td>0.332384</td>\n",
       "      <td>0.625098</td>\n",
       "      <td>0.373669</td>\n",
       "      <td>-0.127666</td>\n",
       "      <td>-0.273327</td>\n",
       "      <td>-0.103007</td>\n",
       "      <td>0.496733</td>\n",
       "      <td>-0.085223</td>\n",
       "      <td>-0.494848</td>\n",
       "      <td>0.480460</td>\n",
       "      <td>-1.312140</td>\n",
       "      <td>0.328338</td>\n",
       "      <td>-0.490456</td>\n",
       "      <td>0.387564</td>\n",
       "      <td>1.738777</td>\n",
       "      <td>0.864402</td>\n",
       "      <td>0.367405</td>\n",
       "      <td>0.029553</td>\n",
       "      <td>-1.003045</td>\n",
       "      <td>1.031052</td>\n",
       "      <td>0.937171</td>\n",
       "      <td>0.801539</td>\n",
       "      <td>0.799956</td>\n",
       "      <td>-0.441056</td>\n",
       "      <td>0.348441</td>\n",
       "      <td>-0.779789</td>\n",
       "      <td>0.634693</td>\n",
       "      <td>1.143023</td>\n",
       "      <td>0.411128</td>\n",
       "      <td>-0.415390</td>\n",
       "      <td>-0.960516</td>\n",
       "      <td>0.041766</td>\n",
       "      <td>-1.244290</td>\n",
       "      <td>-0.800163</td>\n",
       "      <td>0.134285</td>\n",
       "      <td>-1.157098</td>\n",
       "      <td>-0.009894</td>\n",
       "      <td>0.623618</td>\n",
       "      <td>0.070537</td>\n",
       "      <td>-0.811948</td>\n",
       "      <td>-0.063017</td>\n",
       "      <td>0.195875</td>\n",
       "      <td>0.132494</td>\n",
       "      <td>-0.472655</td>\n",
       "      <td>-1.333030</td>\n",
       "      <td>0.717992</td>\n",
       "      <td>0.390614</td>\n",
       "      <td>0.962173</td>\n",
       "      <td>0.262227</td>\n",
       "      <td>0.900870</td>\n",
       "      <td>-0.082890</td>\n",
       "      <td>-0.337498</td>\n",
       "      <td>0.054027</td>\n",
       "      <td>-0.250429</td>\n",
       "      <td>0.272864</td>\n",
       "      <td>0.072126</td>\n",
       "      <td>-0.003761</td>\n",
       "      <td>-0.390650</td>\n",
       "      <td>-1.200388</td>\n",
       "      <td>-0.940271</td>\n",
       "      <td>-0.365464</td>\n",
       "      <td>-0.515058</td>\n",
       "      <td>0.924855</td>\n",
       "      <td>1.167289</td>\n",
       "      <td>-0.188000</td>\n",
       "      <td>-0.659250</td>\n",
       "      <td>1.178323</td>\n",
       "      <td>0.162583</td>\n",
       "      <td>0.248832</td>\n",
       "      <td>-1.504861</td>\n",
       "      <td>1.054700</td>\n",
       "      <td>0.330996</td>\n",
       "      <td>0.728609</td>\n",
       "      <td>0.084362</td>\n",
       "      <td>0.595130</td>\n",
       "      <td>0.555965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.46</td>\n",
       "      <td>-0.749443</td>\n",
       "      <td>-0.098571</td>\n",
       "      <td>-0.366157</td>\n",
       "      <td>-1.203457</td>\n",
       "      <td>0.373798</td>\n",
       "      <td>-1.328080</td>\n",
       "      <td>0.285747</td>\n",
       "      <td>-0.310203</td>\n",
       "      <td>-0.733059</td>\n",
       "      <td>-0.003385</td>\n",
       "      <td>-0.251482</td>\n",
       "      <td>0.566337</td>\n",
       "      <td>-0.795328</td>\n",
       "      <td>-0.147824</td>\n",
       "      <td>1.088557</td>\n",
       "      <td>0.662047</td>\n",
       "      <td>0.426646</td>\n",
       "      <td>-1.387048</td>\n",
       "      <td>0.135401</td>\n",
       "      <td>-1.020397</td>\n",
       "      <td>-0.247882</td>\n",
       "      <td>0.246379</td>\n",
       "      <td>-0.445516</td>\n",
       "      <td>-1.141686</td>\n",
       "      <td>0.456293</td>\n",
       "      <td>0.951081</td>\n",
       "      <td>-1.196160</td>\n",
       "      <td>-0.395614</td>\n",
       "      <td>0.893527</td>\n",
       "      <td>0.154916</td>\n",
       "      <td>0.274923</td>\n",
       "      <td>-0.310512</td>\n",
       "      <td>-0.504186</td>\n",
       "      <td>-0.100960</td>\n",
       "      <td>-0.526614</td>\n",
       "      <td>-0.361057</td>\n",
       "      <td>-1.240552</td>\n",
       "      <td>0.533958</td>\n",
       "      <td>0.207455</td>\n",
       "      <td>0.118717</td>\n",
       "      <td>-0.408524</td>\n",
       "      <td>-0.216827</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>-0.032500</td>\n",
       "      <td>0.409882</td>\n",
       "      <td>-0.355038</td>\n",
       "      <td>-0.036663</td>\n",
       "      <td>0.947852</td>\n",
       "      <td>0.015555</td>\n",
       "      <td>-0.898710</td>\n",
       "      <td>0.332785</td>\n",
       "      <td>-0.897706</td>\n",
       "      <td>-0.791212</td>\n",
       "      <td>0.269955</td>\n",
       "      <td>-0.231708</td>\n",
       "      <td>0.751040</td>\n",
       "      <td>-0.361495</td>\n",
       "      <td>0.499342</td>\n",
       "      <td>0.883306</td>\n",
       "      <td>0.289871</td>\n",
       "      <td>1.459861</td>\n",
       "      <td>-0.511196</td>\n",
       "      <td>0.860112</td>\n",
       "      <td>-0.048901</td>\n",
       "      <td>-1.069134</td>\n",
       "      <td>-0.925358</td>\n",
       "      <td>0.325237</td>\n",
       "      <td>0.053864</td>\n",
       "      <td>0.549295</td>\n",
       "      <td>-0.091663</td>\n",
       "      <td>0.421659</td>\n",
       "      <td>0.050689</td>\n",
       "      <td>0.436353</td>\n",
       "      <td>-0.090495</td>\n",
       "      <td>0.326694</td>\n",
       "      <td>-0.479450</td>\n",
       "      <td>0.253113</td>\n",
       "      <td>-0.164375</td>\n",
       "      <td>-0.706655</td>\n",
       "      <td>0.323677</td>\n",
       "      <td>0.347592</td>\n",
       "      <td>0.505570</td>\n",
       "      <td>1.129843</td>\n",
       "      <td>-1.871729</td>\n",
       "      <td>-0.074374</td>\n",
       "      <td>1.867762</td>\n",
       "      <td>-1.005984</td>\n",
       "      <td>-0.633077</td>\n",
       "      <td>0.326487</td>\n",
       "      <td>-0.193302</td>\n",
       "      <td>-2.614760</td>\n",
       "      <td>0.048211</td>\n",
       "      <td>0.730214</td>\n",
       "      <td>-0.311367</td>\n",
       "      <td>0.484482</td>\n",
       "      <td>-0.576028</td>\n",
       "      <td>-0.472646</td>\n",
       "      <td>-1.601808</td>\n",
       "      <td>-0.331677</td>\n",
       "      <td>0.897003</td>\n",
       "      <td>-0.044937</td>\n",
       "      <td>0.907187</td>\n",
       "      <td>0.146517</td>\n",
       "      <td>0.340527</td>\n",
       "      <td>0.023316</td>\n",
       "      <td>-0.272011</td>\n",
       "      <td>-0.509054</td>\n",
       "      <td>-1.290567</td>\n",
       "      <td>1.870402</td>\n",
       "      <td>0.590044</td>\n",
       "      <td>-0.046778</td>\n",
       "      <td>0.922202</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>-0.258700</td>\n",
       "      <td>0.598631</td>\n",
       "      <td>0.198408</td>\n",
       "      <td>1.260956</td>\n",
       "      <td>0.771179</td>\n",
       "      <td>-1.110048</td>\n",
       "      <td>-1.324780</td>\n",
       "      <td>0.350748</td>\n",
       "      <td>-0.580909</td>\n",
       "      <td>-0.989055</td>\n",
       "      <td>0.094367</td>\n",
       "      <td>-0.237820</td>\n",
       "      <td>0.855156</td>\n",
       "      <td>-0.384294</td>\n",
       "      <td>0.183543</td>\n",
       "      <td>-0.306839</td>\n",
       "      <td>1.057364</td>\n",
       "      <td>-0.465207</td>\n",
       "      <td>0.630526</td>\n",
       "      <td>0.076224</td>\n",
       "      <td>-0.930877</td>\n",
       "      <td>-0.124122</td>\n",
       "      <td>-0.195771</td>\n",
       "      <td>-0.361058</td>\n",
       "      <td>-1.219783</td>\n",
       "      <td>-0.855134</td>\n",
       "      <td>0.957495</td>\n",
       "      <td>0.155511</td>\n",
       "      <td>0.853077</td>\n",
       "      <td>-0.325408</td>\n",
       "      <td>0.375912</td>\n",
       "      <td>0.333384</td>\n",
       "      <td>-0.342820</td>\n",
       "      <td>0.491277</td>\n",
       "      <td>-0.565383</td>\n",
       "      <td>-0.013225</td>\n",
       "      <td>0.527245</td>\n",
       "      <td>0.798573</td>\n",
       "      <td>0.281680</td>\n",
       "      <td>-0.228671</td>\n",
       "      <td>0.633387</td>\n",
       "      <td>-0.108102</td>\n",
       "      <td>-0.389363</td>\n",
       "      <td>-0.387097</td>\n",
       "      <td>-1.219848</td>\n",
       "      <td>0.186485</td>\n",
       "      <td>-0.721129</td>\n",
       "      <td>-0.703561</td>\n",
       "      <td>-0.316796</td>\n",
       "      <td>1.052738</td>\n",
       "      <td>-0.775077</td>\n",
       "      <td>-0.083361</td>\n",
       "      <td>0.671369</td>\n",
       "      <td>0.531359</td>\n",
       "      <td>-0.297876</td>\n",
       "      <td>0.855593</td>\n",
       "      <td>-0.853100</td>\n",
       "      <td>-0.955371</td>\n",
       "      <td>-0.994432</td>\n",
       "      <td>-0.639273</td>\n",
       "      <td>0.012047</td>\n",
       "      <td>-0.601856</td>\n",
       "      <td>1.293677</td>\n",
       "      <td>0.303514</td>\n",
       "      <td>-0.665189</td>\n",
       "      <td>0.959418</td>\n",
       "      <td>1.310877</td>\n",
       "      <td>0.501711</td>\n",
       "      <td>0.770059</td>\n",
       "      <td>-0.176757</td>\n",
       "      <td>0.360714</td>\n",
       "      <td>-0.308157</td>\n",
       "      <td>0.211676</td>\n",
       "      <td>0.587807</td>\n",
       "      <td>0.367511</td>\n",
       "      <td>-0.514712</td>\n",
       "      <td>-0.269907</td>\n",
       "      <td>-1.334703</td>\n",
       "      <td>1.287986</td>\n",
       "      <td>0.792140</td>\n",
       "      <td>0.275841</td>\n",
       "      <td>0.702688</td>\n",
       "      <td>0.886973</td>\n",
       "      <td>-0.074619</td>\n",
       "      <td>0.872234</td>\n",
       "      <td>-0.196475</td>\n",
       "      <td>0.039554</td>\n",
       "      <td>-0.600424</td>\n",
       "      <td>-0.177276</td>\n",
       "      <td>-0.676825</td>\n",
       "      <td>-0.257799</td>\n",
       "      <td>-0.834277</td>\n",
       "      <td>-0.934696</td>\n",
       "      <td>0.291545</td>\n",
       "      <td>1.224345</td>\n",
       "      <td>0.560848</td>\n",
       "      <td>0.614920</td>\n",
       "      <td>-0.820661</td>\n",
       "      <td>-0.552016</td>\n",
       "      <td>-0.128842</td>\n",
       "      <td>0.602383</td>\n",
       "      <td>1.171128</td>\n",
       "      <td>-0.384855</td>\n",
       "      <td>-1.319689</td>\n",
       "      <td>-0.230485</td>\n",
       "      <td>-0.441356</td>\n",
       "      <td>-0.716113</td>\n",
       "      <td>-0.483825</td>\n",
       "      <td>-1.490365</td>\n",
       "      <td>-0.111421</td>\n",
       "      <td>0.091463</td>\n",
       "      <td>-0.511718</td>\n",
       "      <td>0.577919</td>\n",
       "      <td>-0.874078</td>\n",
       "      <td>-0.703042</td>\n",
       "      <td>-0.214166</td>\n",
       "      <td>-0.149047</td>\n",
       "      <td>-0.022601</td>\n",
       "      <td>0.341620</td>\n",
       "      <td>0.331021</td>\n",
       "      <td>-0.684888</td>\n",
       "      <td>-0.906597</td>\n",
       "      <td>-1.485930</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>-0.003598</td>\n",
       "      <td>-0.191711</td>\n",
       "      <td>-0.729394</td>\n",
       "      <td>-1.330785</td>\n",
       "      <td>0.475882</td>\n",
       "      <td>1.231406</td>\n",
       "      <td>-1.320078</td>\n",
       "      <td>0.183847</td>\n",
       "      <td>-0.742423</td>\n",
       "      <td>0.460619</td>\n",
       "      <td>-0.098555</td>\n",
       "      <td>0.172597</td>\n",
       "      <td>-0.857315</td>\n",
       "      <td>-0.073230</td>\n",
       "      <td>-0.181214</td>\n",
       "      <td>0.456467</td>\n",
       "      <td>2.094084</td>\n",
       "      <td>0.162776</td>\n",
       "      <td>0.149736</td>\n",
       "      <td>0.161316</td>\n",
       "      <td>1.226053</td>\n",
       "      <td>-0.171679</td>\n",
       "      <td>0.507676</td>\n",
       "      <td>0.782621</td>\n",
       "      <td>0.572614</td>\n",
       "      <td>-0.562668</td>\n",
       "      <td>0.410960</td>\n",
       "      <td>-0.803358</td>\n",
       "      <td>-0.876836</td>\n",
       "      <td>0.263284</td>\n",
       "      <td>-0.147618</td>\n",
       "      <td>-0.576411</td>\n",
       "      <td>1.181084</td>\n",
       "      <td>0.400624</td>\n",
       "      <td>-1.455873</td>\n",
       "      <td>-0.839950</td>\n",
       "      <td>-0.587590</td>\n",
       "      <td>-1.192751</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>-0.745545</td>\n",
       "      <td>-0.101624</td>\n",
       "      <td>1.430094</td>\n",
       "      <td>-0.569125</td>\n",
       "      <td>0.338735</td>\n",
       "      <td>-0.524272</td>\n",
       "      <td>-0.762091</td>\n",
       "      <td>-1.049388</td>\n",
       "      <td>0.649115</td>\n",
       "      <td>0.157896</td>\n",
       "      <td>0.111735</td>\n",
       "      <td>1.289104</td>\n",
       "      <td>0.822303</td>\n",
       "      <td>0.372039</td>\n",
       "      <td>-0.090557</td>\n",
       "      <td>0.626764</td>\n",
       "      <td>-0.599679</td>\n",
       "      <td>0.765120</td>\n",
       "      <td>-1.096391</td>\n",
       "      <td>1.138387</td>\n",
       "      <td>0.153102</td>\n",
       "      <td>-1.059184</td>\n",
       "      <td>0.907723</td>\n",
       "      <td>1.739993</td>\n",
       "      <td>-0.127071</td>\n",
       "      <td>0.356983</td>\n",
       "      <td>0.220204</td>\n",
       "      <td>0.798265</td>\n",
       "      <td>0.246982</td>\n",
       "      <td>-0.953839</td>\n",
       "      <td>0.125027</td>\n",
       "      <td>0.348625</td>\n",
       "      <td>0.243486</td>\n",
       "      <td>-0.489608</td>\n",
       "      <td>-0.956885</td>\n",
       "      <td>-0.031069</td>\n",
       "      <td>0.974607</td>\n",
       "      <td>-0.971375</td>\n",
       "      <td>-0.382681</td>\n",
       "      <td>-0.945659</td>\n",
       "      <td>-1.297571</td>\n",
       "      <td>0.368988</td>\n",
       "      <td>0.680583</td>\n",
       "      <td>0.639816</td>\n",
       "      <td>-0.292263</td>\n",
       "      <td>-0.200874</td>\n",
       "      <td>-1.547304</td>\n",
       "      <td>0.879053</td>\n",
       "      <td>0.324322</td>\n",
       "      <td>0.180030</td>\n",
       "      <td>-0.602539</td>\n",
       "      <td>-0.570659</td>\n",
       "      <td>0.222043</td>\n",
       "      <td>-0.242540</td>\n",
       "      <td>1.654178</td>\n",
       "      <td>1.184294</td>\n",
       "      <td>0.637380</td>\n",
       "      <td>1.054694</td>\n",
       "      <td>-0.160300</td>\n",
       "      <td>-0.109282</td>\n",
       "      <td>-0.529884</td>\n",
       "      <td>-0.192353</td>\n",
       "      <td>0.427855</td>\n",
       "      <td>-0.275337</td>\n",
       "      <td>-0.536501</td>\n",
       "      <td>0.812177</td>\n",
       "      <td>0.806075</td>\n",
       "      <td>-0.259708</td>\n",
       "      <td>-0.902602</td>\n",
       "      <td>-0.313199</td>\n",
       "      <td>-0.597796</td>\n",
       "      <td>-0.312101</td>\n",
       "      <td>-0.076448</td>\n",
       "      <td>0.111785</td>\n",
       "      <td>-0.155905</td>\n",
       "      <td>-0.473176</td>\n",
       "      <td>0.585917</td>\n",
       "      <td>-0.110156</td>\n",
       "      <td>-0.237753</td>\n",
       "      <td>-0.429785</td>\n",
       "      <td>-0.137947</td>\n",
       "      <td>1.142852</td>\n",
       "      <td>0.097614</td>\n",
       "      <td>0.254343</td>\n",
       "      <td>-0.307527</td>\n",
       "      <td>-0.890695</td>\n",
       "      <td>0.221037</td>\n",
       "      <td>0.808231</td>\n",
       "      <td>1.317838</td>\n",
       "      <td>1.254083</td>\n",
       "      <td>-0.450387</td>\n",
       "      <td>-0.507112</td>\n",
       "      <td>0.416753</td>\n",
       "      <td>0.220055</td>\n",
       "      <td>0.724157</td>\n",
       "      <td>-0.138319</td>\n",
       "      <td>0.284998</td>\n",
       "      <td>-0.673293</td>\n",
       "      <td>-0.030229</td>\n",
       "      <td>-0.522690</td>\n",
       "      <td>-0.833510</td>\n",
       "      <td>-0.839668</td>\n",
       "      <td>-0.179920</td>\n",
       "      <td>-0.466504</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>-1.814417</td>\n",
       "      <td>0.149464</td>\n",
       "      <td>-0.153697</td>\n",
       "      <td>-0.311654</td>\n",
       "      <td>0.207845</td>\n",
       "      <td>-0.356881</td>\n",
       "      <td>-0.605354</td>\n",
       "      <td>-0.744810</td>\n",
       "      <td>-0.498095</td>\n",
       "      <td>-1.330299</td>\n",
       "      <td>0.125353</td>\n",
       "      <td>-0.037136</td>\n",
       "      <td>0.255347</td>\n",
       "      <td>0.452987</td>\n",
       "      <td>-0.466196</td>\n",
       "      <td>-0.413569</td>\n",
       "      <td>1.174643</td>\n",
       "      <td>0.356723</td>\n",
       "      <td>-0.199031</td>\n",
       "      <td>-1.098117</td>\n",
       "      <td>0.126535</td>\n",
       "      <td>-0.253576</td>\n",
       "      <td>-1.247060</td>\n",
       "      <td>-0.638265</td>\n",
       "      <td>-0.878784</td>\n",
       "      <td>0.194846</td>\n",
       "      <td>0.342174</td>\n",
       "      <td>0.469645</td>\n",
       "      <td>-0.888599</td>\n",
       "      <td>-0.529604</td>\n",
       "      <td>-0.720473</td>\n",
       "      <td>0.029722</td>\n",
       "      <td>0.621405</td>\n",
       "      <td>1.006198</td>\n",
       "      <td>-0.866557</td>\n",
       "      <td>-0.158529</td>\n",
       "      <td>0.864191</td>\n",
       "      <td>-1.077978</td>\n",
       "      <td>0.228813</td>\n",
       "      <td>-2.115899</td>\n",
       "      <td>0.061968</td>\n",
       "      <td>0.298630</td>\n",
       "      <td>-0.935301</td>\n",
       "      <td>1.502846</td>\n",
       "      <td>0.342955</td>\n",
       "      <td>0.270812</td>\n",
       "      <td>0.029329</td>\n",
       "      <td>-0.212141</td>\n",
       "      <td>-0.443624</td>\n",
       "      <td>-0.473634</td>\n",
       "      <td>-0.867430</td>\n",
       "      <td>1.179359</td>\n",
       "      <td>-0.336955</td>\n",
       "      <td>-0.445781</td>\n",
       "      <td>1.807294</td>\n",
       "      <td>-0.403697</td>\n",
       "      <td>-0.165659</td>\n",
       "      <td>-0.490828</td>\n",
       "      <td>0.566638</td>\n",
       "      <td>0.508331</td>\n",
       "      <td>1.021329</td>\n",
       "      <td>-0.694598</td>\n",
       "      <td>0.978845</td>\n",
       "      <td>0.556130</td>\n",
       "      <td>0.175262</td>\n",
       "      <td>-0.582990</td>\n",
       "      <td>-0.116856</td>\n",
       "      <td>0.092763</td>\n",
       "      <td>-1.522888</td>\n",
       "      <td>-1.113762</td>\n",
       "      <td>-0.719209</td>\n",
       "      <td>-0.037430</td>\n",
       "      <td>0.384419</td>\n",
       "      <td>0.116580</td>\n",
       "      <td>0.551749</td>\n",
       "      <td>0.236469</td>\n",
       "      <td>-0.540966</td>\n",
       "      <td>0.513225</td>\n",
       "      <td>-0.006765</td>\n",
       "      <td>0.825184</td>\n",
       "      <td>0.055512</td>\n",
       "      <td>-1.552524</td>\n",
       "      <td>0.451008</td>\n",
       "      <td>0.747115</td>\n",
       "      <td>-0.350361</td>\n",
       "      <td>-0.601362</td>\n",
       "      <td>0.330084</td>\n",
       "      <td>-0.850026</td>\n",
       "      <td>-0.916450</td>\n",
       "      <td>-1.753235</td>\n",
       "      <td>0.981869</td>\n",
       "      <td>-0.096701</td>\n",
       "      <td>0.012154</td>\n",
       "      <td>0.569903</td>\n",
       "      <td>-0.257482</td>\n",
       "      <td>0.431447</td>\n",
       "      <td>-0.099163</td>\n",
       "      <td>-1.242696</td>\n",
       "      <td>0.079654</td>\n",
       "      <td>-0.486146</td>\n",
       "      <td>0.515611</td>\n",
       "      <td>-0.140231</td>\n",
       "      <td>-0.814907</td>\n",
       "      <td>1.048044</td>\n",
       "      <td>0.136855</td>\n",
       "      <td>0.138922</td>\n",
       "      <td>-0.704128</td>\n",
       "      <td>0.309598</td>\n",
       "      <td>-0.122267</td>\n",
       "      <td>0.504417</td>\n",
       "      <td>-0.125301</td>\n",
       "      <td>-0.766595</td>\n",
       "      <td>-0.231429</td>\n",
       "      <td>-0.432408</td>\n",
       "      <td>-0.211378</td>\n",
       "      <td>0.331680</td>\n",
       "      <td>0.031405</td>\n",
       "      <td>-0.409302</td>\n",
       "      <td>-0.803247</td>\n",
       "      <td>-0.091855</td>\n",
       "      <td>-0.354815</td>\n",
       "      <td>-0.758724</td>\n",
       "      <td>-0.045611</td>\n",
       "      <td>0.318434</td>\n",
       "      <td>-0.388747</td>\n",
       "      <td>0.510582</td>\n",
       "      <td>1.489094</td>\n",
       "      <td>-1.200607</td>\n",
       "      <td>0.464071</td>\n",
       "      <td>-0.174496</td>\n",
       "      <td>0.332447</td>\n",
       "      <td>1.104886</td>\n",
       "      <td>0.324981</td>\n",
       "      <td>0.597101</td>\n",
       "      <td>-0.990975</td>\n",
       "      <td>0.101849</td>\n",
       "      <td>-0.211951</td>\n",
       "      <td>-0.680470</td>\n",
       "      <td>1.468576</td>\n",
       "      <td>0.584807</td>\n",
       "      <td>-0.035795</td>\n",
       "      <td>0.267821</td>\n",
       "      <td>0.310590</td>\n",
       "      <td>0.202733</td>\n",
       "      <td>0.507496</td>\n",
       "      <td>0.849046</td>\n",
       "      <td>13.810098</td>\n",
       "      <td>0.217498</td>\n",
       "      <td>0.424507</td>\n",
       "      <td>-0.162423</td>\n",
       "      <td>0.646327</td>\n",
       "      <td>-0.246038</td>\n",
       "      <td>-0.558326</td>\n",
       "      <td>-0.173403</td>\n",
       "      <td>-0.300309</td>\n",
       "      <td>0.366463</td>\n",
       "      <td>-0.618099</td>\n",
       "      <td>0.112679</td>\n",
       "      <td>-0.282155</td>\n",
       "      <td>0.940206</td>\n",
       "      <td>-0.572087</td>\n",
       "      <td>-0.738524</td>\n",
       "      <td>0.514402</td>\n",
       "      <td>0.939723</td>\n",
       "      <td>0.039060</td>\n",
       "      <td>-1.017120</td>\n",
       "      <td>0.372926</td>\n",
       "      <td>-0.449614</td>\n",
       "      <td>0.186620</td>\n",
       "      <td>-0.504193</td>\n",
       "      <td>-0.735414</td>\n",
       "      <td>-0.906574</td>\n",
       "      <td>-0.620949</td>\n",
       "      <td>-0.297959</td>\n",
       "      <td>-0.571343</td>\n",
       "      <td>0.491162</td>\n",
       "      <td>0.773338</td>\n",
       "      <td>0.389001</td>\n",
       "      <td>-0.916831</td>\n",
       "      <td>0.520861</td>\n",
       "      <td>-0.102760</td>\n",
       "      <td>0.187174</td>\n",
       "      <td>-0.096972</td>\n",
       "      <td>-0.317347</td>\n",
       "      <td>-0.311745</td>\n",
       "      <td>-0.083950</td>\n",
       "      <td>-0.468372</td>\n",
       "      <td>0.417958</td>\n",
       "      <td>-0.871809</td>\n",
       "      <td>0.111372</td>\n",
       "      <td>-1.179978</td>\n",
       "      <td>0.125957</td>\n",
       "      <td>-0.055086</td>\n",
       "      <td>0.118006</td>\n",
       "      <td>-0.224943</td>\n",
       "      <td>1.173329</td>\n",
       "      <td>-1.092230</td>\n",
       "      <td>-0.321947</td>\n",
       "      <td>0.094633</td>\n",
       "      <td>0.755455</td>\n",
       "      <td>1.366751</td>\n",
       "      <td>-0.055530</td>\n",
       "      <td>-1.005778</td>\n",
       "      <td>0.092789</td>\n",
       "      <td>0.045076</td>\n",
       "      <td>-0.196076</td>\n",
       "      <td>0.442624</td>\n",
       "      <td>-0.156460</td>\n",
       "      <td>0.604558</td>\n",
       "      <td>0.224771</td>\n",
       "      <td>-0.439093</td>\n",
       "      <td>-0.884282</td>\n",
       "      <td>0.837966</td>\n",
       "      <td>0.902498</td>\n",
       "      <td>-1.854478</td>\n",
       "      <td>-0.662036</td>\n",
       "      <td>0.235553</td>\n",
       "      <td>1.269663</td>\n",
       "      <td>1.150201</td>\n",
       "      <td>0.281798</td>\n",
       "      <td>-0.251417</td>\n",
       "      <td>-0.066791</td>\n",
       "      <td>-0.106072</td>\n",
       "      <td>0.051792</td>\n",
       "      <td>1.007021</td>\n",
       "      <td>0.846095</td>\n",
       "      <td>0.376695</td>\n",
       "      <td>-0.401701</td>\n",
       "      <td>-1.045586</td>\n",
       "      <td>0.445287</td>\n",
       "      <td>-0.279466</td>\n",
       "      <td>0.103828</td>\n",
       "      <td>-1.554351</td>\n",
       "      <td>0.487281</td>\n",
       "      <td>0.278111</td>\n",
       "      <td>1.311831</td>\n",
       "      <td>0.096571</td>\n",
       "      <td>-0.277448</td>\n",
       "      <td>0.117304</td>\n",
       "      <td>0.091447</td>\n",
       "      <td>0.901578</td>\n",
       "      <td>-0.187459</td>\n",
       "      <td>-0.334313</td>\n",
       "      <td>-1.461941</td>\n",
       "      <td>0.399218</td>\n",
       "      <td>0.992120</td>\n",
       "      <td>0.097716</td>\n",
       "      <td>0.760746</td>\n",
       "      <td>0.187692</td>\n",
       "      <td>0.093143</td>\n",
       "      <td>-0.252097</td>\n",
       "      <td>-0.769815</td>\n",
       "      <td>0.541137</td>\n",
       "      <td>0.328524</td>\n",
       "      <td>-0.176210</td>\n",
       "      <td>0.395859</td>\n",
       "      <td>-0.176682</td>\n",
       "      <td>0.918239</td>\n",
       "      <td>0.121457</td>\n",
       "      <td>0.860408</td>\n",
       "      <td>0.685412</td>\n",
       "      <td>-0.415399</td>\n",
       "      <td>0.288547</td>\n",
       "      <td>0.015103</td>\n",
       "      <td>0.216210</td>\n",
       "      <td>0.223028</td>\n",
       "      <td>0.311005</td>\n",
       "      <td>-0.136705</td>\n",
       "      <td>-0.132440</td>\n",
       "      <td>0.311305</td>\n",
       "      <td>0.226451</td>\n",
       "      <td>-0.027480</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>-0.786218</td>\n",
       "      <td>-1.022326</td>\n",
       "      <td>0.244705</td>\n",
       "      <td>-0.999520</td>\n",
       "      <td>0.176796</td>\n",
       "      <td>0.988606</td>\n",
       "      <td>-0.307477</td>\n",
       "      <td>-0.168795</td>\n",
       "      <td>-0.456089</td>\n",
       "      <td>-0.565756</td>\n",
       "      <td>0.765105</td>\n",
       "      <td>0.388070</td>\n",
       "      <td>-0.279937</td>\n",
       "      <td>-0.631984</td>\n",
       "      <td>0.377675</td>\n",
       "      <td>-0.999921</td>\n",
       "      <td>-0.913309</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.924227</td>\n",
       "      <td>-0.357229</td>\n",
       "      <td>-0.081519</td>\n",
       "      <td>0.840150</td>\n",
       "      <td>-1.501712</td>\n",
       "      <td>2.406128</td>\n",
       "      <td>-0.208879</td>\n",
       "      <td>-0.486539</td>\n",
       "      <td>0.410345</td>\n",
       "      <td>-0.072436</td>\n",
       "      <td>-1.098973</td>\n",
       "      <td>0.216020</td>\n",
       "      <td>-0.467757</td>\n",
       "      <td>0.835978</td>\n",
       "      <td>-1.368567</td>\n",
       "      <td>-0.225865</td>\n",
       "      <td>1.181766</td>\n",
       "      <td>1.148447</td>\n",
       "      <td>0.321342</td>\n",
       "      <td>0.436042</td>\n",
       "      <td>0.356814</td>\n",
       "      <td>0.529793</td>\n",
       "      <td>0.307496</td>\n",
       "      <td>-0.056465</td>\n",
       "      <td>-0.179904</td>\n",
       "      <td>-0.071077</td>\n",
       "      <td>0.459753</td>\n",
       "      <td>-0.105336</td>\n",
       "      <td>-0.506131</td>\n",
       "      <td>0.463870</td>\n",
       "      <td>-1.298070</td>\n",
       "      <td>0.300489</td>\n",
       "      <td>-0.496427</td>\n",
       "      <td>0.394794</td>\n",
       "      <td>1.759747</td>\n",
       "      <td>0.882884</td>\n",
       "      <td>0.403094</td>\n",
       "      <td>0.123694</td>\n",
       "      <td>-1.037884</td>\n",
       "      <td>0.928086</td>\n",
       "      <td>1.004122</td>\n",
       "      <td>0.827194</td>\n",
       "      <td>0.786512</td>\n",
       "      <td>-0.321982</td>\n",
       "      <td>0.335881</td>\n",
       "      <td>-0.818204</td>\n",
       "      <td>0.719793</td>\n",
       "      <td>1.097862</td>\n",
       "      <td>0.476589</td>\n",
       "      <td>-0.359638</td>\n",
       "      <td>-0.942566</td>\n",
       "      <td>0.056413</td>\n",
       "      <td>-1.260244</td>\n",
       "      <td>-0.761508</td>\n",
       "      <td>0.072833</td>\n",
       "      <td>-1.160706</td>\n",
       "      <td>-0.126908</td>\n",
       "      <td>0.542049</td>\n",
       "      <td>-0.025297</td>\n",
       "      <td>-0.809199</td>\n",
       "      <td>-0.098120</td>\n",
       "      <td>0.244973</td>\n",
       "      <td>0.123797</td>\n",
       "      <td>-0.511056</td>\n",
       "      <td>-1.353314</td>\n",
       "      <td>0.711025</td>\n",
       "      <td>0.376009</td>\n",
       "      <td>0.984575</td>\n",
       "      <td>0.281475</td>\n",
       "      <td>0.811442</td>\n",
       "      <td>-0.114741</td>\n",
       "      <td>-0.319305</td>\n",
       "      <td>0.029654</td>\n",
       "      <td>-0.266911</td>\n",
       "      <td>0.264867</td>\n",
       "      <td>0.021795</td>\n",
       "      <td>-0.044050</td>\n",
       "      <td>-0.415797</td>\n",
       "      <td>-1.123485</td>\n",
       "      <td>-0.934873</td>\n",
       "      <td>-0.339234</td>\n",
       "      <td>-0.418925</td>\n",
       "      <td>0.937384</td>\n",
       "      <td>1.150223</td>\n",
       "      <td>-0.289407</td>\n",
       "      <td>-0.631850</td>\n",
       "      <td>1.171261</td>\n",
       "      <td>0.138702</td>\n",
       "      <td>0.331728</td>\n",
       "      <td>-1.531718</td>\n",
       "      <td>1.063338</td>\n",
       "      <td>0.340977</td>\n",
       "      <td>0.678672</td>\n",
       "      <td>0.015195</td>\n",
       "      <td>0.580006</td>\n",
       "      <td>0.542342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.40</td>\n",
       "      <td>-0.857063</td>\n",
       "      <td>-0.197781</td>\n",
       "      <td>-0.360755</td>\n",
       "      <td>-1.130626</td>\n",
       "      <td>0.433399</td>\n",
       "      <td>-1.356162</td>\n",
       "      <td>0.274503</td>\n",
       "      <td>-0.223516</td>\n",
       "      <td>-0.794815</td>\n",
       "      <td>-0.018643</td>\n",
       "      <td>-0.045138</td>\n",
       "      <td>0.623520</td>\n",
       "      <td>-0.529238</td>\n",
       "      <td>-0.181038</td>\n",
       "      <td>0.889952</td>\n",
       "      <td>0.692126</td>\n",
       "      <td>0.456344</td>\n",
       "      <td>-1.510397</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>-0.990889</td>\n",
       "      <td>-0.245464</td>\n",
       "      <td>0.255041</td>\n",
       "      <td>-0.565098</td>\n",
       "      <td>-1.127335</td>\n",
       "      <td>0.449178</td>\n",
       "      <td>0.737562</td>\n",
       "      <td>-1.282062</td>\n",
       "      <td>-0.299809</td>\n",
       "      <td>1.034325</td>\n",
       "      <td>0.083715</td>\n",
       "      <td>0.291678</td>\n",
       "      <td>-0.301196</td>\n",
       "      <td>-0.608537</td>\n",
       "      <td>-0.120803</td>\n",
       "      <td>-0.577738</td>\n",
       "      <td>-0.561694</td>\n",
       "      <td>-0.898042</td>\n",
       "      <td>0.638654</td>\n",
       "      <td>0.221743</td>\n",
       "      <td>0.123853</td>\n",
       "      <td>-0.475295</td>\n",
       "      <td>-0.105491</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.151222</td>\n",
       "      <td>0.246273</td>\n",
       "      <td>-0.497533</td>\n",
       "      <td>0.005460</td>\n",
       "      <td>0.790434</td>\n",
       "      <td>0.019636</td>\n",
       "      <td>-0.729609</td>\n",
       "      <td>0.364228</td>\n",
       "      <td>-0.898165</td>\n",
       "      <td>-0.677331</td>\n",
       "      <td>0.213105</td>\n",
       "      <td>-0.205531</td>\n",
       "      <td>0.634712</td>\n",
       "      <td>-0.394036</td>\n",
       "      <td>0.373231</td>\n",
       "      <td>0.851021</td>\n",
       "      <td>0.416084</td>\n",
       "      <td>1.469023</td>\n",
       "      <td>-0.530620</td>\n",
       "      <td>0.815946</td>\n",
       "      <td>-0.027941</td>\n",
       "      <td>-1.031988</td>\n",
       "      <td>-0.899010</td>\n",
       "      <td>0.164482</td>\n",
       "      <td>0.030425</td>\n",
       "      <td>0.545800</td>\n",
       "      <td>0.178033</td>\n",
       "      <td>0.372368</td>\n",
       "      <td>0.109837</td>\n",
       "      <td>0.610416</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.520571</td>\n",
       "      <td>-0.567462</td>\n",
       "      <td>0.143227</td>\n",
       "      <td>-0.047161</td>\n",
       "      <td>-0.688618</td>\n",
       "      <td>0.349195</td>\n",
       "      <td>0.276282</td>\n",
       "      <td>0.656060</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>-1.839343</td>\n",
       "      <td>-0.209492</td>\n",
       "      <td>1.846800</td>\n",
       "      <td>-0.872475</td>\n",
       "      <td>-0.586209</td>\n",
       "      <td>0.391285</td>\n",
       "      <td>-0.292442</td>\n",
       "      <td>-2.655720</td>\n",
       "      <td>-0.057521</td>\n",
       "      <td>0.755614</td>\n",
       "      <td>-0.230552</td>\n",
       "      <td>0.420515</td>\n",
       "      <td>-0.462748</td>\n",
       "      <td>-0.454426</td>\n",
       "      <td>-1.671276</td>\n",
       "      <td>-0.438877</td>\n",
       "      <td>0.803473</td>\n",
       "      <td>-0.238716</td>\n",
       "      <td>0.939506</td>\n",
       "      <td>0.116816</td>\n",
       "      <td>0.406810</td>\n",
       "      <td>0.249497</td>\n",
       "      <td>-0.245721</td>\n",
       "      <td>-0.482370</td>\n",
       "      <td>-1.358497</td>\n",
       "      <td>2.073709</td>\n",
       "      <td>0.452553</td>\n",
       "      <td>0.021792</td>\n",
       "      <td>0.924943</td>\n",
       "      <td>0.071360</td>\n",
       "      <td>-0.207858</td>\n",
       "      <td>0.483160</td>\n",
       "      <td>0.295753</td>\n",
       "      <td>1.241012</td>\n",
       "      <td>0.662258</td>\n",
       "      <td>-1.036990</td>\n",
       "      <td>-1.350398</td>\n",
       "      <td>0.398695</td>\n",
       "      <td>-0.566180</td>\n",
       "      <td>-1.041174</td>\n",
       "      <td>0.281467</td>\n",
       "      <td>-0.208438</td>\n",
       "      <td>0.749123</td>\n",
       "      <td>-0.467282</td>\n",
       "      <td>0.171198</td>\n",
       "      <td>-0.387580</td>\n",
       "      <td>1.164927</td>\n",
       "      <td>-0.580411</td>\n",
       "      <td>0.779298</td>\n",
       "      <td>0.024115</td>\n",
       "      <td>-0.993138</td>\n",
       "      <td>0.055619</td>\n",
       "      <td>-0.181455</td>\n",
       "      <td>-0.389649</td>\n",
       "      <td>-1.278570</td>\n",
       "      <td>-0.997796</td>\n",
       "      <td>0.919705</td>\n",
       "      <td>0.124421</td>\n",
       "      <td>0.840801</td>\n",
       "      <td>-0.232948</td>\n",
       "      <td>0.243837</td>\n",
       "      <td>0.711164</td>\n",
       "      <td>-0.278361</td>\n",
       "      <td>0.606251</td>\n",
       "      <td>-0.542213</td>\n",
       "      <td>0.110271</td>\n",
       "      <td>0.507082</td>\n",
       "      <td>0.887166</td>\n",
       "      <td>0.120492</td>\n",
       "      <td>-0.241631</td>\n",
       "      <td>0.713809</td>\n",
       "      <td>-0.038259</td>\n",
       "      <td>-0.083793</td>\n",
       "      <td>-0.321732</td>\n",
       "      <td>-1.124533</td>\n",
       "      <td>0.174437</td>\n",
       "      <td>-0.722111</td>\n",
       "      <td>-0.563352</td>\n",
       "      <td>-0.280626</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>-0.696046</td>\n",
       "      <td>0.096124</td>\n",
       "      <td>0.762789</td>\n",
       "      <td>0.437614</td>\n",
       "      <td>-0.162628</td>\n",
       "      <td>0.658576</td>\n",
       "      <td>-0.888619</td>\n",
       "      <td>-0.937174</td>\n",
       "      <td>-1.002537</td>\n",
       "      <td>-0.620025</td>\n",
       "      <td>-0.037638</td>\n",
       "      <td>-0.497662</td>\n",
       "      <td>1.239712</td>\n",
       "      <td>0.218228</td>\n",
       "      <td>-0.435386</td>\n",
       "      <td>1.177694</td>\n",
       "      <td>1.453738</td>\n",
       "      <td>0.621714</td>\n",
       "      <td>0.707822</td>\n",
       "      <td>-0.222358</td>\n",
       "      <td>0.218179</td>\n",
       "      <td>-0.203008</td>\n",
       "      <td>0.147160</td>\n",
       "      <td>0.651795</td>\n",
       "      <td>0.185749</td>\n",
       "      <td>-0.535373</td>\n",
       "      <td>-0.292582</td>\n",
       "      <td>-1.176272</td>\n",
       "      <td>1.315063</td>\n",
       "      <td>0.849471</td>\n",
       "      <td>0.073633</td>\n",
       "      <td>0.834554</td>\n",
       "      <td>0.778567</td>\n",
       "      <td>-0.103062</td>\n",
       "      <td>0.563622</td>\n",
       "      <td>-0.116904</td>\n",
       "      <td>-0.115872</td>\n",
       "      <td>-0.858578</td>\n",
       "      <td>-0.160163</td>\n",
       "      <td>-0.672919</td>\n",
       "      <td>-0.201171</td>\n",
       "      <td>-0.775342</td>\n",
       "      <td>-0.967213</td>\n",
       "      <td>0.382587</td>\n",
       "      <td>1.142314</td>\n",
       "      <td>0.679020</td>\n",
       "      <td>0.689643</td>\n",
       "      <td>-0.777711</td>\n",
       "      <td>-0.263860</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.514170</td>\n",
       "      <td>1.133368</td>\n",
       "      <td>-0.438484</td>\n",
       "      <td>-1.409057</td>\n",
       "      <td>-0.267889</td>\n",
       "      <td>-0.614496</td>\n",
       "      <td>-0.565937</td>\n",
       "      <td>-0.421485</td>\n",
       "      <td>-1.410576</td>\n",
       "      <td>-0.118472</td>\n",
       "      <td>-0.068280</td>\n",
       "      <td>-0.750299</td>\n",
       "      <td>0.517369</td>\n",
       "      <td>-0.606528</td>\n",
       "      <td>-0.660536</td>\n",
       "      <td>-0.196403</td>\n",
       "      <td>-0.111387</td>\n",
       "      <td>-0.193759</td>\n",
       "      <td>0.378023</td>\n",
       "      <td>0.387297</td>\n",
       "      <td>-0.647688</td>\n",
       "      <td>-0.968959</td>\n",
       "      <td>-1.462543</td>\n",
       "      <td>-0.064031</td>\n",
       "      <td>-0.005024</td>\n",
       "      <td>-0.240323</td>\n",
       "      <td>-0.693794</td>\n",
       "      <td>-1.184708</td>\n",
       "      <td>0.501205</td>\n",
       "      <td>1.616652</td>\n",
       "      <td>-1.416650</td>\n",
       "      <td>0.118797</td>\n",
       "      <td>-0.799336</td>\n",
       "      <td>0.411801</td>\n",
       "      <td>0.026219</td>\n",
       "      <td>0.029920</td>\n",
       "      <td>-0.788406</td>\n",
       "      <td>-0.086289</td>\n",
       "      <td>-0.245935</td>\n",
       "      <td>0.337564</td>\n",
       "      <td>1.944634</td>\n",
       "      <td>0.112124</td>\n",
       "      <td>0.070472</td>\n",
       "      <td>0.284243</td>\n",
       "      <td>1.235332</td>\n",
       "      <td>-0.089691</td>\n",
       "      <td>0.411202</td>\n",
       "      <td>0.751495</td>\n",
       "      <td>0.436115</td>\n",
       "      <td>-0.432318</td>\n",
       "      <td>0.399369</td>\n",
       "      <td>-0.736291</td>\n",
       "      <td>-0.925475</td>\n",
       "      <td>0.202920</td>\n",
       "      <td>0.144372</td>\n",
       "      <td>-0.484589</td>\n",
       "      <td>1.196334</td>\n",
       "      <td>0.416505</td>\n",
       "      <td>-1.442390</td>\n",
       "      <td>-0.811985</td>\n",
       "      <td>-0.679783</td>\n",
       "      <td>-1.141728</td>\n",
       "      <td>-0.080420</td>\n",
       "      <td>-0.666271</td>\n",
       "      <td>-0.049418</td>\n",
       "      <td>1.414865</td>\n",
       "      <td>-0.511304</td>\n",
       "      <td>0.412234</td>\n",
       "      <td>-0.762554</td>\n",
       "      <td>-0.630332</td>\n",
       "      <td>-0.889377</td>\n",
       "      <td>0.657835</td>\n",
       "      <td>0.230497</td>\n",
       "      <td>0.031280</td>\n",
       "      <td>1.452565</td>\n",
       "      <td>1.086924</td>\n",
       "      <td>0.379444</td>\n",
       "      <td>-0.235843</td>\n",
       "      <td>0.537226</td>\n",
       "      <td>-0.673907</td>\n",
       "      <td>0.790586</td>\n",
       "      <td>-1.239580</td>\n",
       "      <td>0.956497</td>\n",
       "      <td>0.093771</td>\n",
       "      <td>-1.129679</td>\n",
       "      <td>0.694391</td>\n",
       "      <td>1.584094</td>\n",
       "      <td>-0.107322</td>\n",
       "      <td>0.405742</td>\n",
       "      <td>0.227096</td>\n",
       "      <td>0.819183</td>\n",
       "      <td>0.338765</td>\n",
       "      <td>-0.858744</td>\n",
       "      <td>0.279879</td>\n",
       "      <td>0.456360</td>\n",
       "      <td>0.096465</td>\n",
       "      <td>-0.479309</td>\n",
       "      <td>-0.956772</td>\n",
       "      <td>-0.019054</td>\n",
       "      <td>0.804485</td>\n",
       "      <td>-1.070583</td>\n",
       "      <td>-0.445522</td>\n",
       "      <td>-0.872635</td>\n",
       "      <td>-1.291553</td>\n",
       "      <td>0.331101</td>\n",
       "      <td>0.817385</td>\n",
       "      <td>0.783406</td>\n",
       "      <td>-0.424562</td>\n",
       "      <td>-0.223315</td>\n",
       "      <td>-1.591913</td>\n",
       "      <td>0.977125</td>\n",
       "      <td>0.113102</td>\n",
       "      <td>0.125281</td>\n",
       "      <td>-0.615123</td>\n",
       "      <td>-0.653087</td>\n",
       "      <td>0.292330</td>\n",
       "      <td>-0.156515</td>\n",
       "      <td>1.541056</td>\n",
       "      <td>1.180048</td>\n",
       "      <td>0.942938</td>\n",
       "      <td>1.035298</td>\n",
       "      <td>-0.422127</td>\n",
       "      <td>-0.152245</td>\n",
       "      <td>-0.462570</td>\n",
       "      <td>-0.073727</td>\n",
       "      <td>0.583474</td>\n",
       "      <td>-0.533425</td>\n",
       "      <td>-0.536332</td>\n",
       "      <td>0.876369</td>\n",
       "      <td>0.873332</td>\n",
       "      <td>-0.255398</td>\n",
       "      <td>-0.952862</td>\n",
       "      <td>-0.405484</td>\n",
       "      <td>-0.747959</td>\n",
       "      <td>-0.478568</td>\n",
       "      <td>-0.163381</td>\n",
       "      <td>-0.042504</td>\n",
       "      <td>-0.205303</td>\n",
       "      <td>-0.422434</td>\n",
       "      <td>0.515208</td>\n",
       "      <td>-0.284977</td>\n",
       "      <td>-0.304822</td>\n",
       "      <td>-0.434386</td>\n",
       "      <td>-0.085120</td>\n",
       "      <td>1.123193</td>\n",
       "      <td>0.040862</td>\n",
       "      <td>0.242616</td>\n",
       "      <td>-0.143464</td>\n",
       "      <td>-0.854485</td>\n",
       "      <td>0.349412</td>\n",
       "      <td>0.909687</td>\n",
       "      <td>1.325083</td>\n",
       "      <td>1.268041</td>\n",
       "      <td>-0.383491</td>\n",
       "      <td>-0.526563</td>\n",
       "      <td>0.470265</td>\n",
       "      <td>0.184879</td>\n",
       "      <td>0.924516</td>\n",
       "      <td>0.028757</td>\n",
       "      <td>0.179295</td>\n",
       "      <td>-0.833563</td>\n",
       "      <td>-0.272071</td>\n",
       "      <td>-0.648685</td>\n",
       "      <td>-0.880806</td>\n",
       "      <td>-0.828425</td>\n",
       "      <td>-0.211412</td>\n",
       "      <td>-0.527209</td>\n",
       "      <td>0.483659</td>\n",
       "      <td>-2.047873</td>\n",
       "      <td>0.201151</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>-0.541647</td>\n",
       "      <td>0.129722</td>\n",
       "      <td>-0.387287</td>\n",
       "      <td>-0.432867</td>\n",
       "      <td>-0.929499</td>\n",
       "      <td>-0.585198</td>\n",
       "      <td>-1.342841</td>\n",
       "      <td>-0.118521</td>\n",
       "      <td>-0.248185</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.370611</td>\n",
       "      <td>-0.372156</td>\n",
       "      <td>-0.478614</td>\n",
       "      <td>1.106689</td>\n",
       "      <td>0.420173</td>\n",
       "      <td>-0.220321</td>\n",
       "      <td>-1.131540</td>\n",
       "      <td>-0.054001</td>\n",
       "      <td>-0.277774</td>\n",
       "      <td>-1.478531</td>\n",
       "      <td>-0.408099</td>\n",
       "      <td>-0.946686</td>\n",
       "      <td>0.072410</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>0.517003</td>\n",
       "      <td>-0.908118</td>\n",
       "      <td>-0.557184</td>\n",
       "      <td>-0.746486</td>\n",
       "      <td>0.127018</td>\n",
       "      <td>0.928284</td>\n",
       "      <td>0.949543</td>\n",
       "      <td>-0.556355</td>\n",
       "      <td>-0.064072</td>\n",
       "      <td>0.857030</td>\n",
       "      <td>-0.976561</td>\n",
       "      <td>0.305526</td>\n",
       "      <td>-2.154242</td>\n",
       "      <td>0.223317</td>\n",
       "      <td>0.225035</td>\n",
       "      <td>-0.839536</td>\n",
       "      <td>1.520393</td>\n",
       "      <td>0.305954</td>\n",
       "      <td>0.253186</td>\n",
       "      <td>-0.169697</td>\n",
       "      <td>-0.203706</td>\n",
       "      <td>-0.476610</td>\n",
       "      <td>-0.466606</td>\n",
       "      <td>-0.758849</td>\n",
       "      <td>1.145413</td>\n",
       "      <td>-0.261004</td>\n",
       "      <td>-0.524954</td>\n",
       "      <td>1.727286</td>\n",
       "      <td>-0.309588</td>\n",
       "      <td>0.175843</td>\n",
       "      <td>-0.460226</td>\n",
       "      <td>0.632846</td>\n",
       "      <td>0.467701</td>\n",
       "      <td>1.090383</td>\n",
       "      <td>-0.589556</td>\n",
       "      <td>0.804530</td>\n",
       "      <td>0.373160</td>\n",
       "      <td>0.190061</td>\n",
       "      <td>-0.612506</td>\n",
       "      <td>-0.258711</td>\n",
       "      <td>0.113132</td>\n",
       "      <td>-1.599032</td>\n",
       "      <td>-1.110064</td>\n",
       "      <td>-0.750210</td>\n",
       "      <td>-0.009055</td>\n",
       "      <td>0.475273</td>\n",
       "      <td>0.351373</td>\n",
       "      <td>0.520845</td>\n",
       "      <td>0.161207</td>\n",
       "      <td>-0.649602</td>\n",
       "      <td>0.345635</td>\n",
       "      <td>-0.142417</td>\n",
       "      <td>0.763289</td>\n",
       "      <td>0.134580</td>\n",
       "      <td>-1.721865</td>\n",
       "      <td>0.297760</td>\n",
       "      <td>0.748106</td>\n",
       "      <td>-0.422488</td>\n",
       "      <td>-0.589187</td>\n",
       "      <td>0.405591</td>\n",
       "      <td>-0.749701</td>\n",
       "      <td>-0.847804</td>\n",
       "      <td>-1.799725</td>\n",
       "      <td>1.100659</td>\n",
       "      <td>-0.209567</td>\n",
       "      <td>-0.014525</td>\n",
       "      <td>0.545105</td>\n",
       "      <td>-0.295599</td>\n",
       "      <td>0.502782</td>\n",
       "      <td>-0.218491</td>\n",
       "      <td>-1.147477</td>\n",
       "      <td>0.301702</td>\n",
       "      <td>-0.355909</td>\n",
       "      <td>0.382653</td>\n",
       "      <td>-0.142286</td>\n",
       "      <td>-0.786498</td>\n",
       "      <td>0.939285</td>\n",
       "      <td>0.105172</td>\n",
       "      <td>0.199054</td>\n",
       "      <td>-0.468503</td>\n",
       "      <td>0.014010</td>\n",
       "      <td>0.128413</td>\n",
       "      <td>0.329533</td>\n",
       "      <td>-0.086558</td>\n",
       "      <td>-0.549329</td>\n",
       "      <td>-0.238406</td>\n",
       "      <td>-0.675041</td>\n",
       "      <td>-0.244936</td>\n",
       "      <td>0.383471</td>\n",
       "      <td>-0.161691</td>\n",
       "      <td>-0.375179</td>\n",
       "      <td>-0.508296</td>\n",
       "      <td>-0.100791</td>\n",
       "      <td>-0.423717</td>\n",
       "      <td>-0.463096</td>\n",
       "      <td>-0.096444</td>\n",
       "      <td>0.203225</td>\n",
       "      <td>-0.389202</td>\n",
       "      <td>0.445992</td>\n",
       "      <td>1.543865</td>\n",
       "      <td>-1.294317</td>\n",
       "      <td>0.476577</td>\n",
       "      <td>-0.151629</td>\n",
       "      <td>-0.005974</td>\n",
       "      <td>0.959711</td>\n",
       "      <td>0.217443</td>\n",
       "      <td>0.651440</td>\n",
       "      <td>-0.827534</td>\n",
       "      <td>-0.017623</td>\n",
       "      <td>-0.301295</td>\n",
       "      <td>-0.650634</td>\n",
       "      <td>1.373814</td>\n",
       "      <td>0.663393</td>\n",
       "      <td>-0.100480</td>\n",
       "      <td>0.215191</td>\n",
       "      <td>0.341025</td>\n",
       "      <td>0.211980</td>\n",
       "      <td>0.476814</td>\n",
       "      <td>0.760928</td>\n",
       "      <td>13.755452</td>\n",
       "      <td>0.186910</td>\n",
       "      <td>0.575501</td>\n",
       "      <td>-0.100905</td>\n",
       "      <td>0.490973</td>\n",
       "      <td>-0.259051</td>\n",
       "      <td>-0.429931</td>\n",
       "      <td>-0.187163</td>\n",
       "      <td>-0.263229</td>\n",
       "      <td>0.319919</td>\n",
       "      <td>-0.742757</td>\n",
       "      <td>0.097327</td>\n",
       "      <td>-0.045151</td>\n",
       "      <td>0.962171</td>\n",
       "      <td>-0.564276</td>\n",
       "      <td>-0.627818</td>\n",
       "      <td>0.620934</td>\n",
       "      <td>0.727996</td>\n",
       "      <td>0.137582</td>\n",
       "      <td>-1.000852</td>\n",
       "      <td>0.295297</td>\n",
       "      <td>-0.410747</td>\n",
       "      <td>0.144109</td>\n",
       "      <td>-0.534853</td>\n",
       "      <td>-0.599668</td>\n",
       "      <td>-0.752876</td>\n",
       "      <td>-0.500221</td>\n",
       "      <td>-0.199155</td>\n",
       "      <td>-0.436160</td>\n",
       "      <td>0.667490</td>\n",
       "      <td>0.808029</td>\n",
       "      <td>0.354899</td>\n",
       "      <td>-0.810560</td>\n",
       "      <td>0.533803</td>\n",
       "      <td>-0.210651</td>\n",
       "      <td>0.139549</td>\n",
       "      <td>-0.306552</td>\n",
       "      <td>-0.276323</td>\n",
       "      <td>-0.176979</td>\n",
       "      <td>-0.080266</td>\n",
       "      <td>-0.448883</td>\n",
       "      <td>0.427966</td>\n",
       "      <td>-0.809047</td>\n",
       "      <td>0.095144</td>\n",
       "      <td>-1.091212</td>\n",
       "      <td>0.272929</td>\n",
       "      <td>-0.127293</td>\n",
       "      <td>0.178987</td>\n",
       "      <td>-0.230544</td>\n",
       "      <td>1.009167</td>\n",
       "      <td>-1.124698</td>\n",
       "      <td>-0.246905</td>\n",
       "      <td>-0.001277</td>\n",
       "      <td>0.824402</td>\n",
       "      <td>1.208642</td>\n",
       "      <td>-0.041223</td>\n",
       "      <td>-1.086967</td>\n",
       "      <td>0.008279</td>\n",
       "      <td>-0.059734</td>\n",
       "      <td>-0.218140</td>\n",
       "      <td>0.476682</td>\n",
       "      <td>-0.383100</td>\n",
       "      <td>0.568983</td>\n",
       "      <td>0.301135</td>\n",
       "      <td>-0.434543</td>\n",
       "      <td>-0.886003</td>\n",
       "      <td>0.891701</td>\n",
       "      <td>0.753125</td>\n",
       "      <td>-1.835825</td>\n",
       "      <td>-0.483442</td>\n",
       "      <td>0.528058</td>\n",
       "      <td>1.511796</td>\n",
       "      <td>1.142417</td>\n",
       "      <td>0.133046</td>\n",
       "      <td>-0.187076</td>\n",
       "      <td>-0.069194</td>\n",
       "      <td>0.017956</td>\n",
       "      <td>-0.104788</td>\n",
       "      <td>0.976280</td>\n",
       "      <td>1.128582</td>\n",
       "      <td>0.491830</td>\n",
       "      <td>-0.445289</td>\n",
       "      <td>-1.154046</td>\n",
       "      <td>0.415983</td>\n",
       "      <td>-0.341539</td>\n",
       "      <td>-0.161013</td>\n",
       "      <td>-1.475966</td>\n",
       "      <td>0.395354</td>\n",
       "      <td>0.499803</td>\n",
       "      <td>1.304428</td>\n",
       "      <td>-0.034853</td>\n",
       "      <td>-0.202827</td>\n",
       "      <td>-0.212101</td>\n",
       "      <td>0.054552</td>\n",
       "      <td>0.847058</td>\n",
       "      <td>-0.345506</td>\n",
       "      <td>-0.309624</td>\n",
       "      <td>-1.505085</td>\n",
       "      <td>0.461277</td>\n",
       "      <td>1.008005</td>\n",
       "      <td>0.038445</td>\n",
       "      <td>0.761725</td>\n",
       "      <td>0.315565</td>\n",
       "      <td>0.080530</td>\n",
       "      <td>-0.118607</td>\n",
       "      <td>-0.903829</td>\n",
       "      <td>0.620333</td>\n",
       "      <td>0.369704</td>\n",
       "      <td>-0.137612</td>\n",
       "      <td>0.353510</td>\n",
       "      <td>-0.180669</td>\n",
       "      <td>0.950037</td>\n",
       "      <td>0.077531</td>\n",
       "      <td>0.843435</td>\n",
       "      <td>0.516253</td>\n",
       "      <td>-0.121426</td>\n",
       "      <td>0.493003</td>\n",
       "      <td>0.014580</td>\n",
       "      <td>0.378812</td>\n",
       "      <td>0.023460</td>\n",
       "      <td>0.185424</td>\n",
       "      <td>-0.081403</td>\n",
       "      <td>-0.107268</td>\n",
       "      <td>0.317219</td>\n",
       "      <td>0.121431</td>\n",
       "      <td>0.039220</td>\n",
       "      <td>0.128190</td>\n",
       "      <td>-0.821351</td>\n",
       "      <td>-1.003214</td>\n",
       "      <td>0.196884</td>\n",
       "      <td>-0.872062</td>\n",
       "      <td>0.173263</td>\n",
       "      <td>1.201262</td>\n",
       "      <td>-0.250084</td>\n",
       "      <td>-0.189277</td>\n",
       "      <td>-0.336288</td>\n",
       "      <td>-0.555796</td>\n",
       "      <td>0.934918</td>\n",
       "      <td>0.559159</td>\n",
       "      <td>-0.095710</td>\n",
       "      <td>-0.517784</td>\n",
       "      <td>0.469165</td>\n",
       "      <td>-0.984998</td>\n",
       "      <td>-1.131007</td>\n",
       "      <td>-0.244719</td>\n",
       "      <td>0.791499</td>\n",
       "      <td>-0.572291</td>\n",
       "      <td>-0.146145</td>\n",
       "      <td>0.905546</td>\n",
       "      <td>-1.457422</td>\n",
       "      <td>2.260087</td>\n",
       "      <td>-0.166206</td>\n",
       "      <td>-0.636642</td>\n",
       "      <td>0.350568</td>\n",
       "      <td>-0.190369</td>\n",
       "      <td>-1.072349</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>-0.370968</td>\n",
       "      <td>0.860484</td>\n",
       "      <td>-1.261691</td>\n",
       "      <td>-0.327150</td>\n",
       "      <td>1.255396</td>\n",
       "      <td>1.209962</td>\n",
       "      <td>0.243347</td>\n",
       "      <td>0.384269</td>\n",
       "      <td>0.434268</td>\n",
       "      <td>0.611408</td>\n",
       "      <td>0.172822</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>-0.260490</td>\n",
       "      <td>0.045967</td>\n",
       "      <td>0.185595</td>\n",
       "      <td>-0.023985</td>\n",
       "      <td>-0.541200</td>\n",
       "      <td>0.514001</td>\n",
       "      <td>-1.352766</td>\n",
       "      <td>0.512786</td>\n",
       "      <td>-0.323352</td>\n",
       "      <td>0.649051</td>\n",
       "      <td>1.774142</td>\n",
       "      <td>0.869626</td>\n",
       "      <td>0.469016</td>\n",
       "      <td>0.098277</td>\n",
       "      <td>-1.127167</td>\n",
       "      <td>0.856277</td>\n",
       "      <td>0.822179</td>\n",
       "      <td>0.668918</td>\n",
       "      <td>0.856857</td>\n",
       "      <td>-0.527306</td>\n",
       "      <td>0.258218</td>\n",
       "      <td>-0.896307</td>\n",
       "      <td>0.704975</td>\n",
       "      <td>1.103452</td>\n",
       "      <td>0.366112</td>\n",
       "      <td>-0.136889</td>\n",
       "      <td>-0.868879</td>\n",
       "      <td>-0.114063</td>\n",
       "      <td>-1.350382</td>\n",
       "      <td>-0.917053</td>\n",
       "      <td>0.103647</td>\n",
       "      <td>-1.240219</td>\n",
       "      <td>-0.155452</td>\n",
       "      <td>0.564412</td>\n",
       "      <td>-0.051062</td>\n",
       "      <td>-0.800765</td>\n",
       "      <td>-0.131301</td>\n",
       "      <td>0.263243</td>\n",
       "      <td>0.237330</td>\n",
       "      <td>-0.587270</td>\n",
       "      <td>-1.327193</td>\n",
       "      <td>0.683113</td>\n",
       "      <td>0.417219</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.309212</td>\n",
       "      <td>0.910836</td>\n",
       "      <td>-0.273428</td>\n",
       "      <td>-0.190106</td>\n",
       "      <td>-0.002491</td>\n",
       "      <td>-0.064133</td>\n",
       "      <td>0.195051</td>\n",
       "      <td>-0.095962</td>\n",
       "      <td>-0.140747</td>\n",
       "      <td>-0.341383</td>\n",
       "      <td>-1.013542</td>\n",
       "      <td>-0.881881</td>\n",
       "      <td>-0.480772</td>\n",
       "      <td>-0.607087</td>\n",
       "      <td>0.905398</td>\n",
       "      <td>1.218029</td>\n",
       "      <td>-0.297764</td>\n",
       "      <td>-0.728446</td>\n",
       "      <td>1.194226</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>0.502102</td>\n",
       "      <td>-1.501001</td>\n",
       "      <td>1.037894</td>\n",
       "      <td>0.368634</td>\n",
       "      <td>0.747409</td>\n",
       "      <td>-0.081113</td>\n",
       "      <td>0.629598</td>\n",
       "      <td>0.661931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.81</td>\n",
       "      <td>-0.768326</td>\n",
       "      <td>-0.114626</td>\n",
       "      <td>-0.443230</td>\n",
       "      <td>-1.205777</td>\n",
       "      <td>0.394893</td>\n",
       "      <td>-1.317676</td>\n",
       "      <td>0.146579</td>\n",
       "      <td>-0.345761</td>\n",
       "      <td>-0.746383</td>\n",
       "      <td>0.023888</td>\n",
       "      <td>-0.251664</td>\n",
       "      <td>0.554369</td>\n",
       "      <td>-0.753446</td>\n",
       "      <td>-0.081943</td>\n",
       "      <td>1.188998</td>\n",
       "      <td>0.728303</td>\n",
       "      <td>0.434606</td>\n",
       "      <td>-1.367036</td>\n",
       "      <td>0.121019</td>\n",
       "      <td>-1.002043</td>\n",
       "      <td>-0.153040</td>\n",
       "      <td>0.268617</td>\n",
       "      <td>-0.417253</td>\n",
       "      <td>-1.109193</td>\n",
       "      <td>0.435450</td>\n",
       "      <td>0.927401</td>\n",
       "      <td>-1.125560</td>\n",
       "      <td>-0.377839</td>\n",
       "      <td>0.894653</td>\n",
       "      <td>0.188905</td>\n",
       "      <td>0.199088</td>\n",
       "      <td>-0.253376</td>\n",
       "      <td>-0.421375</td>\n",
       "      <td>-0.137637</td>\n",
       "      <td>-0.569846</td>\n",
       "      <td>-0.356317</td>\n",
       "      <td>-1.258567</td>\n",
       "      <td>0.480437</td>\n",
       "      <td>0.279175</td>\n",
       "      <td>0.076386</td>\n",
       "      <td>-0.364000</td>\n",
       "      <td>-0.114321</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>-0.085832</td>\n",
       "      <td>0.404673</td>\n",
       "      <td>-0.361076</td>\n",
       "      <td>0.005947</td>\n",
       "      <td>0.847276</td>\n",
       "      <td>0.008235</td>\n",
       "      <td>-0.839578</td>\n",
       "      <td>0.240574</td>\n",
       "      <td>-0.902698</td>\n",
       "      <td>-0.844472</td>\n",
       "      <td>0.261665</td>\n",
       "      <td>-0.252986</td>\n",
       "      <td>0.642920</td>\n",
       "      <td>-0.396456</td>\n",
       "      <td>0.458381</td>\n",
       "      <td>0.927028</td>\n",
       "      <td>0.287451</td>\n",
       "      <td>1.533246</td>\n",
       "      <td>-0.499297</td>\n",
       "      <td>0.835902</td>\n",
       "      <td>-0.107647</td>\n",
       "      <td>-1.103811</td>\n",
       "      <td>-0.931033</td>\n",
       "      <td>0.423953</td>\n",
       "      <td>0.104309</td>\n",
       "      <td>0.507302</td>\n",
       "      <td>-0.125528</td>\n",
       "      <td>0.555493</td>\n",
       "      <td>0.051604</td>\n",
       "      <td>0.491320</td>\n",
       "      <td>-0.141936</td>\n",
       "      <td>0.306437</td>\n",
       "      <td>-0.453313</td>\n",
       "      <td>0.234974</td>\n",
       "      <td>-0.106498</td>\n",
       "      <td>-0.700646</td>\n",
       "      <td>0.379784</td>\n",
       "      <td>0.279638</td>\n",
       "      <td>0.521046</td>\n",
       "      <td>1.053808</td>\n",
       "      <td>-1.875149</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>1.849109</td>\n",
       "      <td>-1.085073</td>\n",
       "      <td>-0.636803</td>\n",
       "      <td>0.276985</td>\n",
       "      <td>-0.236918</td>\n",
       "      <td>-2.630069</td>\n",
       "      <td>0.051414</td>\n",
       "      <td>0.714271</td>\n",
       "      <td>-0.324666</td>\n",
       "      <td>0.585293</td>\n",
       "      <td>-0.608533</td>\n",
       "      <td>-0.421758</td>\n",
       "      <td>-1.585008</td>\n",
       "      <td>-0.259042</td>\n",
       "      <td>0.917902</td>\n",
       "      <td>-0.056592</td>\n",
       "      <td>0.915347</td>\n",
       "      <td>0.088852</td>\n",
       "      <td>0.379784</td>\n",
       "      <td>0.088457</td>\n",
       "      <td>-0.253237</td>\n",
       "      <td>-0.444803</td>\n",
       "      <td>-1.235167</td>\n",
       "      <td>1.905975</td>\n",
       "      <td>0.609949</td>\n",
       "      <td>-0.038246</td>\n",
       "      <td>1.004645</td>\n",
       "      <td>-0.032408</td>\n",
       "      <td>-0.278034</td>\n",
       "      <td>0.617335</td>\n",
       "      <td>0.079395</td>\n",
       "      <td>1.255474</td>\n",
       "      <td>0.802006</td>\n",
       "      <td>-0.994091</td>\n",
       "      <td>-1.305712</td>\n",
       "      <td>0.316480</td>\n",
       "      <td>-0.530193</td>\n",
       "      <td>-1.006342</td>\n",
       "      <td>0.156448</td>\n",
       "      <td>-0.241241</td>\n",
       "      <td>0.889482</td>\n",
       "      <td>-0.356294</td>\n",
       "      <td>0.138224</td>\n",
       "      <td>-0.282094</td>\n",
       "      <td>1.087393</td>\n",
       "      <td>-0.381771</td>\n",
       "      <td>0.625440</td>\n",
       "      <td>0.136886</td>\n",
       "      <td>-0.941601</td>\n",
       "      <td>-0.135591</td>\n",
       "      <td>-0.210563</td>\n",
       "      <td>-0.363259</td>\n",
       "      <td>-1.218550</td>\n",
       "      <td>-0.871033</td>\n",
       "      <td>1.043329</td>\n",
       "      <td>0.141133</td>\n",
       "      <td>0.913745</td>\n",
       "      <td>-0.370625</td>\n",
       "      <td>0.380972</td>\n",
       "      <td>0.278348</td>\n",
       "      <td>-0.371374</td>\n",
       "      <td>0.491264</td>\n",
       "      <td>-0.633946</td>\n",
       "      <td>-0.004098</td>\n",
       "      <td>0.570679</td>\n",
       "      <td>0.854847</td>\n",
       "      <td>0.251489</td>\n",
       "      <td>-0.235313</td>\n",
       "      <td>0.563199</td>\n",
       "      <td>-0.093983</td>\n",
       "      <td>-0.399019</td>\n",
       "      <td>-0.275032</td>\n",
       "      <td>-1.207741</td>\n",
       "      <td>0.178902</td>\n",
       "      <td>-0.747077</td>\n",
       "      <td>-0.680045</td>\n",
       "      <td>-0.320574</td>\n",
       "      <td>1.086369</td>\n",
       "      <td>-0.703175</td>\n",
       "      <td>-0.159146</td>\n",
       "      <td>0.688365</td>\n",
       "      <td>0.493374</td>\n",
       "      <td>-0.268856</td>\n",
       "      <td>0.920584</td>\n",
       "      <td>-0.912291</td>\n",
       "      <td>-1.017921</td>\n",
       "      <td>-0.998971</td>\n",
       "      <td>-0.672669</td>\n",
       "      <td>0.106358</td>\n",
       "      <td>-0.581285</td>\n",
       "      <td>1.337888</td>\n",
       "      <td>0.305901</td>\n",
       "      <td>-0.673987</td>\n",
       "      <td>0.959041</td>\n",
       "      <td>1.176666</td>\n",
       "      <td>0.501457</td>\n",
       "      <td>0.713396</td>\n",
       "      <td>-0.201323</td>\n",
       "      <td>0.440816</td>\n",
       "      <td>-0.366045</td>\n",
       "      <td>0.194164</td>\n",
       "      <td>0.592310</td>\n",
       "      <td>0.468172</td>\n",
       "      <td>-0.544514</td>\n",
       "      <td>-0.214016</td>\n",
       "      <td>-1.341823</td>\n",
       "      <td>1.389505</td>\n",
       "      <td>0.762576</td>\n",
       "      <td>0.273410</td>\n",
       "      <td>0.763099</td>\n",
       "      <td>0.784652</td>\n",
       "      <td>-0.049475</td>\n",
       "      <td>0.862331</td>\n",
       "      <td>-0.281147</td>\n",
       "      <td>0.072881</td>\n",
       "      <td>-0.664718</td>\n",
       "      <td>-0.210628</td>\n",
       "      <td>-0.697912</td>\n",
       "      <td>-0.266265</td>\n",
       "      <td>-0.790932</td>\n",
       "      <td>-0.904315</td>\n",
       "      <td>0.259319</td>\n",
       "      <td>1.197455</td>\n",
       "      <td>0.554125</td>\n",
       "      <td>0.507206</td>\n",
       "      <td>-0.900422</td>\n",
       "      <td>-0.577060</td>\n",
       "      <td>-0.128074</td>\n",
       "      <td>0.691085</td>\n",
       "      <td>1.164617</td>\n",
       "      <td>-0.352429</td>\n",
       "      <td>-1.335013</td>\n",
       "      <td>-0.226352</td>\n",
       "      <td>-0.328443</td>\n",
       "      <td>-0.714022</td>\n",
       "      <td>-0.480458</td>\n",
       "      <td>-1.541269</td>\n",
       "      <td>-0.107138</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>-0.435257</td>\n",
       "      <td>0.660106</td>\n",
       "      <td>-0.888997</td>\n",
       "      <td>-0.727820</td>\n",
       "      <td>-0.212915</td>\n",
       "      <td>-0.067916</td>\n",
       "      <td>0.116845</td>\n",
       "      <td>0.304014</td>\n",
       "      <td>0.364218</td>\n",
       "      <td>-0.736963</td>\n",
       "      <td>-0.910938</td>\n",
       "      <td>-1.491185</td>\n",
       "      <td>-0.071996</td>\n",
       "      <td>-0.098205</td>\n",
       "      <td>-0.253417</td>\n",
       "      <td>-0.808210</td>\n",
       "      <td>-1.360476</td>\n",
       "      <td>0.554854</td>\n",
       "      <td>1.130804</td>\n",
       "      <td>-1.266634</td>\n",
       "      <td>0.161967</td>\n",
       "      <td>-0.714519</td>\n",
       "      <td>0.617150</td>\n",
       "      <td>-0.102353</td>\n",
       "      <td>0.160954</td>\n",
       "      <td>-0.903707</td>\n",
       "      <td>-0.043184</td>\n",
       "      <td>-0.262819</td>\n",
       "      <td>0.445695</td>\n",
       "      <td>2.103155</td>\n",
       "      <td>0.237230</td>\n",
       "      <td>0.233652</td>\n",
       "      <td>0.156792</td>\n",
       "      <td>1.288566</td>\n",
       "      <td>-0.160473</td>\n",
       "      <td>0.500384</td>\n",
       "      <td>0.726031</td>\n",
       "      <td>0.550212</td>\n",
       "      <td>-0.556284</td>\n",
       "      <td>0.349056</td>\n",
       "      <td>-0.755855</td>\n",
       "      <td>-0.873598</td>\n",
       "      <td>0.253216</td>\n",
       "      <td>-0.231266</td>\n",
       "      <td>-0.684444</td>\n",
       "      <td>1.132128</td>\n",
       "      <td>0.465914</td>\n",
       "      <td>-1.456617</td>\n",
       "      <td>-0.871438</td>\n",
       "      <td>-0.611602</td>\n",
       "      <td>-1.078190</td>\n",
       "      <td>-0.082172</td>\n",
       "      <td>-0.749148</td>\n",
       "      <td>-0.010166</td>\n",
       "      <td>1.413571</td>\n",
       "      <td>-0.573272</td>\n",
       "      <td>0.226502</td>\n",
       "      <td>-0.534144</td>\n",
       "      <td>-0.783868</td>\n",
       "      <td>-1.010550</td>\n",
       "      <td>0.693620</td>\n",
       "      <td>0.094775</td>\n",
       "      <td>0.060632</td>\n",
       "      <td>1.248427</td>\n",
       "      <td>0.893053</td>\n",
       "      <td>0.314744</td>\n",
       "      <td>-0.034301</td>\n",
       "      <td>0.649931</td>\n",
       "      <td>-0.625593</td>\n",
       "      <td>0.878793</td>\n",
       "      <td>-1.106819</td>\n",
       "      <td>1.184262</td>\n",
       "      <td>0.094658</td>\n",
       "      <td>-1.077212</td>\n",
       "      <td>0.828606</td>\n",
       "      <td>1.746933</td>\n",
       "      <td>-0.119415</td>\n",
       "      <td>0.267248</td>\n",
       "      <td>0.146313</td>\n",
       "      <td>0.774485</td>\n",
       "      <td>0.290367</td>\n",
       "      <td>-0.984069</td>\n",
       "      <td>0.163573</td>\n",
       "      <td>0.371077</td>\n",
       "      <td>0.173738</td>\n",
       "      <td>-0.540956</td>\n",
       "      <td>-0.984321</td>\n",
       "      <td>-0.057061</td>\n",
       "      <td>0.935040</td>\n",
       "      <td>-0.969775</td>\n",
       "      <td>-0.399637</td>\n",
       "      <td>-0.960755</td>\n",
       "      <td>-1.203546</td>\n",
       "      <td>0.413100</td>\n",
       "      <td>0.716661</td>\n",
       "      <td>0.533791</td>\n",
       "      <td>-0.288866</td>\n",
       "      <td>-0.127486</td>\n",
       "      <td>-1.605333</td>\n",
       "      <td>0.874082</td>\n",
       "      <td>0.395495</td>\n",
       "      <td>0.110656</td>\n",
       "      <td>-0.566782</td>\n",
       "      <td>-0.598744</td>\n",
       "      <td>0.098502</td>\n",
       "      <td>-0.275136</td>\n",
       "      <td>1.581839</td>\n",
       "      <td>1.119314</td>\n",
       "      <td>0.659908</td>\n",
       "      <td>1.122719</td>\n",
       "      <td>-0.220744</td>\n",
       "      <td>-0.052061</td>\n",
       "      <td>-0.471505</td>\n",
       "      <td>-0.247585</td>\n",
       "      <td>0.397779</td>\n",
       "      <td>-0.239707</td>\n",
       "      <td>-0.556417</td>\n",
       "      <td>0.809440</td>\n",
       "      <td>0.808680</td>\n",
       "      <td>-0.166546</td>\n",
       "      <td>-0.989780</td>\n",
       "      <td>-0.370013</td>\n",
       "      <td>-0.731616</td>\n",
       "      <td>-0.371112</td>\n",
       "      <td>-0.083160</td>\n",
       "      <td>0.149169</td>\n",
       "      <td>-0.133807</td>\n",
       "      <td>-0.518683</td>\n",
       "      <td>0.655918</td>\n",
       "      <td>-0.076505</td>\n",
       "      <td>-0.245805</td>\n",
       "      <td>-0.376895</td>\n",
       "      <td>-0.093865</td>\n",
       "      <td>1.073554</td>\n",
       "      <td>0.045067</td>\n",
       "      <td>0.159497</td>\n",
       "      <td>-0.268137</td>\n",
       "      <td>-0.931505</td>\n",
       "      <td>0.212943</td>\n",
       "      <td>0.737053</td>\n",
       "      <td>1.364571</td>\n",
       "      <td>1.270649</td>\n",
       "      <td>-0.439459</td>\n",
       "      <td>-0.560376</td>\n",
       "      <td>0.424254</td>\n",
       "      <td>0.223567</td>\n",
       "      <td>0.681101</td>\n",
       "      <td>-0.097223</td>\n",
       "      <td>0.290508</td>\n",
       "      <td>-0.637415</td>\n",
       "      <td>-0.051757</td>\n",
       "      <td>-0.591724</td>\n",
       "      <td>-0.877876</td>\n",
       "      <td>-0.800953</td>\n",
       "      <td>-0.202127</td>\n",
       "      <td>-0.522013</td>\n",
       "      <td>0.545234</td>\n",
       "      <td>-1.770258</td>\n",
       "      <td>0.136041</td>\n",
       "      <td>-0.135616</td>\n",
       "      <td>-0.375873</td>\n",
       "      <td>0.252454</td>\n",
       "      <td>-0.387679</td>\n",
       "      <td>-0.624776</td>\n",
       "      <td>-0.700880</td>\n",
       "      <td>-0.439825</td>\n",
       "      <td>-1.394108</td>\n",
       "      <td>-0.004928</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>0.232922</td>\n",
       "      <td>0.446101</td>\n",
       "      <td>-0.534080</td>\n",
       "      <td>-0.354860</td>\n",
       "      <td>1.211094</td>\n",
       "      <td>0.385074</td>\n",
       "      <td>-0.237100</td>\n",
       "      <td>-1.032569</td>\n",
       "      <td>0.162628</td>\n",
       "      <td>-0.323247</td>\n",
       "      <td>-1.125692</td>\n",
       "      <td>-0.601831</td>\n",
       "      <td>-0.843181</td>\n",
       "      <td>0.122472</td>\n",
       "      <td>0.285022</td>\n",
       "      <td>0.599703</td>\n",
       "      <td>-0.849793</td>\n",
       "      <td>-0.508561</td>\n",
       "      <td>-0.715026</td>\n",
       "      <td>-0.021927</td>\n",
       "      <td>0.618948</td>\n",
       "      <td>0.970517</td>\n",
       "      <td>-0.790941</td>\n",
       "      <td>-0.099554</td>\n",
       "      <td>0.839952</td>\n",
       "      <td>-1.081604</td>\n",
       "      <td>0.165439</td>\n",
       "      <td>-2.124782</td>\n",
       "      <td>0.013491</td>\n",
       "      <td>0.294583</td>\n",
       "      <td>-0.903626</td>\n",
       "      <td>1.533997</td>\n",
       "      <td>0.394492</td>\n",
       "      <td>0.236793</td>\n",
       "      <td>0.096652</td>\n",
       "      <td>-0.303652</td>\n",
       "      <td>-0.345615</td>\n",
       "      <td>-0.457255</td>\n",
       "      <td>-0.893469</td>\n",
       "      <td>1.135524</td>\n",
       "      <td>-0.307801</td>\n",
       "      <td>-0.340425</td>\n",
       "      <td>1.809699</td>\n",
       "      <td>-0.429004</td>\n",
       "      <td>-0.172216</td>\n",
       "      <td>-0.515150</td>\n",
       "      <td>0.580757</td>\n",
       "      <td>0.585405</td>\n",
       "      <td>1.051507</td>\n",
       "      <td>-0.789916</td>\n",
       "      <td>0.950449</td>\n",
       "      <td>0.669788</td>\n",
       "      <td>0.263933</td>\n",
       "      <td>-0.627923</td>\n",
       "      <td>-0.128958</td>\n",
       "      <td>0.091977</td>\n",
       "      <td>-1.490052</td>\n",
       "      <td>-1.174419</td>\n",
       "      <td>-0.814766</td>\n",
       "      <td>-0.063884</td>\n",
       "      <td>0.262932</td>\n",
       "      <td>0.132131</td>\n",
       "      <td>0.557756</td>\n",
       "      <td>0.247557</td>\n",
       "      <td>-0.560080</td>\n",
       "      <td>0.556593</td>\n",
       "      <td>0.058634</td>\n",
       "      <td>0.825289</td>\n",
       "      <td>0.085803</td>\n",
       "      <td>-1.481190</td>\n",
       "      <td>0.478098</td>\n",
       "      <td>0.802811</td>\n",
       "      <td>-0.338320</td>\n",
       "      <td>-0.568944</td>\n",
       "      <td>0.333436</td>\n",
       "      <td>-0.947319</td>\n",
       "      <td>-1.004601</td>\n",
       "      <td>-1.710660</td>\n",
       "      <td>1.005762</td>\n",
       "      <td>-0.026395</td>\n",
       "      <td>-0.064325</td>\n",
       "      <td>0.595014</td>\n",
       "      <td>-0.280105</td>\n",
       "      <td>0.378764</td>\n",
       "      <td>-0.064372</td>\n",
       "      <td>-1.211483</td>\n",
       "      <td>0.139155</td>\n",
       "      <td>-0.570510</td>\n",
       "      <td>0.491161</td>\n",
       "      <td>-0.132459</td>\n",
       "      <td>-0.875530</td>\n",
       "      <td>1.046322</td>\n",
       "      <td>0.204486</td>\n",
       "      <td>0.171266</td>\n",
       "      <td>-0.651145</td>\n",
       "      <td>0.234697</td>\n",
       "      <td>-0.146406</td>\n",
       "      <td>0.399243</td>\n",
       "      <td>-0.084343</td>\n",
       "      <td>-0.725493</td>\n",
       "      <td>-0.180831</td>\n",
       "      <td>-0.382925</td>\n",
       "      <td>-0.235546</td>\n",
       "      <td>0.378893</td>\n",
       "      <td>-0.058852</td>\n",
       "      <td>-0.439174</td>\n",
       "      <td>-0.765985</td>\n",
       "      <td>-0.061546</td>\n",
       "      <td>-0.419903</td>\n",
       "      <td>-0.761329</td>\n",
       "      <td>-0.078045</td>\n",
       "      <td>0.266731</td>\n",
       "      <td>-0.337940</td>\n",
       "      <td>0.595525</td>\n",
       "      <td>1.535963</td>\n",
       "      <td>-1.170741</td>\n",
       "      <td>0.409035</td>\n",
       "      <td>-0.220448</td>\n",
       "      <td>0.397286</td>\n",
       "      <td>1.116590</td>\n",
       "      <td>0.312850</td>\n",
       "      <td>0.608619</td>\n",
       "      <td>-0.994447</td>\n",
       "      <td>0.194232</td>\n",
       "      <td>-0.228496</td>\n",
       "      <td>-0.662745</td>\n",
       "      <td>1.404307</td>\n",
       "      <td>0.535887</td>\n",
       "      <td>-0.060965</td>\n",
       "      <td>0.266274</td>\n",
       "      <td>0.300520</td>\n",
       "      <td>0.206439</td>\n",
       "      <td>0.499246</td>\n",
       "      <td>0.871262</td>\n",
       "      <td>13.833971</td>\n",
       "      <td>0.202159</td>\n",
       "      <td>0.397264</td>\n",
       "      <td>-0.220723</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>-0.279842</td>\n",
       "      <td>-0.555698</td>\n",
       "      <td>-0.157215</td>\n",
       "      <td>-0.334988</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>-0.575318</td>\n",
       "      <td>0.171688</td>\n",
       "      <td>-0.298960</td>\n",
       "      <td>0.957531</td>\n",
       "      <td>-0.654536</td>\n",
       "      <td>-0.836525</td>\n",
       "      <td>0.567723</td>\n",
       "      <td>0.855264</td>\n",
       "      <td>-0.045613</td>\n",
       "      <td>-0.843105</td>\n",
       "      <td>0.418878</td>\n",
       "      <td>-0.389147</td>\n",
       "      <td>0.166774</td>\n",
       "      <td>-0.528646</td>\n",
       "      <td>-0.713763</td>\n",
       "      <td>-0.860247</td>\n",
       "      <td>-0.622041</td>\n",
       "      <td>-0.243481</td>\n",
       "      <td>-0.567965</td>\n",
       "      <td>0.493762</td>\n",
       "      <td>0.726090</td>\n",
       "      <td>0.366698</td>\n",
       "      <td>-0.870929</td>\n",
       "      <td>0.441909</td>\n",
       "      <td>-0.177847</td>\n",
       "      <td>0.141987</td>\n",
       "      <td>-0.138090</td>\n",
       "      <td>-0.326323</td>\n",
       "      <td>-0.252565</td>\n",
       "      <td>-0.042827</td>\n",
       "      <td>-0.373727</td>\n",
       "      <td>0.287655</td>\n",
       "      <td>-0.794317</td>\n",
       "      <td>0.053010</td>\n",
       "      <td>-1.202370</td>\n",
       "      <td>0.158420</td>\n",
       "      <td>-0.121022</td>\n",
       "      <td>0.080395</td>\n",
       "      <td>-0.219004</td>\n",
       "      <td>1.088925</td>\n",
       "      <td>-1.097160</td>\n",
       "      <td>-0.289474</td>\n",
       "      <td>0.162054</td>\n",
       "      <td>0.804853</td>\n",
       "      <td>1.337167</td>\n",
       "      <td>0.014690</td>\n",
       "      <td>-0.996204</td>\n",
       "      <td>0.123476</td>\n",
       "      <td>0.061626</td>\n",
       "      <td>-0.147490</td>\n",
       "      <td>0.418737</td>\n",
       "      <td>-0.128772</td>\n",
       "      <td>0.676014</td>\n",
       "      <td>0.201658</td>\n",
       "      <td>-0.390985</td>\n",
       "      <td>-0.868287</td>\n",
       "      <td>0.897845</td>\n",
       "      <td>0.941632</td>\n",
       "      <td>-1.856855</td>\n",
       "      <td>-0.682957</td>\n",
       "      <td>0.240994</td>\n",
       "      <td>1.229441</td>\n",
       "      <td>1.104259</td>\n",
       "      <td>0.197969</td>\n",
       "      <td>-0.281820</td>\n",
       "      <td>-0.153577</td>\n",
       "      <td>-0.060661</td>\n",
       "      <td>-0.003830</td>\n",
       "      <td>0.976371</td>\n",
       "      <td>0.880805</td>\n",
       "      <td>0.299838</td>\n",
       "      <td>-0.247525</td>\n",
       "      <td>-1.036887</td>\n",
       "      <td>0.442813</td>\n",
       "      <td>-0.292170</td>\n",
       "      <td>0.126248</td>\n",
       "      <td>-1.514624</td>\n",
       "      <td>0.424401</td>\n",
       "      <td>0.234528</td>\n",
       "      <td>1.300573</td>\n",
       "      <td>-0.017075</td>\n",
       "      <td>-0.293650</td>\n",
       "      <td>0.196987</td>\n",
       "      <td>0.031474</td>\n",
       "      <td>0.771320</td>\n",
       "      <td>-0.110691</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-1.414609</td>\n",
       "      <td>0.421740</td>\n",
       "      <td>1.064328</td>\n",
       "      <td>0.037880</td>\n",
       "      <td>0.667590</td>\n",
       "      <td>0.196636</td>\n",
       "      <td>0.041633</td>\n",
       "      <td>-0.269171</td>\n",
       "      <td>-0.646298</td>\n",
       "      <td>0.598211</td>\n",
       "      <td>0.340289</td>\n",
       "      <td>-0.213025</td>\n",
       "      <td>0.390018</td>\n",
       "      <td>-0.119403</td>\n",
       "      <td>0.929268</td>\n",
       "      <td>0.042302</td>\n",
       "      <td>0.799159</td>\n",
       "      <td>0.631597</td>\n",
       "      <td>-0.425722</td>\n",
       "      <td>0.363460</td>\n",
       "      <td>0.030877</td>\n",
       "      <td>0.171625</td>\n",
       "      <td>0.270233</td>\n",
       "      <td>0.316210</td>\n",
       "      <td>-0.154966</td>\n",
       "      <td>-0.149342</td>\n",
       "      <td>0.374820</td>\n",
       "      <td>0.277414</td>\n",
       "      <td>-0.014272</td>\n",
       "      <td>0.081079</td>\n",
       "      <td>-0.732452</td>\n",
       "      <td>-1.082934</td>\n",
       "      <td>0.222616</td>\n",
       "      <td>-0.899667</td>\n",
       "      <td>0.185834</td>\n",
       "      <td>0.939747</td>\n",
       "      <td>-0.323772</td>\n",
       "      <td>-0.151285</td>\n",
       "      <td>-0.419598</td>\n",
       "      <td>-0.602493</td>\n",
       "      <td>0.644153</td>\n",
       "      <td>0.372283</td>\n",
       "      <td>-0.273051</td>\n",
       "      <td>-0.588839</td>\n",
       "      <td>0.398777</td>\n",
       "      <td>-1.052181</td>\n",
       "      <td>-0.893190</td>\n",
       "      <td>0.074296</td>\n",
       "      <td>0.977648</td>\n",
       "      <td>-0.363500</td>\n",
       "      <td>-0.136151</td>\n",
       "      <td>0.862840</td>\n",
       "      <td>-1.556968</td>\n",
       "      <td>2.429061</td>\n",
       "      <td>-0.168989</td>\n",
       "      <td>-0.561527</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>-0.089533</td>\n",
       "      <td>-1.040147</td>\n",
       "      <td>0.236964</td>\n",
       "      <td>-0.472911</td>\n",
       "      <td>0.857241</td>\n",
       "      <td>-1.358730</td>\n",
       "      <td>-0.202116</td>\n",
       "      <td>1.071472</td>\n",
       "      <td>1.144902</td>\n",
       "      <td>0.375883</td>\n",
       "      <td>0.347154</td>\n",
       "      <td>0.276868</td>\n",
       "      <td>0.587678</td>\n",
       "      <td>0.353309</td>\n",
       "      <td>-0.145163</td>\n",
       "      <td>-0.243819</td>\n",
       "      <td>-0.136560</td>\n",
       "      <td>0.516772</td>\n",
       "      <td>-0.094067</td>\n",
       "      <td>-0.435980</td>\n",
       "      <td>0.474158</td>\n",
       "      <td>-1.319113</td>\n",
       "      <td>0.304544</td>\n",
       "      <td>-0.496666</td>\n",
       "      <td>0.357485</td>\n",
       "      <td>1.791377</td>\n",
       "      <td>0.839931</td>\n",
       "      <td>0.384224</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>-1.024778</td>\n",
       "      <td>1.006014</td>\n",
       "      <td>0.906514</td>\n",
       "      <td>0.815624</td>\n",
       "      <td>0.691230</td>\n",
       "      <td>-0.399412</td>\n",
       "      <td>0.384132</td>\n",
       "      <td>-0.739347</td>\n",
       "      <td>0.684005</td>\n",
       "      <td>1.156261</td>\n",
       "      <td>0.443427</td>\n",
       "      <td>-0.418569</td>\n",
       "      <td>-0.934572</td>\n",
       "      <td>0.043966</td>\n",
       "      <td>-1.243049</td>\n",
       "      <td>-0.774558</td>\n",
       "      <td>0.215238</td>\n",
       "      <td>-1.097093</td>\n",
       "      <td>-0.040102</td>\n",
       "      <td>0.619841</td>\n",
       "      <td>0.075169</td>\n",
       "      <td>-0.822426</td>\n",
       "      <td>-0.083557</td>\n",
       "      <td>0.169311</td>\n",
       "      <td>0.124444</td>\n",
       "      <td>-0.488816</td>\n",
       "      <td>-1.325683</td>\n",
       "      <td>0.749879</td>\n",
       "      <td>0.395323</td>\n",
       "      <td>0.917498</td>\n",
       "      <td>0.295289</td>\n",
       "      <td>0.862854</td>\n",
       "      <td>-0.068503</td>\n",
       "      <td>-0.331440</td>\n",
       "      <td>0.049661</td>\n",
       "      <td>-0.274055</td>\n",
       "      <td>0.314875</td>\n",
       "      <td>0.017540</td>\n",
       "      <td>-0.015881</td>\n",
       "      <td>-0.408418</td>\n",
       "      <td>-1.173379</td>\n",
       "      <td>-0.878732</td>\n",
       "      <td>-0.373150</td>\n",
       "      <td>-0.524417</td>\n",
       "      <td>0.895135</td>\n",
       "      <td>1.139725</td>\n",
       "      <td>-0.247240</td>\n",
       "      <td>-0.661082</td>\n",
       "      <td>1.219325</td>\n",
       "      <td>0.185854</td>\n",
       "      <td>0.283039</td>\n",
       "      <td>-1.495237</td>\n",
       "      <td>1.063481</td>\n",
       "      <td>0.349460</td>\n",
       "      <td>0.709531</td>\n",
       "      <td>0.101135</td>\n",
       "      <td>0.548717</td>\n",
       "      <td>0.565705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elongation_value         0         1         2         3         4  \\\n",
       "0             41.73 -0.916031 -0.127834 -0.412525 -1.148250  0.397523   \n",
       "1             23.58 -0.769254 -0.110520 -0.435552 -1.213306  0.333992   \n",
       "2             37.46 -0.749443 -0.098571 -0.366157 -1.203457  0.373798   \n",
       "3             21.40 -0.857063 -0.197781 -0.360755 -1.130626  0.433399   \n",
       "4             24.81 -0.768326 -0.114626 -0.443230 -1.205777  0.394893   \n",
       "\n",
       "          5         6         7         8         9        10        11  \\\n",
       "0 -1.339542  0.413916 -0.331999 -0.839937 -0.017726 -0.237142  0.552550   \n",
       "1 -1.318727  0.145431 -0.358220 -0.788955  0.019995 -0.265530  0.570121   \n",
       "2 -1.328080  0.285747 -0.310203 -0.733059 -0.003385 -0.251482  0.566337   \n",
       "3 -1.356162  0.274503 -0.223516 -0.794815 -0.018643 -0.045138  0.623520   \n",
       "4 -1.317676  0.146579 -0.345761 -0.746383  0.023888 -0.251664  0.554369   \n",
       "\n",
       "         12        13        14        15        16        17        18  \\\n",
       "0 -0.703550 -0.196383  0.964938  0.763047  0.408478 -1.437083  0.081418   \n",
       "1 -0.785721 -0.091056  1.174088  0.734770  0.395986 -1.373323  0.114723   \n",
       "2 -0.795328 -0.147824  1.088557  0.662047  0.426646 -1.387048  0.135401   \n",
       "3 -0.529238 -0.181038  0.889952  0.692126  0.456344 -1.510397  0.002298   \n",
       "4 -0.753446 -0.081943  1.188998  0.728303  0.434606 -1.367036  0.121019   \n",
       "\n",
       "         19        20        21        22        23        24        25  \\\n",
       "0 -1.040954 -0.242027  0.237640 -0.416142 -1.184805  0.471824  0.912598   \n",
       "1 -1.023738 -0.200627  0.218253 -0.401422 -1.050738  0.424351  0.935047   \n",
       "2 -1.020397 -0.247882  0.246379 -0.445516 -1.141686  0.456293  0.951081   \n",
       "3 -0.990889 -0.245464  0.255041 -0.565098 -1.127335  0.449178  0.737562   \n",
       "4 -1.002043 -0.153040  0.268617 -0.417253 -1.109193  0.435450  0.927401   \n",
       "\n",
       "         26        27        28        29        30        31        32  \\\n",
       "0 -1.178648 -0.339284  0.987294  0.131147  0.404637 -0.291238 -0.534806   \n",
       "1 -1.177028 -0.371224  0.908881  0.170098  0.186938 -0.317118 -0.452783   \n",
       "2 -1.196160 -0.395614  0.893527  0.154916  0.274923 -0.310512 -0.504186   \n",
       "3 -1.282062 -0.299809  1.034325  0.083715  0.291678 -0.301196 -0.608537   \n",
       "4 -1.125560 -0.377839  0.894653  0.188905  0.199088 -0.253376 -0.421375   \n",
       "\n",
       "         33        34        35        36        37        38        39  \\\n",
       "0 -0.136035 -0.561124 -0.430609 -1.161139  0.540728  0.158976  0.127382   \n",
       "1 -0.103247 -0.510493 -0.389673 -1.214257  0.490864  0.273634  0.087987   \n",
       "2 -0.100960 -0.526614 -0.361057 -1.240552  0.533958  0.207455  0.118717   \n",
       "3 -0.120803 -0.577738 -0.561694 -0.898042  0.638654  0.221743  0.123853   \n",
       "4 -0.137637 -0.569846 -0.356317 -1.258567  0.480437  0.279175  0.076386   \n",
       "\n",
       "         40        41        42        43        44        45        46  \\\n",
       "0 -0.515667 -0.194996 -0.083271  0.051068  0.357276 -0.439148  0.012219   \n",
       "1 -0.438402 -0.125063  0.043757 -0.132434  0.417061 -0.381706 -0.023056   \n",
       "2 -0.408524 -0.216827  0.001501 -0.032500  0.409882 -0.355038 -0.036663   \n",
       "3 -0.475295 -0.105491  0.001198  0.151222  0.246273 -0.497533  0.005460   \n",
       "4 -0.364000 -0.114321  0.078446 -0.085832  0.404673 -0.361076  0.005947   \n",
       "\n",
       "         47        48        49        50        51        52        53  \\\n",
       "0  0.885475  0.116124 -0.836317  0.382456 -0.985915 -0.827866  0.248264   \n",
       "1  0.911881  0.028548 -0.842582  0.326267 -0.870367 -0.874450  0.291878   \n",
       "2  0.947852  0.015555 -0.898710  0.332785 -0.897706 -0.791212  0.269955   \n",
       "3  0.790434  0.019636 -0.729609  0.364228 -0.898165 -0.677331  0.213105   \n",
       "4  0.847276  0.008235 -0.839578  0.240574 -0.902698 -0.844472  0.261665   \n",
       "\n",
       "         54        55        56        57        58        59        60  \\\n",
       "0 -0.176879  0.748900 -0.294604  0.388489  0.880481  0.315506  1.552602   \n",
       "1 -0.271213  0.588776 -0.399369  0.460631  0.911604  0.279139  1.530984   \n",
       "2 -0.231708  0.751040 -0.361495  0.499342  0.883306  0.289871  1.459861   \n",
       "3 -0.205531  0.634712 -0.394036  0.373231  0.851021  0.416084  1.469023   \n",
       "4 -0.252986  0.642920 -0.396456  0.458381  0.927028  0.287451  1.533246   \n",
       "\n",
       "         61        62        63        64        65        66        67  \\\n",
       "0 -0.475215  0.805777 -0.029898 -1.077119 -0.866457  0.293282 -0.007556   \n",
       "1 -0.494210  0.867013 -0.091208 -1.103582 -0.940434  0.380958  0.080520   \n",
       "2 -0.511196  0.860112 -0.048901 -1.069134 -0.925358  0.325237  0.053864   \n",
       "3 -0.530620  0.815946 -0.027941 -1.031988 -0.899010  0.164482  0.030425   \n",
       "4 -0.499297  0.835902 -0.107647 -1.103811 -0.931033  0.423953  0.104309   \n",
       "\n",
       "         68        69        70        71        72        73        74  \\\n",
       "0  0.541936  0.016930  0.378152  0.142043  0.456180 -0.084118  0.352744   \n",
       "1  0.490296 -0.088894  0.577815  0.054718  0.488963 -0.104318  0.331916   \n",
       "2  0.549295 -0.091663  0.421659  0.050689  0.436353 -0.090495  0.326694   \n",
       "3  0.545800  0.178033  0.372368  0.109837  0.610416  0.021682  0.520571   \n",
       "4  0.507302 -0.125528  0.555493  0.051604  0.491320 -0.141936  0.306437   \n",
       "\n",
       "         75        76        77        78        79        80        81  \\\n",
       "0 -0.443961  0.123427 -0.109332 -0.707241  0.336127  0.325942  0.590749   \n",
       "1 -0.399748  0.254990 -0.105435 -0.622705  0.362436  0.303355  0.485840   \n",
       "2 -0.479450  0.253113 -0.164375 -0.706655  0.323677  0.347592  0.505570   \n",
       "3 -0.567462  0.143227 -0.047161 -0.688618  0.349195  0.276282  0.656060   \n",
       "4 -0.453313  0.234974 -0.106498 -0.700646  0.379784  0.279638  0.521046   \n",
       "\n",
       "         82        83        84        85        86        87        88  \\\n",
       "0  1.251289 -1.884276 -0.172657  1.881408 -1.107557 -0.589994  0.271838   \n",
       "1  1.052350 -1.925727 -0.018693  1.878029 -1.045059 -0.622814  0.314003   \n",
       "2  1.129843 -1.871729 -0.074374  1.867762 -1.005984 -0.633077  0.326487   \n",
       "3  1.112799 -1.839343 -0.209492  1.846800 -0.872475 -0.586209  0.391285   \n",
       "4  1.053808 -1.875149  0.011796  1.849109 -1.085073 -0.636803  0.276985   \n",
       "\n",
       "         89        90        91        92        93        94        95  \\\n",
       "0 -0.179294 -2.616996 -0.034784  0.754296 -0.333799  0.478040 -0.462951   \n",
       "1 -0.157367 -2.626469 -0.014078  0.751633 -0.309571  0.609824 -0.659410   \n",
       "2 -0.193302 -2.614760  0.048211  0.730214 -0.311367  0.484482 -0.576028   \n",
       "3 -0.292442 -2.655720 -0.057521  0.755614 -0.230552  0.420515 -0.462748   \n",
       "4 -0.236918 -2.630069  0.051414  0.714271 -0.324666  0.585293 -0.608533   \n",
       "\n",
       "         96        97        98        99       100       101       102  \\\n",
       "0 -0.434281 -1.663120 -0.357730  0.856380 -0.111549  0.935525  0.180116   \n",
       "1 -0.434214 -1.617741 -0.295857  0.893189 -0.029646  0.856420  0.063499   \n",
       "2 -0.472646 -1.601808 -0.331677  0.897003 -0.044937  0.907187  0.146517   \n",
       "3 -0.454426 -1.671276 -0.438877  0.803473 -0.238716  0.939506  0.116816   \n",
       "4 -0.421758 -1.585008 -0.259042  0.917902 -0.056592  0.915347  0.088852   \n",
       "\n",
       "        103       104       105       106       107       108       109  \\\n",
       "0  0.325657  0.052699 -0.351309 -0.472753 -1.405189  1.926005  0.610800   \n",
       "1  0.351936  0.032566 -0.247557 -0.478003 -1.229001  1.917317  0.598785   \n",
       "2  0.340527  0.023316 -0.272011 -0.509054 -1.290567  1.870402  0.590044   \n",
       "3  0.406810  0.249497 -0.245721 -0.482370 -1.358497  2.073709  0.452553   \n",
       "4  0.379784  0.088457 -0.253237 -0.444803 -1.235167  1.905975  0.609949   \n",
       "\n",
       "        110       111       112       113       114       115       116  \\\n",
       "0 -0.034712  0.864820  0.068866 -0.338850  0.528926  0.259941  1.278410   \n",
       "1 -0.094997  0.943426 -0.009482 -0.250427  0.553012  0.099468  1.290062   \n",
       "2 -0.046778  0.922202  0.010045 -0.258700  0.598631  0.198408  1.260956   \n",
       "3  0.021792  0.924943  0.071360 -0.207858  0.483160  0.295753  1.241012   \n",
       "4 -0.038246  1.004645 -0.032408 -0.278034  0.617335  0.079395  1.255474   \n",
       "\n",
       "        117       118       119       120       121       122       123  \\\n",
       "0  0.794152 -1.114567 -1.409564  0.410061 -0.541645 -1.011619  0.224822   \n",
       "1  0.771183 -0.946937 -1.310587  0.289362 -0.556137 -0.916540  0.178835   \n",
       "2  0.771179 -1.110048 -1.324780  0.350748 -0.580909 -0.989055  0.094367   \n",
       "3  0.662258 -1.036990 -1.350398  0.398695 -0.566180 -1.041174  0.281467   \n",
       "4  0.802006 -0.994091 -1.305712  0.316480 -0.530193 -1.006342  0.156448   \n",
       "\n",
       "        124       125       126       127       128       129       130  \\\n",
       "0 -0.347563  0.756954 -0.383644  0.156032 -0.375340  1.037328 -0.471992   \n",
       "1 -0.241169  0.913484 -0.351088  0.207845 -0.331350  1.061864 -0.432438   \n",
       "2 -0.237820  0.855156 -0.384294  0.183543 -0.306839  1.057364 -0.465207   \n",
       "3 -0.208438  0.749123 -0.467282  0.171198 -0.387580  1.164927 -0.580411   \n",
       "4 -0.241241  0.889482 -0.356294  0.138224 -0.282094  1.087393 -0.381771   \n",
       "\n",
       "        131       132       133       134       135       136       137  \\\n",
       "0  0.772341  0.096003 -0.950419 -0.046104 -0.171486 -0.393246 -1.251333   \n",
       "1  0.614693  0.166565 -0.969748 -0.090778 -0.246744 -0.317533 -1.176315   \n",
       "2  0.630526  0.076224 -0.930877 -0.124122 -0.195771 -0.361058 -1.219783   \n",
       "3  0.779298  0.024115 -0.993138  0.055619 -0.181455 -0.389649 -1.278570   \n",
       "4  0.625440  0.136886 -0.941601 -0.135591 -0.210563 -0.363259 -1.218550   \n",
       "\n",
       "        138       139       140       141       142       143       144  \\\n",
       "0 -0.877932  0.965971  0.209952  0.803545 -0.279004  0.416956  0.488927   \n",
       "1 -0.874115  1.039821  0.197554  0.923907 -0.375042  0.438759  0.306860   \n",
       "2 -0.855134  0.957495  0.155511  0.853077 -0.325408  0.375912  0.333384   \n",
       "3 -0.997796  0.919705  0.124421  0.840801 -0.232948  0.243837  0.711164   \n",
       "4 -0.871033  1.043329  0.141133  0.913745 -0.370625  0.380972  0.278348   \n",
       "\n",
       "        145       146       147       148       149       150       151  \\\n",
       "0 -0.368015  0.556807 -0.499697 -0.001963  0.471412  0.764955  0.172411   \n",
       "1 -0.358336  0.469697 -0.664448 -0.017813  0.663424  0.849352  0.285440   \n",
       "2 -0.342820  0.491277 -0.565383 -0.013225  0.527245  0.798573  0.281680   \n",
       "3 -0.278361  0.606251 -0.542213  0.110271  0.507082  0.887166  0.120492   \n",
       "4 -0.371374  0.491264 -0.633946 -0.004098  0.570679  0.854847  0.251489   \n",
       "\n",
       "        152       153       154       155       156       157       158  \\\n",
       "0 -0.077837  0.619746 -0.104510 -0.279822 -0.446809 -1.110106  0.099923   \n",
       "1 -0.255188  0.544138 -0.138618 -0.428508 -0.296978 -1.145255  0.132171   \n",
       "2 -0.228671  0.633387 -0.108102 -0.389363 -0.387097 -1.219848  0.186485   \n",
       "3 -0.241631  0.713809 -0.038259 -0.083793 -0.321732 -1.124533  0.174437   \n",
       "4 -0.235313  0.563199 -0.093983 -0.399019 -0.275032 -1.207741  0.178902   \n",
       "\n",
       "        159       160       161       162       163       164       165  \\\n",
       "0 -0.774132 -0.616423 -0.429239  0.999981 -0.701345  0.038776  0.685462   \n",
       "1 -0.771986 -0.653468 -0.304016  1.058321 -0.700630 -0.116372  0.740907   \n",
       "2 -0.721129 -0.703561 -0.316796  1.052738 -0.775077 -0.083361  0.671369   \n",
       "3 -0.722111 -0.563352 -0.280626  0.999416 -0.696046  0.096124  0.762789   \n",
       "4 -0.747077 -0.680045 -0.320574  1.086369 -0.703175 -0.159146  0.688365   \n",
       "\n",
       "        166       167       168       169       170       171       172  \\\n",
       "0  0.514711 -0.315413  0.729795 -0.894981 -0.838126 -1.053222 -0.674418   \n",
       "1  0.435352 -0.259140  0.864672 -0.906711 -1.065525 -0.916127 -0.655933   \n",
       "2  0.531359 -0.297876  0.855593 -0.853100 -0.955371 -0.994432 -0.639273   \n",
       "3  0.437614 -0.162628  0.658576 -0.888619 -0.937174 -1.002537 -0.620025   \n",
       "4  0.493374 -0.268856  0.920584 -0.912291 -1.017921 -0.998971 -0.672669   \n",
       "\n",
       "        173       174       175       176       177       178       179  \\\n",
       "0 -0.048791 -0.641334  1.288083  0.290813 -0.590914  0.936593  1.334001   \n",
       "1  0.122071 -0.639361  1.283238  0.300518 -0.726053  0.953515  1.213310   \n",
       "2  0.012047 -0.601856  1.293677  0.303514 -0.665189  0.959418  1.310877   \n",
       "3 -0.037638 -0.497662  1.239712  0.218228 -0.435386  1.177694  1.453738   \n",
       "4  0.106358 -0.581285  1.337888  0.305901 -0.673987  0.959041  1.176666   \n",
       "\n",
       "        180       181       182       183       184       185       186  \\\n",
       "0  0.530370  0.796496 -0.193131  0.246324 -0.305265  0.067173  0.584022   \n",
       "1  0.491617  0.754326 -0.140990  0.369727 -0.336232  0.226733  0.576442   \n",
       "2  0.501711  0.770059 -0.176757  0.360714 -0.308157  0.211676  0.587807   \n",
       "3  0.621714  0.707822 -0.222358  0.218179 -0.203008  0.147160  0.651795   \n",
       "4  0.501457  0.713396 -0.201323  0.440816 -0.366045  0.194164  0.592310   \n",
       "\n",
       "        187       188       189       190       191       192       193  \\\n",
       "0  0.266881 -0.530092 -0.365654 -1.346749  1.413864  0.873827  0.206865   \n",
       "1  0.446121 -0.500488 -0.196406 -1.352761  1.316477  0.800528  0.305253   \n",
       "2  0.367511 -0.514712 -0.269907 -1.334703  1.287986  0.792140  0.275841   \n",
       "3  0.185749 -0.535373 -0.292582 -1.176272  1.315063  0.849471  0.073633   \n",
       "4  0.468172 -0.544514 -0.214016 -1.341823  1.389505  0.762576  0.273410   \n",
       "\n",
       "        194       195       196       197       198       199       200  \\\n",
       "0  0.781309  0.919980 -0.130631  0.789748 -0.290507 -0.037331 -0.623846   \n",
       "1  0.788092  0.795825 -0.009118  0.905062 -0.285423  0.042176 -0.684889   \n",
       "2  0.702688  0.886973 -0.074619  0.872234 -0.196475  0.039554 -0.600424   \n",
       "3  0.834554  0.778567 -0.103062  0.563622 -0.116904 -0.115872 -0.858578   \n",
       "4  0.763099  0.784652 -0.049475  0.862331 -0.281147  0.072881 -0.664718   \n",
       "\n",
       "        201       202       203       204       205       206       207  \\\n",
       "0 -0.222425 -0.700611 -0.265118 -0.805169 -1.019531  0.276073  1.237971   \n",
       "1 -0.174024 -0.669468 -0.252664 -0.754569 -0.917132  0.291402  1.193981   \n",
       "2 -0.177276 -0.676825 -0.257799 -0.834277 -0.934696  0.291545  1.224345   \n",
       "3 -0.160163 -0.672919 -0.201171 -0.775342 -0.967213  0.382587  1.142314   \n",
       "4 -0.210628 -0.697912 -0.266265 -0.790932 -0.904315  0.259319  1.197455   \n",
       "\n",
       "        208       209       210       211       212       213       214  \\\n",
       "0  0.510868  0.639456 -0.772032 -0.498545 -0.070208  0.586046  1.138827   \n",
       "1  0.584014  0.585057 -0.867754 -0.621958 -0.114463  0.635479  1.200061   \n",
       "2  0.560848  0.614920 -0.820661 -0.552016 -0.128842  0.602383  1.171128   \n",
       "3  0.679020  0.689643 -0.777711 -0.263860  0.002022  0.514170  1.133368   \n",
       "4  0.554125  0.507206 -0.900422 -0.577060 -0.128074  0.691085  1.164617   \n",
       "\n",
       "        215       216       217       218       219       220       221  \\\n",
       "0 -0.435073 -1.419451 -0.213396 -0.491136 -0.631334 -0.511933 -1.402461   \n",
       "1 -0.331071 -1.387399 -0.272733 -0.406333 -0.732201 -0.498863 -1.469920   \n",
       "2 -0.384855 -1.319689 -0.230485 -0.441356 -0.716113 -0.483825 -1.490365   \n",
       "3 -0.438484 -1.409057 -0.267889 -0.614496 -0.565937 -0.421485 -1.410576   \n",
       "4 -0.352429 -1.335013 -0.226352 -0.328443 -0.714022 -0.480458 -1.541269   \n",
       "\n",
       "        222       223       224       225       226       227       228  \\\n",
       "0 -0.135511  0.057072 -0.531335  0.572108 -0.911924 -0.690377 -0.255268   \n",
       "1 -0.141468  0.065462 -0.470045  0.564274 -0.897157 -0.695762 -0.202772   \n",
       "2 -0.111421  0.091463 -0.511718  0.577919 -0.874078 -0.703042 -0.214166   \n",
       "3 -0.118472 -0.068280 -0.750299  0.517369 -0.606528 -0.660536 -0.196403   \n",
       "4 -0.107138  0.008340 -0.435257  0.660106 -0.888997 -0.727820 -0.212915   \n",
       "\n",
       "        229       230       231       232       233       234       235  \\\n",
       "0 -0.198094 -0.191315  0.432752  0.301143 -0.751887 -0.945049 -1.429686   \n",
       "1 -0.032059  0.109280  0.381726  0.328952 -0.673152 -0.922260 -1.523873   \n",
       "2 -0.149047 -0.022601  0.341620  0.331021 -0.684888 -0.906597 -1.485930   \n",
       "3 -0.111387 -0.193759  0.378023  0.387297 -0.647688 -0.968959 -1.462543   \n",
       "4 -0.067916  0.116845  0.304014  0.364218 -0.736963 -0.910938 -1.491185   \n",
       "\n",
       "        236       237       238       239       240       241       242  \\\n",
       "0 -0.049388  0.066865 -0.282562 -0.674942 -1.291429  0.413257  1.328818   \n",
       "1 -0.087670 -0.092140 -0.234189 -0.774250 -1.304067  0.548550  1.151371   \n",
       "2 -0.066667 -0.003598 -0.191711 -0.729394 -1.330785  0.475882  1.231406   \n",
       "3 -0.064031 -0.005024 -0.240323 -0.693794 -1.184708  0.501205  1.616652   \n",
       "4 -0.071996 -0.098205 -0.253417 -0.808210 -1.360476  0.554854  1.130804   \n",
       "\n",
       "        243       244       245       246       247       248       249  \\\n",
       "0 -1.339934  0.138815 -0.768350  0.424794 -0.130532  0.110422 -0.818007   \n",
       "1 -1.343521  0.191375 -0.741978  0.558551 -0.080042  0.079785 -0.840949   \n",
       "2 -1.320078  0.183847 -0.742423  0.460619 -0.098555  0.172597 -0.857315   \n",
       "3 -1.416650  0.118797 -0.799336  0.411801  0.026219  0.029920 -0.788406   \n",
       "4 -1.266634  0.161967 -0.714519  0.617150 -0.102353  0.160954 -0.903707   \n",
       "\n",
       "        250       251       252       253       254       255       256  \\\n",
       "0 -0.111718 -0.222480  0.471001  2.025520  0.220927  0.105805  0.146110   \n",
       "1  0.012454 -0.201479  0.459524  2.159117  0.284523  0.164675  0.126171   \n",
       "2 -0.073230 -0.181214  0.456467  2.094084  0.162776  0.149736  0.161316   \n",
       "3 -0.086289 -0.245935  0.337564  1.944634  0.112124  0.070472  0.284243   \n",
       "4 -0.043184 -0.262819  0.445695  2.103155  0.237230  0.233652  0.156792   \n",
       "\n",
       "        257       258       259       260       261       262       263  \\\n",
       "0  1.269073 -0.140321  0.531963  0.788315  0.576997 -0.545387  0.475949   \n",
       "1  1.270731 -0.168872  0.522522  0.733846  0.569694 -0.520281  0.377859   \n",
       "2  1.226053 -0.171679  0.507676  0.782621  0.572614 -0.562668  0.410960   \n",
       "3  1.235332 -0.089691  0.411202  0.751495  0.436115 -0.432318  0.399369   \n",
       "4  1.288566 -0.160473  0.500384  0.726031  0.550212 -0.556284  0.349056   \n",
       "\n",
       "        264       265       266       267       268       269       270  \\\n",
       "0 -0.894918 -0.860079  0.102825 -0.095659 -0.507907  1.222366  0.398551   \n",
       "1 -0.783319 -0.931253  0.271501 -0.197730 -0.680632  1.172317  0.487209   \n",
       "2 -0.803358 -0.876836  0.263284 -0.147618 -0.576411  1.181084  0.400624   \n",
       "3 -0.736291 -0.925475  0.202920  0.144372 -0.484589  1.196334  0.416505   \n",
       "4 -0.755855 -0.873598  0.253216 -0.231266 -0.684444  1.132128  0.465914   \n",
       "\n",
       "        271       272       273       274       275       276       277  \\\n",
       "0 -1.468970 -0.884805 -0.613770 -1.238994 -0.103361 -0.684106 -0.115385   \n",
       "1 -1.462741 -0.878852 -0.638744 -1.108701 -0.037423 -0.741607 -0.064968   \n",
       "2 -1.455873 -0.839950 -0.587590 -1.192751  0.000274 -0.745545 -0.101624   \n",
       "3 -1.442390 -0.811985 -0.679783 -1.141728 -0.080420 -0.666271 -0.049418   \n",
       "4 -1.456617 -0.871438 -0.611602 -1.078190 -0.082172 -0.749148 -0.010166   \n",
       "\n",
       "        278       279       280       281       282       283       284  \\\n",
       "0  1.404014 -0.497012  0.441565 -0.594046 -0.747068 -1.084847  0.666250   \n",
       "1  1.391426 -0.549247  0.262075 -0.550730 -0.728045 -1.035685  0.662205   \n",
       "2  1.430094 -0.569125  0.338735 -0.524272 -0.762091 -1.049388  0.649115   \n",
       "3  1.414865 -0.511304  0.412234 -0.762554 -0.630332 -0.889377  0.657835   \n",
       "4  1.413571 -0.573272  0.226502 -0.534144 -0.783868 -1.010550  0.693620   \n",
       "\n",
       "        285       286       287       288       289       290       291  \\\n",
       "0  0.213535  0.080009  1.397216  0.944891  0.390578 -0.060897  0.607301   \n",
       "1  0.071736  0.033617  1.220232  0.923134  0.308076 -0.042995  0.615008   \n",
       "2  0.157896  0.111735  1.289104  0.822303  0.372039 -0.090557  0.626764   \n",
       "3  0.230497  0.031280  1.452565  1.086924  0.379444 -0.235843  0.537226   \n",
       "4  0.094775  0.060632  1.248427  0.893053  0.314744 -0.034301  0.649931   \n",
       "\n",
       "        292       293       294       295       296       297       298  \\\n",
       "0 -0.633116  0.749693 -1.190474  1.135499  0.246069 -1.088464  0.841257   \n",
       "1 -0.607901  0.795418 -1.134020  1.164500  0.084182 -1.144427  0.897849   \n",
       "2 -0.599679  0.765120 -1.096391  1.138387  0.153102 -1.059184  0.907723   \n",
       "3 -0.673907  0.790586 -1.239580  0.956497  0.093771 -1.129679  0.694391   \n",
       "4 -0.625593  0.878793 -1.106819  1.184262  0.094658 -1.077212  0.828606   \n",
       "\n",
       "        299       300       301       302       303       304       305  \\\n",
       "0  1.666158 -0.033129  0.449991  0.225496  0.843186  0.301087 -0.876974   \n",
       "1  1.755242 -0.061617  0.238834  0.129344  0.754704  0.310985 -1.041284   \n",
       "2  1.739993 -0.127071  0.356983  0.220204  0.798265  0.246982 -0.953839   \n",
       "3  1.584094 -0.107322  0.405742  0.227096  0.819183  0.338765 -0.858744   \n",
       "4  1.746933 -0.119415  0.267248  0.146313  0.774485  0.290367 -0.984069   \n",
       "\n",
       "        306       307       308       309       310       311       312  \\\n",
       "0  0.184274  0.272972  0.071353 -0.503152 -1.050257  0.035765  1.039412   \n",
       "1  0.157351  0.425053  0.131695 -0.474184 -0.967048 -0.045287  0.930147   \n",
       "2  0.125027  0.348625  0.243486 -0.489608 -0.956885 -0.031069  0.974607   \n",
       "3  0.279879  0.456360  0.096465 -0.479309 -0.956772 -0.019054  0.804485   \n",
       "4  0.163573  0.371077  0.173738 -0.540956 -0.984321 -0.057061  0.935040   \n",
       "\n",
       "        313       314       315       316       317       318       319  \\\n",
       "0 -1.039289 -0.416652 -0.862202 -1.245696  0.361685  0.666548  0.697989   \n",
       "1 -0.942065 -0.427162 -0.923421 -1.236298  0.451016  0.677714  0.578491   \n",
       "2 -0.971375 -0.382681 -0.945659 -1.297571  0.368988  0.680583  0.639816   \n",
       "3 -1.070583 -0.445522 -0.872635 -1.291553  0.331101  0.817385  0.783406   \n",
       "4 -0.969775 -0.399637 -0.960755 -1.203546  0.413100  0.716661  0.533791   \n",
       "\n",
       "        320       321       322       323       324       325       326  \\\n",
       "0 -0.266058 -0.210500 -1.571464  0.865381  0.274597  0.124707 -0.676559   \n",
       "1 -0.285586 -0.137903 -1.592543  0.941437  0.319753  0.130286 -0.522547   \n",
       "2 -0.292263 -0.200874 -1.547304  0.879053  0.324322  0.180030 -0.602539   \n",
       "3 -0.424562 -0.223315 -1.591913  0.977125  0.113102  0.125281 -0.615123   \n",
       "4 -0.288866 -0.127486 -1.605333  0.874082  0.395495  0.110656 -0.566782   \n",
       "\n",
       "        327       328       329       330       331       332       333  \\\n",
       "0 -0.667969  0.206986 -0.170810  1.665514  1.224472  0.805410  0.966988   \n",
       "1 -0.579336  0.124565 -0.219538  1.640512  1.093547  0.660521  1.132050   \n",
       "2 -0.570659  0.222043 -0.242540  1.654178  1.184294  0.637380  1.054694   \n",
       "3 -0.653087  0.292330 -0.156515  1.541056  1.180048  0.942938  1.035298   \n",
       "4 -0.598744  0.098502 -0.275136  1.581839  1.119314  0.659908  1.122719   \n",
       "\n",
       "        334       335       336       337       338       339       340  \\\n",
       "0 -0.228395 -0.133413 -0.482085 -0.110164  0.434618 -0.308112 -0.498381   \n",
       "1 -0.212897 -0.083036 -0.449174 -0.254495  0.483878 -0.251386 -0.545303   \n",
       "2 -0.160300 -0.109282 -0.529884 -0.192353  0.427855 -0.275337 -0.536501   \n",
       "3 -0.422127 -0.152245 -0.462570 -0.073727  0.583474 -0.533425 -0.536332   \n",
       "4 -0.220744 -0.052061 -0.471505 -0.247585  0.397779 -0.239707 -0.556417   \n",
       "\n",
       "        341       342       343       344       345       346       347  \\\n",
       "0  0.811527  0.802559 -0.212064 -1.015529 -0.293165 -0.660605 -0.298868   \n",
       "1  0.803491  0.833312 -0.211865 -0.982234 -0.377478 -0.658292 -0.374307   \n",
       "2  0.812177  0.806075 -0.259708 -0.902602 -0.313199 -0.597796 -0.312101   \n",
       "3  0.876369  0.873332 -0.255398 -0.952862 -0.405484 -0.747959 -0.478568   \n",
       "4  0.809440  0.808680 -0.166546 -0.989780 -0.370013 -0.731616 -0.371112   \n",
       "\n",
       "        348       349       350       351       352       353       354  \\\n",
       "0 -0.153508  0.022591 -0.103366 -0.438295  0.454937 -0.212763 -0.351217   \n",
       "1 -0.092113  0.131410 -0.127149 -0.445892  0.601887 -0.119428 -0.218186   \n",
       "2 -0.076448  0.111785 -0.155905 -0.473176  0.585917 -0.110156 -0.237753   \n",
       "3 -0.163381 -0.042504 -0.205303 -0.422434  0.515208 -0.284977 -0.304822   \n",
       "4 -0.083160  0.149169 -0.133807 -0.518683  0.655918 -0.076505 -0.245805   \n",
       "\n",
       "        355       356       357       358       359       360       361  \\\n",
       "0 -0.391846 -0.090670  1.201962  0.087204  0.361371 -0.241886 -0.849253   \n",
       "1 -0.345315 -0.131767  1.128863  0.055303  0.182105 -0.261048 -0.915739   \n",
       "2 -0.429785 -0.137947  1.142852  0.097614  0.254343 -0.307527 -0.890695   \n",
       "3 -0.434386 -0.085120  1.123193  0.040862  0.242616 -0.143464 -0.854485   \n",
       "4 -0.376895 -0.093865  1.073554  0.045067  0.159497 -0.268137 -0.931505   \n",
       "\n",
       "        362       363       364       365       366       367       368  \\\n",
       "0  0.305798  0.937777  1.382287  1.204304 -0.419141 -0.563582  0.505697   \n",
       "1  0.212881  0.735336  1.378469  1.292215 -0.426474 -0.569086  0.460583   \n",
       "2  0.221037  0.808231  1.317838  1.254083 -0.450387 -0.507112  0.416753   \n",
       "3  0.349412  0.909687  1.325083  1.268041 -0.383491 -0.526563  0.470265   \n",
       "4  0.212943  0.737053  1.364571  1.270649 -0.439459 -0.560376  0.424254   \n",
       "\n",
       "        369       370       371       372       373       374       375  \\\n",
       "0  0.208627  0.761790 -0.002871  0.303617 -0.697072 -0.131776 -0.536411   \n",
       "1  0.192373  0.693539 -0.068297  0.259698 -0.648445 -0.071153 -0.617521   \n",
       "2  0.220055  0.724157 -0.138319  0.284998 -0.673293 -0.030229 -0.522690   \n",
       "3  0.184879  0.924516  0.028757  0.179295 -0.833563 -0.272071 -0.648685   \n",
       "4  0.223567  0.681101 -0.097223  0.290508 -0.637415 -0.051757 -0.591724   \n",
       "\n",
       "        376       377       378       379       380       381       382  \\\n",
       "0 -0.844009 -0.792295 -0.247314 -0.595349  0.590092 -1.869353  0.188370   \n",
       "1 -0.802165 -0.869009 -0.216430 -0.515363  0.585334 -1.772227  0.192960   \n",
       "2 -0.833510 -0.839668 -0.179920 -0.466504  0.575758 -1.814417  0.149464   \n",
       "3 -0.880806 -0.828425 -0.211412 -0.527209  0.483659 -2.047873  0.201151   \n",
       "4 -0.877876 -0.800953 -0.202127 -0.522013  0.545234 -1.770258  0.136041   \n",
       "\n",
       "        383       384       385       386       387       388       389  \\\n",
       "0 -0.149342 -0.424916  0.260395 -0.403129 -0.619658 -0.722321 -0.538546   \n",
       "1 -0.081374 -0.384915  0.202669 -0.318162 -0.583692 -0.685025 -0.493772   \n",
       "2 -0.153697 -0.311654  0.207845 -0.356881 -0.605354 -0.744810 -0.498095   \n",
       "3  0.002593 -0.541647  0.129722 -0.387287 -0.432867 -0.929499 -0.585198   \n",
       "4 -0.135616 -0.375873  0.252454 -0.387679 -0.624776 -0.700880 -0.439825   \n",
       "\n",
       "        390       391       392       393       394       395       396  \\\n",
       "0 -1.297676  0.115491 -0.066180  0.236784  0.415496 -0.458166 -0.462058   \n",
       "1 -1.375381  0.034264 -0.037577  0.264094  0.507684 -0.538325 -0.351766   \n",
       "2 -1.330299  0.125353 -0.037136  0.255347  0.452987 -0.466196 -0.413569   \n",
       "3 -1.342841 -0.118521 -0.248185  0.214100  0.370611 -0.372156 -0.478614   \n",
       "4 -1.394108 -0.004928 -0.000743  0.232922  0.446101 -0.534080 -0.354860   \n",
       "\n",
       "        397       398       399       400       401       402       403  \\\n",
       "0  1.221647  0.445055 -0.227902 -1.135463  0.163360 -0.193833 -1.298360   \n",
       "1  1.216479  0.357146 -0.213810 -1.028683  0.131457 -0.298029 -1.235245   \n",
       "2  1.174643  0.356723 -0.199031 -1.098117  0.126535 -0.253576 -1.247060   \n",
       "3  1.106689  0.420173 -0.220321 -1.131540 -0.054001 -0.277774 -1.478531   \n",
       "4  1.211094  0.385074 -0.237100 -1.032569  0.162628 -0.323247 -1.125692   \n",
       "\n",
       "        404       405       406       407       408       409       410  \\\n",
       "0 -0.535846 -0.838075  0.104269  0.359346  0.459879 -0.845323 -0.473110   \n",
       "1 -0.668961 -0.892707  0.195993  0.289603  0.483021 -0.896739 -0.520981   \n",
       "2 -0.638265 -0.878784  0.194846  0.342174  0.469645 -0.888599 -0.529604   \n",
       "3 -0.408099 -0.946686  0.072410  0.339300  0.517003 -0.908118 -0.557184   \n",
       "4 -0.601831 -0.843181  0.122472  0.285022  0.599703 -0.849793 -0.508561   \n",
       "\n",
       "        411       412       413       414       415       416       417  \\\n",
       "0 -0.738906  0.098208  0.634441  0.979829 -0.759346 -0.059345  0.817776   \n",
       "1 -0.730229  0.088305  0.607742  0.995007 -0.798895 -0.049450  0.829252   \n",
       "2 -0.720473  0.029722  0.621405  1.006198 -0.866557 -0.158529  0.864191   \n",
       "3 -0.746486  0.127018  0.928284  0.949543 -0.556355 -0.064072  0.857030   \n",
       "4 -0.715026 -0.021927  0.618948  0.970517 -0.790941 -0.099554  0.839952   \n",
       "\n",
       "        418       419       420       421       422       423       424  \\\n",
       "0 -0.983007  0.203411 -2.159915  0.082703  0.267683 -0.920031  1.404773   \n",
       "1 -1.047841  0.163200 -2.152037  0.034751  0.296912 -0.972056  1.520006   \n",
       "2 -1.077978  0.228813 -2.115899  0.061968  0.298630 -0.935301  1.502846   \n",
       "3 -0.976561  0.305526 -2.154242  0.223317  0.225035 -0.839536  1.520393   \n",
       "4 -1.081604  0.165439 -2.124782  0.013491  0.294583 -0.903626  1.533997   \n",
       "\n",
       "        425       426       427       428       429       430       431  \\\n",
       "0  0.251037  0.232732 -0.032009 -0.220980 -0.446403 -0.452745 -0.820232   \n",
       "1  0.387377  0.293731  0.047997 -0.336874 -0.408052 -0.488919 -0.972460   \n",
       "2  0.342955  0.270812  0.029329 -0.212141 -0.443624 -0.473634 -0.867430   \n",
       "3  0.305954  0.253186 -0.169697 -0.203706 -0.476610 -0.466606 -0.758849   \n",
       "4  0.394492  0.236793  0.096652 -0.303652 -0.345615 -0.457255 -0.893469   \n",
       "\n",
       "        432       433       434       435       436       437       438  \\\n",
       "0  1.125852 -0.381940 -0.465709  1.790494 -0.339537 -0.053865 -0.511411   \n",
       "1  1.158347 -0.306049 -0.385948  1.809788 -0.399892 -0.213527 -0.582929   \n",
       "2  1.179359 -0.336955 -0.445781  1.807294 -0.403697 -0.165659 -0.490828   \n",
       "3  1.145413 -0.261004 -0.524954  1.727286 -0.309588  0.175843 -0.460226   \n",
       "4  1.135524 -0.307801 -0.340425  1.809699 -0.429004 -0.172216 -0.515150   \n",
       "\n",
       "        439       440       441       442       443       444       445  \\\n",
       "0  0.585588  0.433298  1.116106 -0.550744  1.050105  0.518447  0.202749   \n",
       "1  0.648600  0.623755  1.045417 -0.773080  0.937119  0.663291  0.261242   \n",
       "2  0.566638  0.508331  1.021329 -0.694598  0.978845  0.556130  0.175262   \n",
       "3  0.632846  0.467701  1.090383 -0.589556  0.804530  0.373160  0.190061   \n",
       "4  0.580757  0.585405  1.051507 -0.789916  0.950449  0.669788  0.263933   \n",
       "\n",
       "        446       447       448       449       450       451       452  \\\n",
       "0 -0.542480 -0.088704  0.131593 -1.613492 -1.141693 -0.732665  0.001233   \n",
       "1 -0.604901 -0.192325  0.113959 -1.520509 -1.137753 -0.818679 -0.075022   \n",
       "2 -0.582990 -0.116856  0.092763 -1.522888 -1.113762 -0.719209 -0.037430   \n",
       "3 -0.612506 -0.258711  0.113132 -1.599032 -1.110064 -0.750210 -0.009055   \n",
       "4 -0.627923 -0.128958  0.091977 -1.490052 -1.174419 -0.814766 -0.063884   \n",
       "\n",
       "        453       454       455       456       457       458       459  \\\n",
       "0  0.386457  0.174765  0.514060  0.166606 -0.521211  0.464471 -0.112905   \n",
       "1  0.352320  0.168844  0.506019  0.207167 -0.540203  0.548214  0.025663   \n",
       "2  0.384419  0.116580  0.551749  0.236469 -0.540966  0.513225 -0.006765   \n",
       "3  0.475273  0.351373  0.520845  0.161207 -0.649602  0.345635 -0.142417   \n",
       "4  0.262932  0.132131  0.557756  0.247557 -0.560080  0.556593  0.058634   \n",
       "\n",
       "        460       461       462       463       464       465       466  \\\n",
       "0  0.905570  0.160346 -1.562244  0.467454  0.778058 -0.327656 -0.583430   \n",
       "1  0.808421  0.026290 -1.540848  0.451329  0.781690 -0.370763 -0.549507   \n",
       "2  0.825184  0.055512 -1.552524  0.451008  0.747115 -0.350361 -0.601362   \n",
       "3  0.763289  0.134580 -1.721865  0.297760  0.748106 -0.422488 -0.589187   \n",
       "4  0.825289  0.085803 -1.481190  0.478098  0.802811 -0.338320 -0.568944   \n",
       "\n",
       "        467       468       469       470       471       472       473  \\\n",
       "0  0.372757 -0.825826 -0.913649 -1.754187  1.064206 -0.191573  0.070641   \n",
       "1  0.316583 -0.911931 -0.936688 -1.797986  0.995487 -0.139737 -0.059015   \n",
       "2  0.330084 -0.850026 -0.916450 -1.753235  0.981869 -0.096701  0.012154   \n",
       "3  0.405591 -0.749701 -0.847804 -1.799725  1.100659 -0.209567 -0.014525   \n",
       "4  0.333436 -0.947319 -1.004601 -1.710660  1.005762 -0.026395 -0.064325   \n",
       "\n",
       "        474       475       476       477       478       479       480  \\\n",
       "0  0.507003 -0.309522  0.488213 -0.121472 -1.256016  0.158666 -0.351518   \n",
       "1  0.605686 -0.217134  0.404120 -0.039379 -1.154441  0.116717 -0.551561   \n",
       "2  0.569903 -0.257482  0.431447 -0.099163 -1.242696  0.079654 -0.486146   \n",
       "3  0.545105 -0.295599  0.502782 -0.218491 -1.147477  0.301702 -0.355909   \n",
       "4  0.595014 -0.280105  0.378764 -0.064372 -1.211483  0.139155 -0.570510   \n",
       "\n",
       "        481       482       483       484       485       486       487  \\\n",
       "0  0.449145 -0.167377 -0.780235  0.966628  0.080630  0.068091 -0.685602   \n",
       "1  0.516555 -0.145332 -0.828297  0.995400  0.169654  0.185134 -0.641524   \n",
       "2  0.515611 -0.140231 -0.814907  1.048044  0.136855  0.138922 -0.704128   \n",
       "3  0.382653 -0.142286 -0.786498  0.939285  0.105172  0.199054 -0.468503   \n",
       "4  0.491161 -0.132459 -0.875530  1.046322  0.204486  0.171266 -0.651145   \n",
       "\n",
       "        488       489       490       491       492       493       494  \\\n",
       "0  0.208218 -0.048632  0.485111 -0.168559 -0.784368 -0.237996 -0.587028   \n",
       "1  0.236970 -0.168315  0.429707 -0.097793 -0.738991 -0.140453 -0.329424   \n",
       "2  0.309598 -0.122267  0.504417 -0.125301 -0.766595 -0.231429 -0.432408   \n",
       "3  0.014010  0.128413  0.329533 -0.086558 -0.549329 -0.238406 -0.675041   \n",
       "4  0.234697 -0.146406  0.399243 -0.084343 -0.725493 -0.180831 -0.382925   \n",
       "\n",
       "        495       496       497       498       499       500       501  \\\n",
       "0 -0.269412  0.361229 -0.062331 -0.342411 -0.683894 -0.093147 -0.290296   \n",
       "1 -0.173847  0.354213  0.009764 -0.416106 -0.700126 -0.079852 -0.427185   \n",
       "2 -0.211378  0.331680  0.031405 -0.409302 -0.803247 -0.091855 -0.354815   \n",
       "3 -0.244936  0.383471 -0.161691 -0.375179 -0.508296 -0.100791 -0.423717   \n",
       "4 -0.235546  0.378893 -0.058852 -0.439174 -0.765985 -0.061546 -0.419903   \n",
       "\n",
       "        502       503       504       505       506       507       508  \\\n",
       "0 -0.621593  0.017103  0.344595 -0.449510  0.455677  1.485255 -1.178215   \n",
       "1 -0.773300 -0.112472  0.274777 -0.332992  0.551611  1.530572 -1.192895   \n",
       "2 -0.758724 -0.045611  0.318434 -0.388747  0.510582  1.489094 -1.200607   \n",
       "3 -0.463096 -0.096444  0.203225 -0.389202  0.445992  1.543865 -1.294317   \n",
       "4 -0.761329 -0.078045  0.266731 -0.337940  0.595525  1.535963 -1.170741   \n",
       "\n",
       "        509       510       511       512       513       514       515  \\\n",
       "0  0.439544 -0.130356  0.240152  1.002190  0.385913  0.449242 -0.988080   \n",
       "1  0.418921 -0.150063  0.373168  1.092209  0.343963  0.671086 -1.067681   \n",
       "2  0.464071 -0.174496  0.332447  1.104886  0.324981  0.597101 -0.990975   \n",
       "3  0.476577 -0.151629 -0.005974  0.959711  0.217443  0.651440 -0.827534   \n",
       "4  0.409035 -0.220448  0.397286  1.116590  0.312850  0.608619 -0.994447   \n",
       "\n",
       "        516       517       518       519       520       521       522  \\\n",
       "0  0.000966 -0.263088 -0.772724  1.516419  0.534272 -0.034968  0.282683   \n",
       "1  0.157845 -0.203760 -0.728330  1.416629  0.558890 -0.078986  0.259909   \n",
       "2  0.101849 -0.211951 -0.680470  1.468576  0.584807 -0.035795  0.267821   \n",
       "3 -0.017623 -0.301295 -0.650634  1.373814  0.663393 -0.100480  0.215191   \n",
       "4  0.194232 -0.228496 -0.662745  1.404307  0.535887 -0.060965  0.266274   \n",
       "\n",
       "        523       524       525       526        527       528       529  \\\n",
       "0  0.308866  0.257303  0.469700  0.922770  13.691738  0.192573  0.510660   \n",
       "1  0.280206  0.136865  0.535399  0.836058  13.755681  0.185654  0.420050   \n",
       "2  0.310590  0.202733  0.507496  0.849046  13.810098  0.217498  0.424507   \n",
       "3  0.341025  0.211980  0.476814  0.760928  13.755452  0.186910  0.575501   \n",
       "4  0.300520  0.206439  0.499246  0.871262  13.833971  0.202159  0.397264   \n",
       "\n",
       "        530       531       532       533       534       535       536  \\\n",
       "0 -0.188680  0.650560 -0.250861 -0.452052 -0.228531 -0.320912  0.281924   \n",
       "1 -0.249418  0.579467 -0.209460 -0.544391 -0.237524 -0.309772  0.310431   \n",
       "2 -0.162423  0.646327 -0.246038 -0.558326 -0.173403 -0.300309  0.366463   \n",
       "3 -0.100905  0.490973 -0.259051 -0.429931 -0.187163 -0.263229  0.319919   \n",
       "4 -0.220723  0.624000 -0.279842 -0.555698 -0.157215 -0.334988  0.294200   \n",
       "\n",
       "        537       538       539       540       541       542       543  \\\n",
       "0 -0.671686  0.128511 -0.248603  0.883401 -0.509352 -0.723377  0.521739   \n",
       "1 -0.550121  0.138718 -0.358635  0.975893 -0.705316 -0.779524  0.535575   \n",
       "2 -0.618099  0.112679 -0.282155  0.940206 -0.572087 -0.738524  0.514402   \n",
       "3 -0.742757  0.097327 -0.045151  0.962171 -0.564276 -0.627818  0.620934   \n",
       "4 -0.575318  0.171688 -0.298960  0.957531 -0.654536 -0.836525  0.567723   \n",
       "\n",
       "        544       545       546       547       548       549       550  \\\n",
       "0  0.965522  0.064517 -1.063517  0.328387 -0.436082  0.095997 -0.473636   \n",
       "1  0.868967 -0.005817 -0.957481  0.345602 -0.456151  0.205090 -0.477702   \n",
       "2  0.939723  0.039060 -1.017120  0.372926 -0.449614  0.186620 -0.504193   \n",
       "3  0.727996  0.137582 -1.000852  0.295297 -0.410747  0.144109 -0.534853   \n",
       "4  0.855264 -0.045613 -0.843105  0.418878 -0.389147  0.166774 -0.528646   \n",
       "\n",
       "        551       552       553       554       555       556       557  \\\n",
       "0 -0.651135 -0.830119 -0.580499 -0.267841 -0.452461  0.503747  0.786223   \n",
       "1 -0.706677 -0.839750 -0.656065 -0.255254 -0.575219  0.534209  0.721351   \n",
       "2 -0.735414 -0.906574 -0.620949 -0.297959 -0.571343  0.491162  0.773338   \n",
       "3 -0.599668 -0.752876 -0.500221 -0.199155 -0.436160  0.667490  0.808029   \n",
       "4 -0.713763 -0.860247 -0.622041 -0.243481 -0.567965  0.493762  0.726090   \n",
       "\n",
       "        558       559       560       561       562       563       564  \\\n",
       "0  0.420469 -0.973155  0.459810 -0.148301  0.152842 -0.173274 -0.254788   \n",
       "1  0.356078 -0.894912  0.475254 -0.203818  0.135967 -0.162353 -0.338342   \n",
       "2  0.389001 -0.916831  0.520861 -0.102760  0.187174 -0.096972 -0.317347   \n",
       "3  0.354899 -0.810560  0.533803 -0.210651  0.139549 -0.306552 -0.276323   \n",
       "4  0.366698 -0.870929  0.441909 -0.177847  0.141987 -0.138090 -0.326323   \n",
       "\n",
       "        565       566       567       568       569       570       571  \\\n",
       "0 -0.289856 -0.108738 -0.556000  0.392114 -0.837225  0.068129 -1.171508   \n",
       "1 -0.270852 -0.057112 -0.419428  0.319831 -0.778240  0.100756 -1.252283   \n",
       "2 -0.311745 -0.083950 -0.468372  0.417958 -0.871809  0.111372 -1.179978   \n",
       "3 -0.176979 -0.080266 -0.448883  0.427966 -0.809047  0.095144 -1.091212   \n",
       "4 -0.252565 -0.042827 -0.373727  0.287655 -0.794317  0.053010 -1.202370   \n",
       "\n",
       "        572       573       574       575       576       577       578  \\\n",
       "0  0.110569 -0.041277  0.116804 -0.248698  1.166849 -1.129025 -0.253690   \n",
       "1  0.214270 -0.107290  0.146275 -0.228278  1.099355 -1.125695 -0.292137   \n",
       "2  0.125957 -0.055086  0.118006 -0.224943  1.173329 -1.092230 -0.321947   \n",
       "3  0.272929 -0.127293  0.178987 -0.230544  1.009167 -1.124698 -0.246905   \n",
       "4  0.158420 -0.121022  0.080395 -0.219004  1.088925 -1.097160 -0.289474   \n",
       "\n",
       "        579       580       581       582       583       584       585  \\\n",
       "0 -0.017193  0.800823  1.375973 -0.047102 -1.018535  0.102864  0.049689   \n",
       "1  0.128582  0.777553  1.387327 -0.014129 -1.020147  0.112671  0.022097   \n",
       "2  0.094633  0.755455  1.366751 -0.055530 -1.005778  0.092789  0.045076   \n",
       "3 -0.001277  0.824402  1.208642 -0.041223 -1.086967  0.008279 -0.059734   \n",
       "4  0.162054  0.804853  1.337167  0.014690 -0.996204  0.123476  0.061626   \n",
       "\n",
       "        586       587       588       589       590       591       592  \\\n",
       "0 -0.237023  0.519906 -0.185900  0.519788  0.215782 -0.430854 -0.915275   \n",
       "1 -0.205237  0.464534 -0.145499  0.643182  0.158840 -0.376464 -0.871322   \n",
       "2 -0.196076  0.442624 -0.156460  0.604558  0.224771 -0.439093 -0.884282   \n",
       "3 -0.218140  0.476682 -0.383100  0.568983  0.301135 -0.434543 -0.886003   \n",
       "4 -0.147490  0.418737 -0.128772  0.676014  0.201658 -0.390985 -0.868287   \n",
       "\n",
       "        593       594       595       596       597       598       599  \\\n",
       "0  0.838968  0.814252 -1.847867 -0.664324  0.348469  1.322639  1.097698   \n",
       "1  0.895346  0.938654 -1.890932 -0.644186  0.292456  1.214450  1.147751   \n",
       "2  0.837966  0.902498 -1.854478 -0.662036  0.235553  1.269663  1.150201   \n",
       "3  0.891701  0.753125 -1.835825 -0.483442  0.528058  1.511796  1.142417   \n",
       "4  0.897845  0.941632 -1.856855 -0.682957  0.240994  1.229441  1.104259   \n",
       "\n",
       "        600       601       602       603       604       605       606  \\\n",
       "0  0.237241 -0.224808 -0.041211 -0.058312  0.027385  1.010817  0.910949   \n",
       "1  0.188052 -0.259500 -0.040903 -0.088932  0.081010  0.955893  0.926698   \n",
       "2  0.281798 -0.251417 -0.066791 -0.106072  0.051792  1.007021  0.846095   \n",
       "3  0.133046 -0.187076 -0.069194  0.017956 -0.104788  0.976280  1.128582   \n",
       "4  0.197969 -0.281820 -0.153577 -0.060661 -0.003830  0.976371  0.880805   \n",
       "\n",
       "        607       608       609       610       611       612       613  \\\n",
       "0  0.413210 -0.312327 -1.012775  0.550531 -0.248307  0.016721 -1.514807   \n",
       "1  0.379206 -0.272599 -1.025534  0.466301 -0.318006  0.110879 -1.479651   \n",
       "2  0.376695 -0.401701 -1.045586  0.445287 -0.279466  0.103828 -1.554351   \n",
       "3  0.491830 -0.445289 -1.154046  0.415983 -0.341539 -0.161013 -1.475966   \n",
       "4  0.299838 -0.247525 -1.036887  0.442813 -0.292170  0.126248 -1.514624   \n",
       "\n",
       "        614       615       616       617       618       619       620  \\\n",
       "0  0.496081  0.372792  1.370030  0.141930 -0.329672  0.000750  0.076919   \n",
       "1  0.474943  0.288888  1.347598 -0.008139 -0.372184  0.151988 -0.031868   \n",
       "2  0.487281  0.278111  1.311831  0.096571 -0.277448  0.117304  0.091447   \n",
       "3  0.395354  0.499803  1.304428 -0.034853 -0.202827 -0.212101  0.054552   \n",
       "4  0.424401  0.234528  1.300573 -0.017075 -0.293650  0.196987  0.031474   \n",
       "\n",
       "        621       622       623       624       625       626       627  \\\n",
       "0  0.799148 -0.326141 -0.362494 -1.473805  0.478124  0.986631  0.054843   \n",
       "1  0.879389 -0.177461 -0.285828 -1.547795  0.418664  1.061381  0.049695   \n",
       "2  0.901578 -0.187459 -0.334313 -1.461941  0.399218  0.992120  0.097716   \n",
       "3  0.847058 -0.345506 -0.309624 -1.505085  0.461277  1.008005  0.038445   \n",
       "4  0.771320 -0.110691 -0.310547 -1.414609  0.421740  1.064328  0.037880   \n",
       "\n",
       "        628       629       630       631       632       633       634  \\\n",
       "0  0.782568  0.209285  0.091113 -0.189220 -0.718675  0.601441  0.282407   \n",
       "1  0.702683  0.165721  0.105098 -0.253561 -0.748394  0.583135  0.362687   \n",
       "2  0.760746  0.187692  0.093143 -0.252097 -0.769815  0.541137  0.328524   \n",
       "3  0.761725  0.315565  0.080530 -0.118607 -0.903829  0.620333  0.369704   \n",
       "4  0.667590  0.196636  0.041633 -0.269171 -0.646298  0.598211  0.340289   \n",
       "\n",
       "        635       636       637       638       639       640       641  \\\n",
       "0 -0.210989  0.434217 -0.159499  1.016381  0.122543  0.843650  0.714568   \n",
       "1 -0.193986  0.372298 -0.138735  0.862881  0.127770  0.841560  0.671501   \n",
       "2 -0.176210  0.395859 -0.176682  0.918239  0.121457  0.860408  0.685412   \n",
       "3 -0.137612  0.353510 -0.180669  0.950037  0.077531  0.843435  0.516253   \n",
       "4 -0.213025  0.390018 -0.119403  0.929268  0.042302  0.799159  0.631597   \n",
       "\n",
       "        642       643       644       645       646       647       648  \\\n",
       "0 -0.292461  0.375786 -0.017425  0.312415  0.117133  0.218766 -0.157159   \n",
       "1 -0.442474  0.338687  0.018475  0.194239  0.252680  0.299385 -0.152371   \n",
       "2 -0.415399  0.288547  0.015103  0.216210  0.223028  0.311005 -0.136705   \n",
       "3 -0.121426  0.493003  0.014580  0.378812  0.023460  0.185424 -0.081403   \n",
       "4 -0.425722  0.363460  0.030877  0.171625  0.270233  0.316210 -0.154966   \n",
       "\n",
       "        649       650       651       652       653       654       655  \\\n",
       "0 -0.020092  0.248823  0.275047 -0.059181  0.095104 -0.751459 -1.037601   \n",
       "1 -0.172670  0.327163  0.279736  0.031042  0.071036 -0.689128 -1.041822   \n",
       "2 -0.132440  0.311305  0.226451 -0.027480  0.071210 -0.786218 -1.022326   \n",
       "3 -0.107268  0.317219  0.121431  0.039220  0.128190 -0.821351 -1.003214   \n",
       "4 -0.149342  0.374820  0.277414 -0.014272  0.081079 -0.732452 -1.082934   \n",
       "\n",
       "        656       657       658       659       660       661       662  \\\n",
       "0  0.293187 -0.890594  0.206552  1.019548 -0.293066 -0.053301 -0.471833   \n",
       "1  0.259532 -0.961228  0.242013  0.963285 -0.365468 -0.200003 -0.385047   \n",
       "2  0.244705 -0.999520  0.176796  0.988606 -0.307477 -0.168795 -0.456089   \n",
       "3  0.196884 -0.872062  0.173263  1.201262 -0.250084 -0.189277 -0.336288   \n",
       "4  0.222616 -0.899667  0.185834  0.939747 -0.323772 -0.151285 -0.419598   \n",
       "\n",
       "        663       664       665       666       667       668       669  \\\n",
       "0 -0.515374  0.820705  0.508655 -0.206545 -0.616110  0.389341 -1.095838   \n",
       "1 -0.607396  0.685862  0.407754 -0.322496 -0.570370  0.388105 -1.039391   \n",
       "2 -0.565756  0.765105  0.388070 -0.279937 -0.631984  0.377675 -0.999921   \n",
       "3 -0.555796  0.934918  0.559159 -0.095710 -0.517784  0.469165 -0.984998   \n",
       "4 -0.602493  0.644153  0.372283 -0.273051 -0.588839  0.398777 -1.052181   \n",
       "\n",
       "        670       671       672       673       674       675       676  \\\n",
       "0 -0.964378 -0.095803  1.038210 -0.507393 -0.056993  0.850198 -1.490931   \n",
       "1 -0.946039  0.123374  0.939547 -0.354201 -0.129019  0.860494 -1.518184   \n",
       "2 -0.913309 -0.000018  0.924227 -0.357229 -0.081519  0.840150 -1.501712   \n",
       "3 -1.131007 -0.244719  0.791499 -0.572291 -0.146145  0.905546 -1.457422   \n",
       "4 -0.893190  0.074296  0.977648 -0.363500 -0.136151  0.862840 -1.556968   \n",
       "\n",
       "        677       678       679       680       681       682       683  \\\n",
       "0  2.412739 -0.268311 -0.456288  0.394387 -0.013602 -1.086184  0.230685   \n",
       "1  2.376690 -0.151651 -0.584268  0.447015 -0.127059 -1.072402  0.276638   \n",
       "2  2.406128 -0.208879 -0.486539  0.410345 -0.072436 -1.098973  0.216020   \n",
       "3  2.260087 -0.166206 -0.636642  0.350568 -0.190369 -1.072349  0.267607   \n",
       "4  2.429061 -0.168989 -0.561527  0.368714 -0.089533 -1.040147  0.236964   \n",
       "\n",
       "        684       685       686       687       688       689       690  \\\n",
       "0 -0.522678  0.920389 -1.407173 -0.286586  1.165661  1.206922  0.380767   \n",
       "1 -0.455084  0.823231 -1.364535 -0.263401  1.176278  1.089824  0.364919   \n",
       "2 -0.467757  0.835978 -1.368567 -0.225865  1.181766  1.148447  0.321342   \n",
       "3 -0.370968  0.860484 -1.261691 -0.327150  1.255396  1.209962  0.243347   \n",
       "4 -0.472911  0.857241 -1.358730 -0.202116  1.071472  1.144902  0.375883   \n",
       "\n",
       "        691       692       693       694       695       696       697  \\\n",
       "0  0.468275  0.375186  0.571283  0.216194  0.003894 -0.155045 -0.092205   \n",
       "1  0.361851  0.332384  0.625098  0.373669 -0.127666 -0.273327 -0.103007   \n",
       "2  0.436042  0.356814  0.529793  0.307496 -0.056465 -0.179904 -0.071077   \n",
       "3  0.384269  0.434268  0.611408  0.172822  0.003738 -0.260490  0.045967   \n",
       "4  0.347154  0.276868  0.587678  0.353309 -0.145163 -0.243819 -0.136560   \n",
       "\n",
       "        698       699       700       701       702       703       704  \\\n",
       "0  0.311307 -0.081987 -0.547525  0.480314 -1.272792  0.368472 -0.351340   \n",
       "1  0.496733 -0.085223 -0.494848  0.480460 -1.312140  0.328338 -0.490456   \n",
       "2  0.459753 -0.105336 -0.506131  0.463870 -1.298070  0.300489 -0.496427   \n",
       "3  0.185595 -0.023985 -0.541200  0.514001 -1.352766  0.512786 -0.323352   \n",
       "4  0.516772 -0.094067 -0.435980  0.474158 -1.319113  0.304544 -0.496666   \n",
       "\n",
       "        705       706       707       708       709       710       711  \\\n",
       "0  0.470269  1.709743  0.781073  0.444272  0.081989 -1.170666  0.959817   \n",
       "1  0.387564  1.738777  0.864402  0.367405  0.029553 -1.003045  1.031052   \n",
       "2  0.394794  1.759747  0.882884  0.403094  0.123694 -1.037884  0.928086   \n",
       "3  0.649051  1.774142  0.869626  0.469016  0.098277 -1.127167  0.856277   \n",
       "4  0.357485  1.791377  0.839931  0.384224  0.002200 -1.024778  1.006014   \n",
       "\n",
       "        712       713       714       715       716       717       718  \\\n",
       "0  1.008016  0.708834  0.809033 -0.359720  0.237015 -0.831460  0.810471   \n",
       "1  0.937171  0.801539  0.799956 -0.441056  0.348441 -0.779789  0.634693   \n",
       "2  1.004122  0.827194  0.786512 -0.321982  0.335881 -0.818204  0.719793   \n",
       "3  0.822179  0.668918  0.856857 -0.527306  0.258218 -0.896307  0.704975   \n",
       "4  0.906514  0.815624  0.691230 -0.399412  0.384132 -0.739347  0.684005   \n",
       "\n",
       "        719       720       721       722       723       724       725  \\\n",
       "0  1.055264  0.407583 -0.347130 -0.879955  0.054611 -1.294139 -0.795184   \n",
       "1  1.143023  0.411128 -0.415390 -0.960516  0.041766 -1.244290 -0.800163   \n",
       "2  1.097862  0.476589 -0.359638 -0.942566  0.056413 -1.260244 -0.761508   \n",
       "3  1.103452  0.366112 -0.136889 -0.868879 -0.114063 -1.350382 -0.917053   \n",
       "4  1.156261  0.443427 -0.418569 -0.934572  0.043966 -1.243049 -0.774558   \n",
       "\n",
       "        726       727       728       729       730       731       732  \\\n",
       "0  0.129581 -1.226594 -0.223163  0.609400 -0.022221 -0.733969 -0.034820   \n",
       "1  0.134285 -1.157098 -0.009894  0.623618  0.070537 -0.811948 -0.063017   \n",
       "2  0.072833 -1.160706 -0.126908  0.542049 -0.025297 -0.809199 -0.098120   \n",
       "3  0.103647 -1.240219 -0.155452  0.564412 -0.051062 -0.800765 -0.131301   \n",
       "4  0.215238 -1.097093 -0.040102  0.619841  0.075169 -0.822426 -0.083557   \n",
       "\n",
       "        733       734       735       736       737       738       739  \\\n",
       "0  0.313441  0.183914 -0.549012 -1.346349  0.704137  0.329678  1.018111   \n",
       "1  0.195875  0.132494 -0.472655 -1.333030  0.717992  0.390614  0.962173   \n",
       "2  0.244973  0.123797 -0.511056 -1.353314  0.711025  0.376009  0.984575   \n",
       "3  0.263243  0.237330 -0.587270 -1.327193  0.683113  0.417219  0.999400   \n",
       "4  0.169311  0.124444 -0.488816 -1.325683  0.749879  0.395323  0.917498   \n",
       "\n",
       "        740       741       742       743       744       745       746  \\\n",
       "0  0.340090  0.799143 -0.126810 -0.326063  0.074100 -0.198835  0.311579   \n",
       "1  0.262227  0.900870 -0.082890 -0.337498  0.054027 -0.250429  0.272864   \n",
       "2  0.281475  0.811442 -0.114741 -0.319305  0.029654 -0.266911  0.264867   \n",
       "3  0.309212  0.910836 -0.273428 -0.190106 -0.002491 -0.064133  0.195051   \n",
       "4  0.295289  0.862854 -0.068503 -0.331440  0.049661 -0.274055  0.314875   \n",
       "\n",
       "        747       748       749       750       751       752       753  \\\n",
       "0 -0.047607 -0.019473 -0.386535 -1.092652 -0.932427 -0.356115 -0.496315   \n",
       "1  0.072126 -0.003761 -0.390650 -1.200388 -0.940271 -0.365464 -0.515058   \n",
       "2  0.021795 -0.044050 -0.415797 -1.123485 -0.934873 -0.339234 -0.418925   \n",
       "3 -0.095962 -0.140747 -0.341383 -1.013542 -0.881881 -0.480772 -0.607087   \n",
       "4  0.017540 -0.015881 -0.408418 -1.173379 -0.878732 -0.373150 -0.524417   \n",
       "\n",
       "        754       755       756       757       758       759       760  \\\n",
       "0  0.951529  1.134090 -0.301524 -0.653371  1.149359  0.122159  0.432706   \n",
       "1  0.924855  1.167289 -0.188000 -0.659250  1.178323  0.162583  0.248832   \n",
       "2  0.937384  1.150223 -0.289407 -0.631850  1.171261  0.138702  0.331728   \n",
       "3  0.905398  1.218029 -0.297764 -0.728446  1.194226  0.010579  0.502102   \n",
       "4  0.895135  1.139725 -0.247240 -0.661082  1.219325  0.185854  0.283039   \n",
       "\n",
       "        761       762       763       764       765       766       767  \n",
       "0 -1.536304  1.144165  0.402096  0.735742  0.011276  0.615236  0.555950  \n",
       "1 -1.504861  1.054700  0.330996  0.728609  0.084362  0.595130  0.555965  \n",
       "2 -1.531718  1.063338  0.340977  0.678672  0.015195  0.580006  0.542342  \n",
       "3 -1.501001  1.037894  0.368634  0.747409 -0.081113  0.629598  0.661931  \n",
       "4 -1.495237  1.063481  0.349460  0.709531  0.101135  0.548717  0.565705  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = load_exp_data(pred_prop='Elongation_value', fes=['embeddings'],\n",
    "                                  split_ratio=0.6, seed=666)\n",
    "print(df_train.shape, df_test.shape)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744247a6-160e-4f80-a3f4-905fa1e9a8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56eee0e5-2d21-4c16-97b8-9f31a888b313",
   "metadata": {},
   "source": [
    "## predict with ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de4a7dc3-0b84-476c-af20-66be50914e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:42:33.733652Z",
     "iopub.status.busy": "2023-10-07T13:42:33.732654Z",
     "iopub.status.idle": "2023-10-07T13:42:33.746617Z",
     "shell.execute_reply": "2023-10-07T13:42:33.746617Z",
     "shell.execute_reply.started": "2023-10-07T13:42:33.733652Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = 'r2'\n",
    "time_min = 3\n",
    "split_ratio = 0.7\n",
    "seed = 666"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848cc7af-7638-4c7e-8ad0-201694877d7c",
   "metadata": {},
   "source": [
    "### actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f41e66-5a4e-4ab2-92fd-f175a26eb26e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:42:33.748611Z",
     "iopub.status.busy": "2023-10-07T13:42:33.747615Z",
     "iopub.status.idle": "2023-10-07T13:43:46.084179Z",
     "shell.execute_reply": "2023-10-07T13:43:46.083181Z",
     "shell.execute_reply.started": "2023-10-07T13:42:33.748611Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231007_134235\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=5, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231007_134235\\\"\n",
      "AutoGluon Version:  0.6.1\n",
      "Python Version:     3.9.15\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Train Data Rows:    45\n",
      "Train Data Columns: 5\n",
      "Label Column: Tensile_value\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1542.73, 317.62, 993.04156, 274.51637)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    27104.22 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.0 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 5 | ['solution treatment temperature/deg', 'solution treatment time/min', 'cold rolling/%', 'annealing temperature/deg', 'annealing time/min']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', [])       : 3 | ['cold rolling/%', 'annealing temperature/deg', 'annealing time/min']\n",
      "\t\t('int', ['bool']) : 2 | ['solution treatment temperature/deg', 'solution treatment time/min']\n",
      "\t0.0s = Fit runtime\n",
      "\t5 features in original data used to generate 5 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.0 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.02s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 179.98s of the 179.97s of remaining time.\n",
      "\t0.59\t = Validation score   (r2)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 179.95s of the 179.95s of remaining time.\n",
      "\t0.3799\t = Validation score   (r2)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 179.93s of the 179.92s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 6) (20, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.0694\t = Validation score   (r2)\n",
      "\t1.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 166.56s of the 166.56s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.0694\t = Validation score   (r2)\n",
      "\t1.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 159.44s of the 159.44s of remaining time.\n",
      "\t0.876\t = Validation score   (r2)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 159.08s of the 159.08s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fes = ['actions']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4d33f-b454-49d6-aed4-af1430772438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8f9a66d-5354-4cc3-a5c6-788ee9f2213f",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e8ca3-432d-4bf4-8929-e0e9d0ed10d9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-07T13:43:46.084179Z",
     "iopub.status.idle": "2023-10-07T13:43:46.085175Z",
     "shell.execute_reply": "2023-10-07T13:43:46.084179Z",
     "shell.execute_reply.started": "2023-10-07T13:43:46.084179Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fes = ['text']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc7b2c9-e451-46c8-8b18-153df69b2f5e",
   "metadata": {},
   "source": [
    "### com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f8464d-276f-42bd-8652-e5df3932580f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-07T13:43:46.086173Z",
     "iopub.status.idle": "2023-10-07T13:43:46.086173Z",
     "shell.execute_reply": "2023-10-07T13:43:46.086173Z",
     "shell.execute_reply.started": "2023-10-07T13:43:46.086173Z"
    }
   },
   "outputs": [],
   "source": [
    "fes = ['com']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bebefcb-e683-440c-a459-0510a597c210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97793240-fc02-4960-95fa-dbc369ee2392",
   "metadata": {},
   "source": [
    "### com + actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17291c7f-5f02-4b2b-8b1e-be9a44120db5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-07T13:43:46.087170Z",
     "iopub.status.idle": "2023-10-07T13:43:46.087170Z",
     "shell.execute_reply": "2023-10-07T13:43:46.087170Z",
     "shell.execute_reply.started": "2023-10-07T13:43:46.087170Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fes = ['com', 'actions']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6f9d3-478c-42c6-87a3-503ed68be135",
   "metadata": {},
   "source": [
    "### com + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cccf3a-902c-49c0-a718-40bc3151c6bf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-07T13:43:46.087170Z",
     "iopub.status.idle": "2023-10-07T13:43:46.088167Z",
     "shell.execute_reply": "2023-10-07T13:43:46.088167Z",
     "shell.execute_reply.started": "2023-10-07T13:43:46.088167Z"
    }
   },
   "outputs": [],
   "source": [
    "fes = ['com', 'text']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dd520-5e7e-4090-ac65-8319a1f31bf4",
   "metadata": {},
   "source": [
    "### actions+text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2888171-b267-408b-b625-87ef1a46abbe",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-07T13:43:46.088167Z",
     "iopub.status.idle": "2023-10-07T13:43:46.089165Z",
     "shell.execute_reply": "2023-10-07T13:43:46.088167Z",
     "shell.execute_reply.started": "2023-10-07T13:43:46.088167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fes = ['actions', 'text']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c68d38-d9d9-478b-9636-841067cf8a25",
   "metadata": {},
   "source": [
    "### com+actions+text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683ac67-9ed0-46c3-aade-2e41f54f96e7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-07T13:43:46.090162Z",
     "iopub.status.idle": "2023-10-07T13:43:46.090162Z",
     "shell.execute_reply": "2023-10-07T13:43:46.090162Z",
     "shell.execute_reply.started": "2023-10-07T13:43:46.090162Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fes = ['com', 'actions', 'text']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00fe788-ff6d-4219-8f42-bfce63c88b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68f31531-34b0-405e-837f-7b1b133d7ce3",
   "metadata": {},
   "source": [
    "## predict with steelberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a0be5-7baa-4486-be94-1969640ec5ac",
   "metadata": {},
   "source": [
    "### embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26745eee-72dc-4631-b818-323fdecc7ef9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.248433Z",
     "iopub.status.idle": "2023-10-06T02:32:45.248433Z",
     "shell.execute_reply": "2023-10-06T02:32:45.248433Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.248433Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fes = ['embeddings']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97e7fc-171a-4f8f-817c-b977187ca8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9861868-38f4-4618-a063-d9b049ac5278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51ea675c-a518-4b01-b231-1faf38a1e075",
   "metadata": {},
   "source": [
    "### com+embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09a127-92d6-45fe-9453-87d796ef842e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.249430Z",
     "iopub.status.idle": "2023-10-06T02:32:45.250429Z",
     "shell.execute_reply": "2023-10-06T02:32:45.250429Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.250429Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fes = ['com', 'embeddings']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93e02e0-f754-4129-8bc5-9882d0201568",
   "metadata": {},
   "source": [
    "### com+embed_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e98c6e-c26b-4d01-ad00-250b50a0a249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f162796-36fa-4879-8de3-27e551feefcd",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.251425Z",
     "iopub.status.idle": "2023-10-06T02:32:45.252422Z",
     "shell.execute_reply": "2023-10-06T02:32:45.252422Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.252422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fes = ['com', 'embeddings']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    \n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    # tsne \n",
    "    train_embed = pd.DataFrame(TSNE(n_components=3, learning_rate='auto',\n",
    "                      init='random', perplexity=3).fit_transform(df_train.iloc[:,-768:]), \n",
    "                             columns=['tsne'+str(i) for i in range(3)])\n",
    "    test_embed = pd.DataFrame(TSNE(n_components=3, learning_rate='auto',\n",
    "                      init='random', perplexity=3).fit_transform(df_test.iloc[:,-768:]),\n",
    "                    columns=['tsne'+str(i) for i in range(3)])\n",
    "\n",
    "    drop_cols = list(df_train.columns[-768:])\n",
    "    df_train.drop(columns=drop_cols, inplace=True)\n",
    "    df_test.drop(columns=drop_cols, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_train = pd.concat([df_train, train_embed], axis=1)\n",
    "    df_test = pd.concat([df_test, test_embed], axis=1)\n",
    "\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aecc289-3c75-42a1-8a1c-fe67c798df40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f14cb-4dfd-4e2d-a293-bbaaa6809e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d933a-e474-468a-a9d9-a10a2ce0b7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3503c1-fb21-42ae-b462-8b4979513340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "954a211e-708b-4e3f-accb-1f9130445cc9",
   "metadata": {},
   "source": [
    "### com+text+embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652c9c2-699a-4c2a-816e-72b283fe21e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a65bf9-6c81-4c55-837b-910d6b16433a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.253420Z",
     "iopub.status.idle": "2023-10-06T02:32:45.253420Z",
     "shell.execute_reply": "2023-10-06T02:32:45.253420Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.253420Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fes = ['com', 'text', 'embeddings']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d0c38-854d-472e-b770-37619211fb3c",
   "metadata": {},
   "source": [
    "### com+actions+embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27068c32-ee04-45f5-b1f8-e6adbaf10279",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.255414Z",
     "iopub.status.idle": "2023-10-06T02:32:45.256412Z",
     "shell.execute_reply": "2023-10-06T02:32:45.255414Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.255414Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fes = ['com', 'actions', 'embeddings']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5e291-e4bd-4cb4-b0a6-0107296bdbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f227210f-aad3-4e52-b7aa-973d0bdf0bb6",
   "metadata": {},
   "source": [
    "### Text+actions+embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86f56c-cd6f-4191-b6e6-ce06c1a3d0e4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.256412Z",
     "iopub.status.idle": "2023-10-06T02:32:45.257408Z",
     "shell.execute_reply": "2023-10-06T02:32:45.257408Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.257408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fes = ['text', 'actions', 'embeddings']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8a87d-574b-415a-aefb-bc70bf360fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b93b95-8fa8-487a-b7c5-2d63b9a1a17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da9d0b-f801-4115-b59a-daf7ba4d6dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92a39b34-7f98-48fe-959a-4dc25f29bb64",
   "metadata": {},
   "source": [
    "## Multiple ColumnsMultiple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b5903-d652-4daa-9c88-6cf6da004bfe",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.258405Z",
     "iopub.status.idle": "2023-10-06T02:32:45.258405Z",
     "shell.execute_reply": "2023-10-06T02:32:45.258405Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.258405Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = 'r2'\n",
    "time_min=15\n",
    "seed=666"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568597e-142b-4b67-8966-ad92c64c89d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multilabel reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71b5f9-293f-4c69-9792-884b99cb96d0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.259403Z",
     "iopub.status.idle": "2023-10-06T02:32:45.259403Z",
     "shell.execute_reply": "2023-10-06T02:32:45.259403Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.259403Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.common.utils.utils import setup_outputdir\n",
    "from autogluon.core.utils.loaders import load_pkl\n",
    "from autogluon.core.utils.savers import save_pkl\n",
    "import os.path\n",
    "\n",
    "class MultilabelPredictor():\n",
    "    \"\"\" Tabular Predictor for predicting multiple columns in table.\n",
    "        Creates multiple TabularPredictor objects which you can also use individually.\n",
    "        You can access the TabularPredictor for a particular label via: `multilabel_predictor.get_predictor(label_i)`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List[str]\n",
    "            The ith element of this list is the column (i.e. `label`) predicted by the ith TabularPredictor stored in this object.\n",
    "        path : str, default = None\n",
    "            Path to directory where models and intermediate outputs should be saved.\n",
    "            If unspecified, a time-stamped folder called \"AutogluonModels/ag-[TIMESTAMP]\" will be created in the working directory to store all models.\n",
    "            Note: To call `fit()` twice and save all results of each fit, you must specify different `path` locations or don't specify `path` at all.\n",
    "            Otherwise files from first `fit()` will be overwritten by second `fit()`.\n",
    "            Caution: when predicting many labels, this directory may grow large as it needs to store many TabularPredictors.\n",
    "        problem_types : List[str], default = None\n",
    "            The ith element is the `problem_type` for the ith TabularPredictor stored in this object.\n",
    "        eval_metrics : List[str], default = None\n",
    "            The ith element is the `eval_metric` for the ith TabularPredictor stored in this object.\n",
    "        consider_labels_correlation : bool, default = True\n",
    "            Whether the predictions of multiple labels should account for label correlations or predict each label independently of the others.\n",
    "            If True, the ordering of `labels` may affect resulting accuracy as each label is predicted conditional on the previous labels appearing earlier in this list (i.e. in an auto-regressive fashion).\n",
    "            Set to False if during inference you may want to individually use just the ith TabularPredictor without predicting all the other labels.\n",
    "        kwargs :\n",
    "            Arguments passed into the initialization of each TabularPredictor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    multi_predictor_file = 'multilabel_predictor.pkl'\n",
    "\n",
    "    def __init__(self, labels, path=None, problem_types=None, eval_metrics=None, consider_labels_correlation=True, **kwargs):\n",
    "        if len(labels) < 2:\n",
    "            raise ValueError(\"MultilabelPredictor is only intended for predicting MULTIPLE labels (columns), use TabularPredictor for predicting one label (column).\")\n",
    "        if (problem_types is not None) and (len(problem_types) != len(labels)):\n",
    "            raise ValueError(\"If provided, `problem_types` must have same length as `labels`\")\n",
    "        if (eval_metrics is not None) and (len(eval_metrics) != len(labels)):\n",
    "            raise ValueError(\"If provided, `eval_metrics` must have same length as `labels`\")\n",
    "        self.path = setup_outputdir(path, warn_if_exist=False)\n",
    "        self.labels = labels\n",
    "        self.consider_labels_correlation = consider_labels_correlation\n",
    "        self.predictors = {}  # key = label, value = TabularPredictor or str path to the TabularPredictor for this label\n",
    "        if eval_metrics is None:\n",
    "            self.eval_metrics = {}\n",
    "        else:\n",
    "            self.eval_metrics = {labels[i] : eval_metrics[i] for i in range(len(labels))}\n",
    "        problem_type = None\n",
    "        eval_metric = None\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            path_i = self.path + \"Predictor_\" + label\n",
    "            if problem_types is not None:\n",
    "                problem_type = problem_types[i]\n",
    "            if eval_metrics is not None:\n",
    "                eval_metric = eval_metrics[i]\n",
    "            self.predictors[label] = TabularPredictor(label=label, problem_type=problem_type, eval_metric=eval_metric, path=path_i, **kwargs)\n",
    "\n",
    "    def fit(self, train_data, tuning_data=None, **kwargs):\n",
    "        \"\"\" Fits a separate TabularPredictor to predict each of the labels.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_data, tuning_data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                See documentation for `TabularPredictor.fit()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `fit()` call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        if isinstance(train_data, str):\n",
    "            train_data = TabularDataset(train_data)\n",
    "        if tuning_data is not None and isinstance(tuning_data, str):\n",
    "            tuning_data = TabularDataset(tuning_data)\n",
    "        train_data_og = train_data.copy()\n",
    "        if tuning_data is not None:\n",
    "            tuning_data_og = tuning_data.copy()\n",
    "        else:\n",
    "            tuning_data_og = None\n",
    "        save_metrics = len(self.eval_metrics) == 0\n",
    "        for i in range(len(self.labels)):\n",
    "            label = self.labels[i]\n",
    "            predictor = self.get_predictor(label)\n",
    "            if not self.consider_labels_correlation:\n",
    "                labels_to_drop = [l for l in self.labels if l != label]\n",
    "            else:\n",
    "                labels_to_drop = [self.labels[j] for j in range(i+1, len(self.labels))]\n",
    "            train_data = train_data_og.drop(labels_to_drop, axis=1)\n",
    "            if tuning_data is not None:\n",
    "                tuning_data = tuning_data_og.drop(labels_to_drop, axis=1)\n",
    "            print(f\"Fitting TabularPredictor for label: {label} ...\")\n",
    "            predictor.fit(train_data=train_data, tuning_data=tuning_data, **kwargs)\n",
    "            self.predictors[label] = predictor.path\n",
    "            if save_metrics:\n",
    "                self.eval_metrics[label] = predictor.eval_metric\n",
    "        self.save()\n",
    "\n",
    "    def predict(self, data, **kwargs):\n",
    "        \"\"\" Returns DataFrame with label columns containing predictions for each label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. If label columns are present in this data, they will be ignored. See documentation for `TabularPredictor.predict()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the predict() call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=False, **kwargs)\n",
    "\n",
    "    def predict_proba(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `predict_proba()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. See documentation for `TabularPredictor.predict()` and `TabularPredictor.predict_proba()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `predict_proba()` call for each TabularPredictor (also passed into a `predict()` call).\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=True, **kwargs)\n",
    "\n",
    "    def evaluate(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `evaluate()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to evalate predictions of all labels for, must contain all labels as columns. See documentation for `TabularPredictor.evaluate()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `evaluate()` call for each TabularPredictor (also passed into the `predict()` call).\n",
    "        \"\"\"\n",
    "        data = self._get_data(data)\n",
    "        eval_dict = {}\n",
    "        for label in self.labels:\n",
    "            print(f\"Evaluating TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            eval_dict[label] = predictor.evaluate(data, **kwargs)\n",
    "            if self.consider_labels_correlation:\n",
    "                data[label] = predictor.predict(data, **kwargs)\n",
    "        return eval_dict\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" Save MultilabelPredictor to disk. \"\"\"\n",
    "        for label in self.labels:\n",
    "            if not isinstance(self.predictors[label], str):\n",
    "                self.predictors[label] = self.predictors[label].path\n",
    "        save_pkl.save(path=self.path+self.multi_predictor_file, object=self)\n",
    "        print(f\"MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('{self.path}')\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\" Load MultilabelPredictor from disk `path` previously specified when creating this MultilabelPredictor. \"\"\"\n",
    "        path = os.path.expanduser(path)\n",
    "        if path[-1] != os.path.sep:\n",
    "            path = path + os.path.sep\n",
    "        return load_pkl.load(path=path+cls.multi_predictor_file)\n",
    "\n",
    "    def get_predictor(self, label):\n",
    "        \"\"\" Returns TabularPredictor which is used to predict this label. \"\"\"\n",
    "        predictor = self.predictors[label]\n",
    "        if isinstance(predictor, str):\n",
    "            return TabularPredictor.load(path=predictor)\n",
    "        return predictor\n",
    "\n",
    "    def _get_data(self, data):\n",
    "        if isinstance(data, str):\n",
    "            return TabularDataset(data)\n",
    "        return data.copy()\n",
    "\n",
    "    def _predict(self, data, as_proba=False, **kwargs):\n",
    "        data = self._get_data(data)\n",
    "        if as_proba:\n",
    "            predproba_dict = {}\n",
    "        for label in self.labels:\n",
    "            print(f\"Predicting with TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            if as_proba:\n",
    "                predproba_dict[label] = predictor.predict_proba(data, as_multiclass=True, **kwargs)\n",
    "            data[label] = predictor.predict(data, **kwargs)\n",
    "        if not as_proba:\n",
    "            return data[self.labels]\n",
    "        else:\n",
    "            return predproba_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73037e-40d6-458e-a819-649b0d703997",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1453ab-15ed-4433-bf00-545e4fb7e788",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.260401Z",
     "iopub.status.idle": "2023-10-06T02:32:45.260401Z",
     "shell.execute_reply": "2023-10-06T02:32:45.260401Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.260401Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, df_test = load_exp_data(pred_prop='all', fes=fes,\n",
    "                                  split_ratio=split_ratio, seed=seed)\n",
    "\n",
    "labels = ['Tensile_value','Yield_value','Elongation_value']  # which columns to predict based on the others\n",
    "problem_types = ['regression','regression','regression']  # type of each prediction problem (optional)\n",
    "eval_metrics = ['r2','r2','r2']  # metrics used to evaluate predictions for each label (optional)\n",
    "presets = ['best_quality', 'best_quality', 'best_quality']\n",
    "save_path = 'agModels-predictEducationClass'  # specifies folder to store trained models (optional)\n",
    "\n",
    "time_limit = 3  # how many seconds to train the TabularPredictor for each label, set much larger in your applications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b10ae-0eb0-4752-8884-256614efccc3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.261398Z",
     "iopub.status.idle": "2023-10-06T02:32:45.261398Z",
     "shell.execute_reply": "2023-10-06T02:32:45.261398Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.261398Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "multi_predictor = MultilabelPredictor(labels=labels, problem_types=problem_types, eval_metrics=eval_metrics, \n",
    "                                      path=save_path)\n",
    "\n",
    "multi_predictor.fit(df_train, time_limit=time_limit*60, presets=presets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e83ec-f47d-4cf8-be82-9d9735afddb5",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c052c5-237e-4e80-9e42-c96bb87e00b2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.262396Z",
     "iopub.status.idle": "2023-10-06T02:32:45.262396Z",
     "shell.execute_reply": "2023-10-06T02:32:45.262396Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.262396Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = multi_predictor.evaluate(df_test)\n",
    "print(evaluations)\n",
    "print(\"Evaluated using metrics:\", multi_predictor.eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5808aa5-85b0-4e5a-b921-43206e22a86b",
   "metadata": {},
   "source": [
    "## pred with compostion embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b5bf30-1698-448d-be82-83dd3293ac3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca8e36-cb85-4b42-920f-595e96a67df1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.263392Z",
     "iopub.status.idle": "2023-10-06T02:32:45.263392Z",
     "shell.execute_reply": "2023-10-06T02:32:45.263392Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.263392Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = 'r2'\n",
    "time_min = 3\n",
    "split_ratio = 0.7\n",
    "seed = 666\n",
    "fes=['com', 'embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd87ed7-bc16-4711-a642-2785e96fcd0e",
   "metadata": {},
   "source": [
    "### combed+CLS_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23347c-cfc8-4738-a1bb-b7fba727c5d2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.264390Z",
     "iopub.status.idle": "2023-10-06T02:32:45.264390Z",
     "shell.execute_reply": "2023-10-06T02:32:45.264390Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.264390Z"
    }
   },
   "outputs": [],
   "source": [
    "def ele_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_ele_embeddinngs(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    # return cls_pooling(model_output).detach().cpu().numpy()[0].tolist()\n",
    "    return ele_pooling(model_output).detach().cpu().numpy()\n",
    "\n",
    "def add_ele_embed(df):\n",
    "    coms = list(df.columns[-786:-768])\n",
    "\n",
    "    x = np.matrix(get_ele_embeddinngs(coms))\n",
    "    w1 = list(df.iloc[0, -786:-768])\n",
    "    y = np.average(x, axis=0, weights=w1)\n",
    "\n",
    "    for i in np.arange(1, df.shape[0]):\n",
    "        \n",
    "        w = list(df.iloc[i, -786:-768])\n",
    "        temp_y = np.average(x, axis=0, weights=w)\n",
    "        y = np.vstack((y, temp_y))\n",
    "\n",
    "    ele_df = pd.DataFrame(y, columns=['ele'+str(i) for i in range(768)])\n",
    "    \n",
    "    return ele_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d123502-fd2d-43c8-ac7c-18912d0d9e77",
   "metadata": {},
   "source": [
    "#### comd_tsne+embed_CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8857ebe-e909-4882-ae48-1e6d3b003af2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.265387Z",
     "iopub.status.idle": "2023-10-06T02:32:45.265387Z",
     "shell.execute_reply": "2023-10-06T02:32:45.265387Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.265387Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fes = ['com', 'embeddings']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    # df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "    #                                   split_ratio=split_ratio, seed=seed)\n",
    "    # print(df_train.shape, df_test.shape)\n",
    "    \n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "\n",
    "    drop_cols = list(df_train.columns[-786:-768])\n",
    "    df_train.drop(columns=drop_cols, inplace=True)\n",
    "    df_test.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    # normal\n",
    "    train_ele = add_ele_embed(df_train)\n",
    "    test_ele = add_ele_embed(df_test)\n",
    "    \n",
    "    # tsne \n",
    "    train_ele = pd.DataFrame(TSNE(n_components=3, learning_rate='auto',\n",
    "                      init='random', perplexity=3).fit_transform(add_ele_embed(df_train)), \n",
    "                             columns=['ele'+str(i) for i in range(3)])\n",
    "    test_ele = pd.DataFrame(TSNE(n_components=3, learning_rate='auto',\n",
    "                      init='random', perplexity=3).fit_transform(add_ele_embed(df_test)),\n",
    "                    columns=['ele'+str(i) for i in range(3)])\n",
    "\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    df_train = pd.concat([df_train, train_ele], axis=1)\n",
    "    df_test = pd.concat([df_test, test_ele], axis=1)\n",
    "\n",
    "    print(df_train.shape, df_test.shape)\n",
    "\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec62e6-0a56-487b-940a-19365f168876",
   "metadata": {},
   "source": [
    "#### combed+embed_CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936c020-c085-4ee5-a597-6bb5a5e22ea3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.265387Z",
     "iopub.status.idle": "2023-10-06T02:32:45.266385Z",
     "shell.execute_reply": "2023-10-06T02:32:45.266385Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.266385Z"
    }
   },
   "outputs": [],
   "source": [
    "fes = ['com', 'embeddings']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    # df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "    #                                   split_ratio=split_ratio, seed=seed)\n",
    "    # print(df_train.shape, df_test.shape)\n",
    "    \n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "\n",
    "    drop_cols = list(df_train.columns[-786:-768])\n",
    "    df_train.drop(columns=drop_cols, inplace=True)\n",
    "    df_test.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    # normal\n",
    "    train_ele = add_ele_embed(df_train)\n",
    "    test_ele = add_ele_embed(df_test)\n",
    "\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    df_train = pd.concat([df_train, train_ele], axis=1)\n",
    "    df_test = pd.concat([df_test, test_ele], axis=1)\n",
    "\n",
    "    print(df_train.shape, df_test.shape)\n",
    "\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e983103-1e44-47b0-a83c-a124312360bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b9a56-b706-41cf-abb5-a256f7f65866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa68d8f4-c90b-429f-a797-63709ea4f33b",
   "metadata": {},
   "source": [
    "### combed+CLS_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ad507-6c52-48ad-8cee-d389ac292eaf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.266385Z",
     "iopub.status.idle": "2023-10-06T02:32:45.267382Z",
     "shell.execute_reply": "2023-10-06T02:32:45.267382Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.267382Z"
    }
   },
   "outputs": [],
   "source": [
    "def ele_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 1]\n",
    "\n",
    "def get_ele_embeddinngs(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    # return cls_pooling(model_output).detach().cpu().numpy()[0].tolist()\n",
    "    return ele_pooling(model_output).detach().cpu().numpy()\n",
    "\n",
    "def add_ele_embed(df):\n",
    "    coms = list(df.columns[-786:-768])\n",
    "\n",
    "    x = np.matrix(get_ele_embeddinngs(coms))\n",
    "    w1 = list(df.iloc[0, -786:-768])\n",
    "    y = np.average(x, axis=0, weights=w1)\n",
    "\n",
    "    for i in np.arange(1, df.shape[0]):\n",
    "        \n",
    "        w = list(df.iloc[i, -786:-768])\n",
    "        temp_y = np.average(x, axis=0, weights=w)\n",
    "        y = np.vstack((y, temp_y))\n",
    "\n",
    "    ele_df = pd.DataFrame(y, columns=['ele'+str(i) for i in range(768)])\n",
    "    \n",
    "    return ele_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3527f7-236a-48d9-8b29-141c6709f589",
   "metadata": {},
   "source": [
    "#### comd_tsne+embd_CLS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199fa4c7-521b-4217-a785-61a657ab086a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.267382Z",
     "iopub.status.idle": "2023-10-06T02:32:45.268379Z",
     "shell.execute_reply": "2023-10-06T02:32:45.268379Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.268379Z"
    }
   },
   "outputs": [],
   "source": [
    "fes = ['com', 'embeddings']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    # df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "    #                                   split_ratio=split_ratio, seed=seed)\n",
    "    # print(df_train.shape, df_test.shape)\n",
    "    \n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "\n",
    "    drop_cols = list(df_train.columns[-786:-768])\n",
    "    df_train.drop(columns=drop_cols, inplace=True)\n",
    "    df_test.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    # normal\n",
    "    train_ele = add_ele_embed(df_train)\n",
    "    test_ele = add_ele_embed(df_test)\n",
    "\n",
    "    # tsne \n",
    "    train_ele = pd.DataFrame(TSNE(n_components=3, learning_rate='auto',\n",
    "                      init='random', perplexity=3).fit_transform(add_ele_embed(df_train)), \n",
    "                             columns=['ele'+str(i) for i in range(3)])\n",
    "    test_ele = pd.DataFrame(TSNE(n_components=3, learning_rate='auto',\n",
    "                      init='random', perplexity=3).fit_transform(add_ele_embed(df_test)),\n",
    "                    columns=['ele'+str(i) for i in range(3)])\n",
    "\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    df_train = pd.concat([df_train, train_ele], axis=1)\n",
    "    df_test = pd.concat([df_test, test_ele], axis=1)\n",
    "\n",
    "    print(df_train.shape, df_test.shape)\n",
    "\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf07d10a-6145-47d4-a4f1-15f8b5af9a1f",
   "metadata": {},
   "source": [
    "#### combed+embed_CLS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d55a4-7191-481e-80f8-251bbc40bdc4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.268379Z",
     "iopub.status.idle": "2023-10-06T02:32:45.269376Z",
     "shell.execute_reply": "2023-10-06T02:32:45.269376Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.269376Z"
    }
   },
   "outputs": [],
   "source": [
    "fes = ['com', 'embeddings']\n",
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    # df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "    #                                   split_ratio=split_ratio, seed=seed)\n",
    "    # print(df_train.shape, df_test.shape)\n",
    "    \n",
    "    df_train, df_test = load_exp_data(pred_prop=p, fes=fes,\n",
    "                                      split_ratio=split_ratio, seed=seed)\n",
    "\n",
    "    drop_cols = list(df_train.columns[-786:-768])\n",
    "    df_train.drop(columns=drop_cols, inplace=True)\n",
    "    df_test.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    # normal\n",
    "    train_ele = add_ele_embed(df_train)\n",
    "    test_ele = add_ele_embed(df_test)\n",
    "\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    df_train = pd.concat([df_train, train_ele], axis=1)\n",
    "    df_test = pd.concat([df_test, test_ele], axis=1)\n",
    "\n",
    "    print(df_train.shape, df_test.shape)\n",
    "\n",
    "    train_performance, test_performance = auto_reg(p, df_train, df_test,\n",
    "                                                   metric=metric, time_min=time_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b6e02-df80-417f-b147-9f4ca9cb750b",
   "metadata": {},
   "source": [
    "## pred with ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afcefa5a-eb0f-4ca8-a5a6-5c56d7c24df4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T08:18:39.466949Z",
     "iopub.status.busy": "2023-10-07T08:18:39.466949Z",
     "iopub.status.idle": "2023-10-07T08:18:39.472932Z",
     "shell.execute_reply": "2023-10-07T08:18:39.472932Z",
     "shell.execute_reply.started": "2023-10-07T08:18:39.466949Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Model evaluation for predicted results with \"r2\" and \"rootmean squared error\"\n",
    "    \"\"\"\n",
    "    y_true = y_true.detach().cpu().numpy() if torch.is_tensor(y_true) else y_true\n",
    "    y_pred = y_pred.detach().cpu().numpy() if torch.is_tensor(y_pred) else y_pred\n",
    "\n",
    "    r2 = round(r2_score(y_true, y_pred), 5)\n",
    "    rmse = round(mean_squared_error(y_true, y_pred, squared=False),5)\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 5) \n",
    "    result = {\"r2\":r2, \"rmse\":rmse, \"mae\":mae}\n",
    "    # print(f\"r2: {r2} \\n root_mean_squared_error: {rmse}\\n\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24aea59f-95ea-4b3c-8fff-ab69b4b8f5cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T13:50:03.435899Z",
     "iopub.status.busy": "2023-10-06T13:50:03.434901Z",
     "iopub.status.idle": "2023-10-06T13:50:03.451857Z",
     "shell.execute_reply": "2023-10-06T13:50:03.451857Z",
     "shell.execute_reply.started": "2023-10-06T13:50:03.435899Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbr_reg = GradientBoostingRegressor(learning_rate=0.01,\n",
    "                                    n_estimators=100,\n",
    "                                    random_state=0)\n",
    "\n",
    "svr_reg = SVR(C=0.8, epsilon=0.2)\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(base_score=0.4, \n",
    "                           learning_rate=0.1,\n",
    "                           random_state=789,\n",
    "                           verbosity=0,)\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=10,\n",
    "                               max_depth=2,\n",
    "                               random_state=123)\n",
    "\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=[2, 4, 4],\n",
    "                       activation='logistic',\n",
    "                       random_state=456,\n",
    "                       solver='lbfgs',\n",
    "                       alpha=0.001,\n",
    "                       batch_size=8,\n",
    "                       learning_rate_init=0.005,\n",
    "                       max_iter=200)\n",
    "\n",
    "def ml_pred_exp(reg, target='Tensile_value', random_seed=666):\n",
    "    \"\"\"\n",
    "    Using traditionanl ML model to predict single property with different festures combinations.\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    train_data, test_data = load_exp_data(pred_prop=target, fes=['actions'],\n",
    "                                      split_ratio=0.7, seed=random_seed)\n",
    "    \n",
    "#     # normalization\n",
    "#     train_data[target] = train_data[target]  / train_data[target].abs().max() \n",
    "#     test_data[target] = test_data[target]  / test_data[target].abs().max()\n",
    "    \n",
    "#     y_train = train_data.iloc[:,0].values \n",
    "#     y_test = test_data.iloc[:,0].values\n",
    "    \n",
    "    # # tsne \n",
    "    # X_train = pd.DataFrame(TSNE(n_components=3, learning_rate='auto',\n",
    "    #                   init='random', perplexity=3).fit_transform(train_data.iloc[:, 1:]), \n",
    "    #                          columns=['tsne'+str(i) for i in range(3)])\n",
    "    # X_test = pd.DataFrame(TSNE(n_components=3, learning_rate='auto',\n",
    "    #                   init='random', perplexity=3).fit_transform(test_data.iloc[:, 1:]),\n",
    "    #                 columns=['tsne'+str(i) for i in range(3)])\n",
    "\n",
    "    \n",
    "    X_train, y_train = train_data.iloc[:,1:].values, train_data.iloc[:,0].values\n",
    "    X_test, y_test = test_data.iloc[:,1:].values, test_data.iloc[:,0].values\n",
    "    # print(train_data.shape, test_data.shape)\n",
    "    regr = reg.fit(X_train, y_train)\n",
    "    # preds = regr.predict(X_test)\n",
    "\n",
    "    train_r2 = eval_model(y_train, regr.predict(X_train))['r2']\n",
    "    test_r2 = eval_model(y_test, regr.predict(X_test))['r2']\n",
    "\n",
    "    return train_r2, test_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a524daa-e881-4e0e-b52c-0965960da9dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T07:28:22.969564Z",
     "iopub.status.busy": "2023-10-06T07:28:22.969564Z",
     "iopub.status.idle": "2023-10-06T07:28:56.968662Z",
     "shell.execute_reply": "2023-10-06T07:28:56.968662Z",
     "shell.execute_reply.started": "2023-10-06T07:28:22.969564Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensile_value gbr_reg : (0.44548, -1.37703)\n",
      "Tensile_value svr_reg : (0.18982, -0.22504)\n",
      "Tensile_value xgb_reg : (0.29226, -0.85956)\n",
      "Tensile_value rf_reg : (0.3493, -0.39588)\n",
      "Tensile_value mlp_reg : (0.39668, -0.39254)\n",
      "Yield_value gbr_reg : (0.28477, -0.35381)\n",
      "Yield_value svr_reg : (0.39059, -0.38815)\n",
      "Yield_value xgb_reg : (0.40422, -0.28819)\n",
      "Yield_value rf_reg : (0.42089, -0.61466)\n",
      "Yield_value mlp_reg : (0.38594, -0.34012)\n",
      "Elongation_value gbr_reg : (0.34271, -0.37262)\n",
      "Elongation_value svr_reg : (0.4035, -0.5542)\n",
      "Elongation_value xgb_reg : (0.30025, -0.32516)\n",
      "Elongation_value rf_reg : (0.35658, -0.57345)\n",
      "Elongation_value mlp_reg : (0.33407, -0.24034)\n"
     ]
    }
   ],
   "source": [
    "for p in ['Tensile_value', 'Yield_value', 'Elongation_value']:\n",
    "    # for m in ['gbr_reg', 'svr_reg', 'xgb_reg', 'rf_reg', 'mlp_reg']:\n",
    "    for m in ['gbr_reg', 'svr_reg', 'xgb_reg', 'rf_reg', 'mlp_reg']:\n",
    "        output = ml_pred_exp(reg=eval('rf_reg'), target=p, random_seed=666)\n",
    "        print(p, m, ':', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3f6c134-95f4-4cb3-b380-4904ad226aee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T13:50:05.978676Z",
     "iopub.status.busy": "2023-10-06T13:50:05.978676Z",
     "iopub.status.idle": "2023-10-06T13:50:57.122946Z",
     "shell.execute_reply": "2023-10-06T13:50:57.122946Z",
     "shell.execute_reply.started": "2023-10-06T13:50:05.978676Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for h in [[4,]]:\n",
    "    # for act in ['tanh', 'logistic', 'relu']:\n",
    "    for act in ['relu']:\n",
    "    # for act in ['logistic', 'tanh', 'relu']:\n",
    "        for s in ['adam']:\n",
    "            for a in [1e-4]:\n",
    "                for b in [16, 32]:\n",
    "                    for lr in [1e-3]:\n",
    "                        for m in [600, 1000, 5000,10_000]:\n",
    "                            for r in [789, 666, 123]:\n",
    "                                mlp_reg = MLPRegressor(hidden_layer_sizes=h,\n",
    "                                                       activation=act,\n",
    "                                                       random_state=r,\n",
    "                                                       solver=s,\n",
    "                                                       alpha=a,\n",
    "                                                       batch_size=b,\n",
    "                                                       learning_rate='adaptive',\n",
    "                                                       learning_rate_init=lr,\n",
    "                                                       max_iter=m)\n",
    "                                # print(h, act, s, a, b, lr, m, r)\n",
    "                                with open('new_exp_mlp.csv', 'a+') as csvfile:  \n",
    "                                    # creating a csv writer object  \n",
    "                                    csvwriter = csv.writer(csvfile)\n",
    "                                    \n",
    "                                    train_r2, test_r2 = ml_pred_exp(reg=mlp_reg, target='Elongation_value', random_seed=r)\n",
    "                                    csvwriter.writerow([train_r2, test_r2, h, act, s, a, b, lr, m, r])\n",
    "                                    # print(output['r2'], h, act, s, a, b, lr, m, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461d03d-bb3b-447e-ac00-210e0a16f8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90efad-2fb5-498b-ae00-1a0e7b58ff8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01435e6b-a6f3-4c87-a432-fc0dc950794a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.272369Z",
     "iopub.status.idle": "2023-10-06T02:32:45.273366Z",
     "shell.execute_reply": "2023-10-06T02:32:45.273366Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.273366Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for h in [[1], [2,4,4], [2,4,8], [2,4]]:\n",
    "    for act in ['tanh', 'logistic', 'relu']:\n",
    "    # for act in ['logistic', 'tanh', 'relu']:\n",
    "        for s in ['lbfgs', 'sgd', 'adam']:\n",
    "            for a in [1e-3, 1e-4, 1e-5]:\n",
    "                for b in [4, 8, 16, 32]:\n",
    "                    for lr in [5e-3, 5e-4, 5e-5]:\n",
    "                        for m in [200, 400, 600]:\n",
    "                            for r in [456, 123, 789]:\n",
    "\n",
    "                                mlp_reg = MLPRegressor(hidden_layer_sizes=h,\n",
    "                                                       activation=act,\n",
    "                                                       random_state=r,\n",
    "                                                       solver=s,\n",
    "                                                       alpha=a,\n",
    "                                                       batch_size=b,\n",
    "                                                       learning_rate_init=lr,\n",
    "                                                       max_iter=m)\n",
    "                                # print(h, act, s, a, b, lr, m, r)\n",
    "                                with open('new_exp_mlp.csv', 'a+') as csvfile:  \n",
    "                                    # creating a csv writer object  \n",
    "                                    csvwriter = csv.writer(csvfile)\n",
    "                                    \n",
    "                                    output = ml_pred_exp(reg=mlp_reg, target='Tensile_value', random_seed=r)\n",
    "                                    csvwriter.writerow([output['r2'], h, act, s, a, b, lr, m, r])\n",
    "                                    # print(output['r2'], h, act, s, a, b, lr, m, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baff50e-36ab-4b2e-b449-3d7f9304a8c2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-06T02:32:45.273366Z",
     "iopub.status.idle": "2023-10-06T02:32:45.274364Z",
     "shell.execute_reply": "2023-10-06T02:32:45.274364Z",
     "shell.execute_reply.started": "2023-10-06T02:32:45.274364Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find best MLP parameters\n",
    "# for h in [[2,4,4], [2,4,8], [2,4]]:\n",
    "# with open('exp_mlp.csv', 'a+') as csvfile:  \n",
    "#     # creating a csv writer object  \n",
    "#     csvwriter = csv.writer(csvfile)  \n",
    "        \n",
    "    # writing the fields  \n",
    "    # csvwriter.writerow(fields)  \n",
    "        \n",
    "    # writing the data rows  \n",
    "    # csvwriter.writerows(rows) \n",
    "\n",
    "for h in [[1], [2,4,4], [2,4,8], [2,4]]:\n",
    "    for act in ['tanh', 'logistic', 'relu']:\n",
    "    # for act in ['logistic', 'tanh', 'relu']:\n",
    "        for s in ['lbfgs', 'sgd', 'adam']:\n",
    "            for a in [1e-2, 1e-1]:\n",
    "                for b in [4, 8, 16, 32]:\n",
    "                    for lr in [5e-3, 5e-4, 5e-5]:\n",
    "                        for m in [200, 400, 600]:\n",
    "                            for r in [456, 123, 789]:\n",
    "\n",
    "                                mlp_reg = MLPRegressor(hidden_layer_sizes=h,\n",
    "                                                       activation=act,\n",
    "                                                       random_state=r,\n",
    "                                                       solver=s,\n",
    "                                                       alpha=a,\n",
    "                                                       batch_size=b,\n",
    "                                                       learning_rate_init=lr,\n",
    "                                                       max_iter=m)\n",
    "                                # print(h, act, s, a, b, lr, m, r)\n",
    "                                with open('exp_mlp.csv', 'a+') as csvfile:  \n",
    "                                    # creating a csv writer object  \n",
    "                                    csvwriter = csv.writer(csvfile)\n",
    "                                    \n",
    "                                    output = ml_pred_exp(reg=mlp_reg, target='Tensile_value', random_seed=r)\n",
    "                                    csvwriter.writerow([output['r2'], h, act, s, a, b, lr, m, r])\n",
    "                                    # print(output['r2'], h, act, s, a, b, lr, m, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf738c-f30c-40e5-b73e-689e583d62a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1431a19-0f63-4ded-a726-00fdff926df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5991cc-c842-4030-9934-f191ac5db9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86070f1-125c-4448-83cc-d859f41e2582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463757c-cc97-4c46-bcae-29fe36c7e7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cff038-5dae-4796-b876-fdeca955582b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "411296a9-cf42-417d-92e8-52fbcbeecc73",
   "metadata": {},
   "source": [
    "## pred with number entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7322ea7e-a850-45e7-ac5e-b8844b39e7f1",
   "metadata": {},
   "source": [
    "### pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e67d26-d7af-4de7-8563-f772220e60d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:53:46.208900Z",
     "iopub.status.busy": "2023-10-07T13:53:46.208900Z",
     "iopub.status.idle": "2023-10-07T13:53:46.214884Z",
     "shell.execute_reply": "2023-10-07T13:53:46.214884Z",
     "shell.execute_reply.started": "2023-10-07T13:53:46.208900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Model evaluation for predicted results with \"r2\" and \"rootmean squared error\"\n",
    "    \"\"\"\n",
    "    y_true = y_true.detach().cpu().numpy() if torch.is_tensor(y_true) else y_true\n",
    "    y_pred = y_pred.detach().cpu().numpy() if torch.is_tensor(y_pred) else y_pred\n",
    "\n",
    "    r2 = round(r2_score(y_true, y_pred), 5)\n",
    "    rmse = round(mean_squared_error(y_true, y_pred, squared=False), 5)\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 5)\n",
    "    result = {\"r2\":r2, \"rmse\":rmse, \"mae\":mae}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1f4c6a-a22b-4949-baa8-1928e9803152",
   "metadata": {},
   "source": [
    "#### pt DataClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d32aa-2a87-476b-b493-b14aaec21714",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-07T13:43:46.104124Z",
     "iopub.status.idle": "2023-10-07T13:43:46.104124Z",
     "shell.execute_reply": "2023-10-07T13:43:46.104124Z",
     "shell.execute_reply.started": "2023-10-07T13:43:46.104124Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# containging text and ele features for pytorch\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, notes, targets, tokenizer):\n",
    "        self.texts = notes\n",
    "        # self.eles = eles.values if isinstance(eles, pd.DataFrame) else eles\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        text = str(self.texts[idx])\n",
    "        # ele = self.eles[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=64,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            \"labels\": torch.tensor(target, dtype=torch.float32).to(device),\n",
    "            # \"eles\": torch.tensor(ele, dtype=torch.float32).squeeze(0).to(device),\n",
    "            \"input_ids\": torch.tensor(encoding['input_ids']).squeeze(0).to(device),\n",
    "            \"attention_mask\": torch.tensor(encoding['attention_mask']).squeeze(0).to(device),\n",
    "            \"token_type_ids\": torch.tensor(encoding['token_type_ids']).squeeze(0).to(device),\n",
    "            # 'text':text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276ab6b-8d06-4290-b88b-0a277abdb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### CustomDataset class test\n",
    "# train_dataloader = DataLoader(CustomDataset(train_data['Text'], train_data.iloc[:, 0],\n",
    "#                                             tokenizer=tokenizer), batch_size=2)\n",
    "train_data, test_data = load_exp_data(pred_prop='Elongation_value', fes=['text'],\n",
    "                                      split_ratio=0.7, seed=666)\n",
    "print(train_data.shape, test_data.shape)\n",
    "\n",
    "val_dataloader = DataLoader(CustomDataset(test_data['Text'], test_data.iloc[:, 0],\n",
    "                                            tokenizer=tokenizer), batch_size=2)\n",
    "train_dataloader = DataLoader(CustomDataset(train_data['Text'], train_data.iloc[:, 0],\n",
    "                                            tokenizer=tokenizer), batch_size=2)\n",
    "\n",
    "for batch, s in enumerate(val_dataloader):\n",
    "    print(s['input_ids'].shape)\n",
    "    print(s['attention_mask'].shape)\n",
    "    if batch == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ac792-f418-4405-928e-db2e980c4c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5fce2b-0693-4141-91cb-c27b1e362e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "746fd5a4-f179-4a7e-a53c-43b40cf2f6db",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### pt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14472ee3-abfc-4c7c-af6a-0565ccf7166c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T12:34:04.369296Z",
     "iopub.status.busy": "2023-10-07T12:34:04.369296Z",
     "iopub.status.idle": "2023-10-07T12:34:04.389243Z",
     "shell.execute_reply": "2023-10-07T12:34:04.389243Z",
     "shell.execute_reply.started": "2023-10-07T12:34:04.369296Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        self.transformer= AutoModel.from_pretrained(self.name)\n",
    "        self.text_linear = nn.Sequential(\n",
    "            # nn.Linear(768, 3),        \n",
    "            # nn.Linear(3, 1),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.MultiheadAttention(768, 1, dropout=0.0),\n",
    "        )\n",
    "        self.gamma = nn.Parameter(torch.ones(128, 1))        \n",
    "        self.attention_layer = nn.MultiheadAttention(768, 1, dropout=0.1)\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=768, nhead=1), 1,\n",
    "                                             norm=None, enable_nested_tensor=True)\n",
    "        \n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "# transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "# src = torch.rand(10, 32, 512)\n",
    "# out = transformer_encoder(src)\n",
    "        \n",
    "        self.concat_linear = nn.Sequential(\n",
    "            # nn.Dropout(0.2, inplace=False),\n",
    "            # nn.LayerNorm(768),\n",
    "#             nn.LazyLinear(1),\n",
    "#             nn.ReLU(),\n",
    "            \n",
    "#             # nn.LayerNorm(768),\n",
    "#             nn.Dropout(0.1, inplace=False),\n",
    "#             nn.LazyLinear(4),\n",
    "#             nn.ReLU(),\n",
    "            # nn.LayerNorm(768),\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),           \n",
    "    \n",
    "            # # nn.Dropout(0.2, inplace=False),\n",
    "            # nn.Linear(256, 4),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.2, inplace=False),\n",
    "            nn.Linear(16, 1),\n",
    "            \n",
    "        )        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        \n",
    "#         # using [CLS]\n",
    "#         last_hidden_state = self.transformer(\n",
    "#             input_ids=input_ids, attention_mask=attention_mask, return_dict=True).last_hidden_state[:, 0]\n",
    "        \n",
    "        \n",
    "        # using [Seq]\n",
    "        X = self.transformer(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, return_dict=True).last_hidden_state\n",
    "\n",
    "        # attn_output, attn_output_weights = self.attention_layer(X, X, X)\n",
    "        \n",
    "        attn_output = self.encoder(X)\n",
    "        \n",
    "        # print(attn_output[:,0,:].shape)\n",
    "        # return attn_output\n",
    "\n",
    "# #         # print('attn_output', type(attn_output), attn_output.shape)    \n",
    "        output = self.concat_linear(attn_output[:, 0, :])\n",
    "        # print('output.shape', output.shape)\n",
    "        # output = self.concat_linear(output)\n",
    "\n",
    "        return output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38de3649-093a-4846-ad6c-3e92b9f89c81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T15:14:11.208348Z",
     "iopub.status.busy": "2023-10-06T15:14:11.207350Z",
     "iopub.status.idle": "2023-10-06T15:14:12.984117Z",
     "shell.execute_reply": "2023-10-06T15:14:12.983120Z",
     "shell.execute_reply.started": "2023-10-06T15:14:11.208348Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./../model_saved/checkpoint-140000 were not used when initializing DebertaV2Model: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 128])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask', 'token_type_ids'])\n",
      "torch.Size([40])\n"
     ]
    }
   ],
   "source": [
    "########### output shape test\n",
    "test_model = CustomModel(model_name).to(device)\n",
    "for batch, s in enumerate(train_dataloader):\n",
    "    print(s['attention_mask'].shape)\n",
    "    print(s.keys())\n",
    "    temp_ouput = test_model(**s)\n",
    "    print(temp_ouput.shape)\n",
    "    if batch == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e3a5a-53ea-4e4f-8fbf-8177e3e73ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3230722-8214-4831-9552-73a60890dedc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a460a5d-6d5b-4244-b672-4d85ea9048a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T12:34:13.900522Z",
     "iopub.status.busy": "2023-10-07T12:34:13.900522Z",
     "iopub.status.idle": "2023-10-07T12:34:16.160735Z",
     "shell.execute_reply": "2023-10-07T12:34:16.160735Z",
     "shell.execute_reply.started": "2023-10-07T12:34:13.900522Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(696, 2) (175, 2)\n"
     ]
    }
   ],
   "source": [
    "predict_label = 'Elongation_value'\n",
    "\n",
    "# exp data\n",
    "train_data, test_data = load_exp_data(pred_prop=predict_label, fes=['text'],\n",
    "                                      split_ratio=0.7, seed=123)\n",
    "\n",
    "# text data\n",
    "data = load_old_data(pred_prop=predict_label)\n",
    "train_data, test_data  = np.split(data.sample(frac=1, random_state=509, ignore_index=True), [int(0.8*len(data))])\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# train_data[predict_label] = train_data[predict_label] / 100\n",
    "# test_data[predict_label] = test_data[predict_label] / 100\n",
    "    \n",
    "# train_data[predict_label] = train_data[predict_label]  / train_data[predict_label].abs().max() \n",
    "# test_data[predict_label] = test_data[predict_label]  / test_data[predict_label].abs().max()\n",
    "\n",
    "# train_data[predict_label]= np.log(train_data[predict_label] + 1)\n",
    "# test_data[predict_label]= np.log(test_data[predict_label] + 1)\n",
    "\n",
    "\n",
    "print(train_data.shape, test_data.shape)\n",
    "val_dataloader = DataLoader(CustomDataset(test_data['Text'], test_data.iloc[:, 0],\n",
    "                                            tokenizer=tokenizer), batch_size=len(test_data))\n",
    "train_dataloader = DataLoader(CustomDataset(train_data['Text'], train_data.iloc[:, 0],\n",
    "                                            tokenizer=tokenizer), batch_size=45, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2cf7cb-1584-4e56-8283-c01337f3a83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2971d657-77e1-40da-a0d2-3d769da85152",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T12:36:45.780313Z",
     "iopub.status.busy": "2023-10-07T12:36:45.780313Z",
     "iopub.status.idle": "2023-10-07T12:50:43.145853Z",
     "shell.execute_reply": "2023-10-07T12:50:43.143859Z",
     "shell.execute_reply.started": "2023-10-07T12:36:45.780313Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./../model_saved/checkpoint-140000 were not used when initializing DebertaV2Model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 57%|████████████████████████████▍                     | 228/400 [13:55<10:30,  3.66s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reg_model = CustomModel(model_name).to(device)\n",
    "\n",
    "######################## training parameters ############################\n",
    "epoch = 400\n",
    "lr = 0.01\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(reg_model.parameters(), weight_decay=0.01) # best 3e-4\n",
    "# optimizer = torch.optim.LBFGS(params=reg_model.parameters(), lr=lr)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(reg_model.parameters(), lr=1e-54)\n",
    "# scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=50)\n",
    "# scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=lr,                                              \n",
    "                                               # end_factor=1e-4, total_iters=0.8*epoch)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "######################## start training ############################\n",
    "dt_string = datetime.now().strftime(\"%d_%m_%Y_%H.%M.%S\")\n",
    "writer = SummaryWriter('runs', flush_secs=20)\n",
    "test_r2_li = []\n",
    "for epoch_num in tqdm(range(epoch)):\n",
    "    # print(f\"\\n Epoch {epoch_num+1}\\n----------------------------------\")\n",
    "    reg_model.train()\n",
    "    for batch, inputs in enumerate(train_dataloader):\n",
    "        y = inputs['labels'].unsqueeze(1)\n",
    "        size = len(y)\n",
    "    \n",
    "        preds = reg_model(**inputs)\n",
    "        train_loss = loss_fn(preds, y)\n",
    "\n",
    "        result = eval_model(y, preds)\n",
    "        train_r2 = result['r2']\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()        \n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        # if batch % 4 == 0:\n",
    "        #     train_loss, current = train_loss.item(), (batch + 1) * len(y)\n",
    "        #     print(f\"Train batch loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    scheduler.step()\n",
    "    # print(f\"Train avg r2: {train_r2}\")\n",
    "    # print(f\"train_loss {train_loss}\")  \n",
    "\n",
    "    reg_model.eval()\n",
    "    num_batches = len(val_dataloader)\n",
    "    val_loss, val_r2 = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch, inputs in enumerate(val_dataloader):\n",
    "        \n",
    "            y = inputs['labels'].unsqueeze(1)\n",
    "            preds = reg_model(**inputs)\n",
    "            val_loss = loss_fn(preds, y).item()\n",
    "            result = eval_model(y, preds)\n",
    "            val_r2 = result['r2']\n",
    "\n",
    "    val_loss = val_loss/num_batches\n",
    "    val_r2 = val_r2/num_batches\n",
    "    # print(val_loss, val_r2)\n",
    "\n",
    "#     reg_model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch, inputs in enumerate(test_dataloader):\n",
    "#             y = inputs['labels'].unsqueeze(1)\n",
    "#             preds = reg_model(**inputs)\n",
    "#             test_r2 = eval_model(y, preds)['r2']\n",
    "#             print(f\"Test r2: {test_r2:>6f}\\n\")\n",
    "#             print('*'*10)\n",
    "#             print(compute_metrics_for_regression((preds, y)), test_r2)\n",
    "#             print('*'*10)\n",
    "\n",
    "    writer.add_scalars('model_'+dt_string+'/Loss',\n",
    "                    tag_scalar_dict = {'train_loss':train_loss,\n",
    "                                        'val_loss':val_loss},\n",
    "                    global_step = epoch_num+1)\n",
    "    writer.add_scalars('model_'+dt_string+'/R2',\n",
    "                        tag_scalar_dict = {'train_r2':train_r2,\n",
    "                                           'val_r2':val_r2},\n",
    "                                           # 'test_r2':test_r2},\n",
    "                        global_step = epoch_num+1)\n",
    "\n",
    "\n",
    "    # test_r2_li.append(test_r2)\n",
    "writer.close()\n",
    "\n",
    "# return test_r2_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789d95e-fa33-49b6-b982-0f777d63f4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733e53a-9fd1-45c1-9e3c-40d0e2004815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada3d52d-c529-4b81-acee-7bc39823cc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca825747-fae5-4ec4-a5a2-7928fa088f99",
   "metadata": {},
   "source": [
    "#### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2c359-863d-47e0-8066-a588fc545a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16c180b7-93be-45ad-b550-09ea60004008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T05:03:28.245056Z",
     "iopub.status.busy": "2023-10-07T05:03:28.245056Z",
     "iopub.status.idle": "2023-10-07T05:03:28.260016Z",
     "shell.execute_reply": "2023-10-07T05:03:28.260016Z",
     "shell.execute_reply.started": "2023-10-07T05:03:28.245056Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([653.0493, 653.0498, 653.0502, 653.0490, 653.0504, 653.0505, 653.0501,\n",
       "        653.0497, 653.0498, 653.0499, 653.0497, 653.0482, 653.0505, 653.0499,\n",
       "        653.0492, 653.0503], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c9b4f38-04d4-4ba5-a53b-385b46cf445c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T05:03:30.685077Z",
     "iopub.status.busy": "2023-10-07T05:03:30.685077Z",
     "iopub.status.idle": "2023-10-07T05:03:30.708016Z",
     "shell.execute_reply": "2023-10-07T05:03:30.708016Z",
     "shell.execute_reply.started": "2023-10-07T05:03:30.685077Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 896.0000],\n",
       "        [ 710.0000],\n",
       "        [1074.0000],\n",
       "        [ 596.0000],\n",
       "        [ 763.0000],\n",
       "        [ 440.0000],\n",
       "        [ 945.0000],\n",
       "        [ 670.0000],\n",
       "        [ 524.0000],\n",
       "        [ 337.5000],\n",
       "        [1389.0000],\n",
       "        [ 709.0000],\n",
       "        [ 688.0000],\n",
       "        [1090.0000],\n",
       "        [1600.0000],\n",
       "        [ 387.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b2622-b1dc-48c0-a2a6-22d038e1ac5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6240704-b6c6-4d52-b735-e1fa85c52c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "628e2815-8701-4ad4-b929-8644880c9597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T11:40:16.168265Z",
     "iopub.status.busy": "2023-10-06T11:40:16.167267Z",
     "iopub.status.idle": "2023-10-06T11:40:17.813943Z",
     "shell.execute_reply": "2023-10-06T11:40:17.813943Z",
     "shell.execute_reply.started": "2023-10-06T11:40:16.168265Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.8279], device='cuda:0')\n",
      "tensor([3.8273], device='cuda:0')\n",
      "tensor([3.8288], device='cuda:0')\n",
      "tensor([3.8293], device='cuda:0')\n",
      "tensor([3.8264], device='cuda:0')\n",
      "tensor([3.8308], device='cuda:0')\n",
      "tensor([3.8272], device='cuda:0')\n",
      "tensor([3.8272], device='cuda:0')\n",
      "tensor([3.8259], device='cuda:0')\n",
      "tensor([3.8278], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "reg_model.eval()\n",
    "for i in range(10):\n",
    "    with torch.no_grad():\n",
    "        preds = reg_model(input_ids=inputs['input_ids'][i].unsqueeze(0), \n",
    "                          attention_mask=inputs['attention_mask'][i].unsqueeze(0),\n",
    "                          token_type_ids=inputs['token_type_ids'][i].unsqueeze(0),\n",
    "                          labels=10)\n",
    "        print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1023846a-701a-4c2f-a26b-b45dc1523b7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T11:40:19.987676Z",
     "iopub.status.busy": "2023-10-06T11:40:19.987676Z",
     "iopub.status.idle": "2023-10-06T11:40:22.290535Z",
     "shell.execute_reply": "2023-10-06T11:40:22.289537Z",
     "shell.execute_reply.started": "2023-10-06T11:40:19.987676Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283,\n",
       "        3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283,\n",
       "        3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283,\n",
       "        3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283,\n",
       "        3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283, 3.8283],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model.eval()\n",
    "with torch.no_grad():\n",
    "    # for batch, inputs in enumerate(val_dataloader):\n",
    "    for batch, inputs in enumerate(train_dataloader):        \n",
    "        y = inputs['labels']\n",
    "        preds = reg_model(**inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9db7e-ce15-4070-bd79-58a15c5f7dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226f175-53e5-4ecb-8546-bc23e6dd66e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e79b9-ca54-425f-bc44-973c2ce8918c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c5a83-1572-4323-831f-1ae3dad67bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ece3d-a517-47b0-9738-e514530ee3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5f65205b-af02-419a-ae21-48f0e496326e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T11:13:57.227613Z",
     "iopub.status.busy": "2023-10-06T11:13:57.227613Z",
     "iopub.status.idle": "2023-10-06T11:13:57.254540Z",
     "shell.execute_reply": "2023-10-06T11:13:57.254540Z",
     "shell.execute_reply.started": "2023-10-06T11:13:57.227613Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,    14,   205,    52,    23,  3160,    12,   561,  2168,   631,\n",
       "           400,    24,    75,     9,     8,     6,    71,   542, 21404,    15,\n",
       "           701,     5,  2617,   307,  1252,    27,  1316,    30,     6,  3877,\n",
       "           542,  6882,   170,     9,     8,     6,    71,     5, 18142,    24,\n",
       "          6133,   841,    17,   339,     5,   203,    15,    14,     5, 18142,\n",
       "           468,    23,   971,  2287,    11,     5,    10,   204,   359,     7,\n",
       "             5, 25590,    15,    14,     6,   971,  2287,   468,    23,   287,\n",
       "            19,  2081,    24,    45,     7, 14864,   841,    17,   852,  1287,\n",
       "             9,   813,    19,   323,   297,    11,   398,   591,     2,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'][0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a6993-b199-4f68-8c28-87533d6659c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d878f5a-9201-4bcf-94d2-9935b39a11cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabbf0e-3067-4ed1-857d-c2ce3d9651ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78baed47-49d5-4758-9c07-3ae42fb47206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "96c0c07f-542f-4e30-b752-061a0dcd1357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T11:09:38.613964Z",
     "iopub.status.busy": "2023-10-06T11:09:38.613964Z",
     "iopub.status.idle": "2023-10-06T11:09:38.636902Z",
     "shell.execute_reply": "2023-10-06T11:09:38.636902Z",
     "shell.execute_reply.started": "2023-10-06T11:09:38.613964Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([7.0440, 6.9137, 6.9603, 6.8287, 7.1583, 6.8944, 6.9088, 7.1476, 5.7640,\n",
       "         7.0166, 5.9057, 6.7293, 6.8027, 6.8814, 6.7358, 7.0787, 7.0558, 6.8255,\n",
       "         7.0731, 6.9912], device='cuda:0'),\n",
       " 'input_ids': tensor([[  1,  14, 205,  ...,   0,   0,   0],\n",
       "         [  1,  14, 205,  ...,   0,   0,   0],\n",
       "         [  1,  14, 205,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [  1,  14, 205,  ...,   0,   0,   0],\n",
       "         [  1,  14, 205,  ...,   0,   0,   0],\n",
       "         [  1,  14, 205,  ...,   0,   0,   0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266e81b-a46d-4126-8f35-657b1cee3b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c13e64-9404-43a7-bcb2-3505d7fddf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae9fc5-d1c4-4c6e-899a-6a322d06bc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c7c8b-b5a5-4f3c-a1f4-cb7014784688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2d66a722-a655-460c-84a5-bfc70f4f86f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T10:56:25.753736Z",
     "iopub.status.busy": "2023-10-06T10:56:25.753736Z",
     "iopub.status.idle": "2023-10-06T10:56:25.777671Z",
     "shell.execute_reply": "2023-10-06T10:56:25.777671Z",
     "shell.execute_reply.started": "2023-10-06T10:56:25.753736Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'][0] == inputs['input_ids'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ffb5b5d0-3a00-4a74-92c8-e5b09eeb1eb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T10:56:51.716994Z",
     "iopub.status.busy": "2023-10-06T10:56:51.715996Z",
     "iopub.status.idle": "2023-10-06T10:56:51.731954Z",
     "shell.execute_reply": "2023-10-06T10:56:51.731954Z",
     "shell.execute_reply.started": "2023-10-06T10:56:51.716994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] The experimental steel was melted in vacuum induction melting furnace at first, and then hot-forged. After Some small plates were cut from the as-received hot forged plate, and then solution-treated at 1050 °C for 4 h. The solution-treated specimen was cold rolled to a thickness reduction of 79%. The the cold rolled specimen was performed by tempering at temperature of 590 °C for 15 min, followed by air cooling to room temperature.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3aa59437-28c6-4cf3-ae59-5fba8a6c7ea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T10:57:00.064346Z",
     "iopub.status.busy": "2023-10-06T10:57:00.064346Z",
     "iopub.status.idle": "2023-10-06T10:57:00.082298Z",
     "shell.execute_reply": "2023-10-06T10:57:00.082298Z",
     "shell.execute_reply.started": "2023-10-06T10:57:00.064346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] The experimental steel was melted in vacuum induction melting furnace at first, and then hot-forged. After Some small plates were cut from the as-received hot forged plate, and then solution-treated at 1050 °C for 4 h. The solution-treated specimen was cold rolled to a thickness reduction of 57%. The the cold rolled specimen was performed by tempering at temperature of 590 °C for 45 min, followed by air cooling to room temperature.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8f834-2233-4387-88a0-0ae85aa960fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78266f-6523-452d-a901-385e9ee476c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f7b668ac-545d-4a1f-b28a-c023e0a2b451",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T10:32:34.077804Z",
     "iopub.status.busy": "2023-10-06T10:32:34.077804Z",
     "iopub.status.idle": "2023-10-06T10:32:35.645612Z",
     "shell.execute_reply": "2023-10-06T10:32:35.645612Z",
     "shell.execute_reply.started": "2023-10-06T10:32:34.077804Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./../model_saved/checkpoint-140000 were not used when initializing DebertaV2Model: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.8571, -0.1978, -0.3608,  ..., -0.0811,  0.6296,  0.6619],\n",
       "         [ 0.1427, -0.2132, -0.9985,  ..., -0.0360, -0.9109, -0.5654],\n",
       "         [ 0.0971, -0.8286,  0.6010,  ...,  1.8184,  1.3930,  1.0558],\n",
       "         ...,\n",
       "         [ 1.2143,  0.3009, -0.3070,  ..., -0.3721,  0.2009, -0.6853],\n",
       "         [ 1.2143,  0.3009, -0.3070,  ..., -0.3721,  0.2009, -0.6853],\n",
       "         [ 1.2143,  0.3009, -0.3070,  ..., -0.3721,  0.2009, -0.6853]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_inputs_1 = tokenizer.encode_plus(\n",
    "            test_data.iloc[0,1],\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "temp_model_1 = AutoModel.from_pretrained(model_name)\n",
    "outputs_1 = temp_model(**temp_inputs_1)\n",
    "outputs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9d468ca1-4406-448b-ba10-38928a942fc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T10:32:35.900930Z",
     "iopub.status.busy": "2023-10-06T10:32:35.899932Z",
     "iopub.status.idle": "2023-10-06T10:32:37.473730Z",
     "shell.execute_reply": "2023-10-06T10:32:37.473730Z",
     "shell.execute_reply.started": "2023-10-06T10:32:35.900930Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./../model_saved/checkpoint-140000 were not used when initializing DebertaV2Model: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.8569, -0.1894, -0.3846,  ..., -0.0709,  0.6550,  0.6459],\n",
       "         [ 0.1206, -0.1955, -1.0034,  ..., -0.0077, -0.8635, -0.5864],\n",
       "         [ 0.0852, -0.8316,  0.5898,  ...,  1.8501,  1.3794,  1.0458],\n",
       "         ...,\n",
       "         [ 1.2143,  0.3009, -0.3070,  ..., -0.3721,  0.2009, -0.6853],\n",
       "         [ 1.2143,  0.3009, -0.3070,  ..., -0.3721,  0.2009, -0.6853],\n",
       "         [ 1.2143,  0.3009, -0.3070,  ..., -0.3721,  0.2009, -0.6853]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_inputs_2 = tokenizer.encode_plus(\n",
    "            test_data.iloc[1, 1],\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "temp_model_2 = AutoModel.from_pretrained(model_name)\n",
    "outputs_2 = temp_model(**temp_inputs_2)\n",
    "outputs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "975f2e8e-bd41-4301-a1db-cde16e58e69d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T10:32:37.725509Z",
     "iopub.status.busy": "2023-10-06T10:32:37.725509Z",
     "iopub.status.idle": "2023-10-06T10:32:37.739471Z",
     "shell.execute_reply": "2023-10-06T10:32:37.739471Z",
     "shell.execute_reply.started": "2023-10-06T10:32:37.725509Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_1.last_hidden_state.equal(outputs_2.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1948ea79-2e03-4876-a88c-3bf1534dddaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T10:40:12.404148Z",
     "iopub.status.busy": "2023-10-06T10:40:12.404148Z",
     "iopub.status.idle": "2023-10-06T10:40:12.491913Z",
     "shell.execute_reply": "2023-10-06T10:40:12.491913Z",
     "shell.execute_reply.started": "2023-10-06T10:40:12.404148Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.1118], device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model(**temp_inputs_2.to(device), labels=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a153c76e-8bfb-4e9b-a2f1-12ffa2019b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T10:47:09.466814Z",
     "iopub.status.busy": "2023-10-06T10:47:09.466814Z",
     "iopub.status.idle": "2023-10-06T10:47:09.476788Z",
     "shell.execute_reply": "2023-10-06T10:47:09.476788Z",
     "shell.execute_reply.started": "2023-10-06T10:47:09.466814Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The experimental steel was melted in vacuum induction melting furnace at first, and then hot-forged. After Some small plates were cut from the as-received hot forged plate, and then solution-treated at 1050 °C for 4 h. The solution-treated specimen was cold rolled to a thickness reduction of 79%. The the cold rolled specimen was performed by tempering at temperature of 590 °C for 15 min, followed by air cooling to room temperature.'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.iloc[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfdcbb5c-dcd3-4d25-9c5b-3d8864756406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T14:51:17.691041Z",
     "iopub.status.busy": "2023-10-06T14:51:17.690043Z",
     "iopub.status.idle": "2023-10-06T14:51:18.416774Z",
     "shell.execute_reply": "2023-10-06T14:51:18.415776Z",
     "shell.execute_reply.started": "2023-10-06T14:51:17.691041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.54180908, 1.54061759, 1.53958488, 1.53371763, 1.54095399,\n",
       "       1.54053545, 1.54204571, 1.54078841, 1.54007661, 1.539958  ,\n",
       "       1.54051137, 1.54105055, 1.53670955, 1.53353095, 1.54121029,\n",
       "       1.54050779, 1.53656387, 1.5398519 , 1.5402981 , 1.54117703])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(20):\n",
    "    temp_inputs = tokenizer.encode_plus(\n",
    "                test_data.iloc[i, 1],\n",
    "                add_special_tokens=True,\n",
    "                max_length=128,\n",
    "                return_token_type_ids=True,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt')\n",
    "    result += reg_model(**temp_inputs.to(device), labels=100).detach().cpu().numpy().tolist()\n",
    "eval_model(y.detach().cpu().numpy(), np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00743377-9caf-49e2-9236-9bacf10d5120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a626469c-1714-41fd-96be-e17850658f4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T14:51:21.809924Z",
     "iopub.status.busy": "2023-10-06T14:51:21.809924Z",
     "iopub.status.idle": "2023-10-06T14:51:21.824884Z",
     "shell.execute_reply": "2023-10-06T14:51:21.824884Z",
     "shell.execute_reply.started": "2023-10-06T14:51:21.809924Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.88  ],\n",
       "       [11.2624],\n",
       "       [15.4273],\n",
       "       [11.8544],\n",
       "       [ 8.85  ],\n",
       "       [ 8.9926],\n",
       "       [ 7.9   ],\n",
       "       [11.4785],\n",
       "       [ 3.6614],\n",
       "       [12.4898],\n",
       "       [ 5.32  ],\n",
       "       [ 4.9911],\n",
       "       [10.2694],\n",
       "       [14.8314],\n",
       "       [ 7.21  ],\n",
       "       [ 2.7722],\n",
       "       [12.1586],\n",
       "       [12.8377],\n",
       "       [10.0152],\n",
       "       [ 5.7666]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162b19f-99e9-4d30-bde4-35dbeea6d06d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39b8f69d-1974-46c7-b828-53e31eb823cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T14:51:28.961975Z",
     "iopub.status.busy": "2023-10-06T14:51:28.960978Z",
     "iopub.status.idle": "2023-10-06T14:51:28.979954Z",
     "shell.execute_reply": "2023-10-06T14:51:28.979954Z",
     "shell.execute_reply.started": "2023-10-06T14:51:28.961975Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r2': -4.76816, 'rmse': 8.47942, 'mae': 7.70886}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(y.detach().cpu().numpy(), np.array(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2caa0-55c6-4e59-8591-484c89d6d695",
   "metadata": {},
   "source": [
    "### pytorch simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983c789-b345-4abc-ae6a-f3f0499feecb",
   "metadata": {},
   "source": [
    "#### dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d30fa2ce-1886-4e0a-be12-6a7e9a2831b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:54:36.142076Z",
     "iopub.status.busy": "2023-10-07T13:54:36.141079Z",
     "iopub.status.idle": "2023-10-07T13:54:38.224508Z",
     "shell.execute_reply": "2023-10-07T13:54:38.224508Z",
     "shell.execute_reply.started": "2023-10-07T13:54:36.142076Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 769) (20, 769)\n"
     ]
    }
   ],
   "source": [
    "# containging text and ele features for pytorch\n",
    "class CustomSimpleDataset(Dataset):\n",
    "    def __init__(self, embeddings, targets):\n",
    "        self.embeddings = embeddings.values if isinstance(embeddings, pd.DataFrame) else embeddings\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        embedding = self.embeddings[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        return {\n",
    "            \"labels\": torch.tensor(target, dtype=torch.float32).to(device),\n",
    "            \"embeddings\": torch.tensor(embedding, dtype=torch.float32).squeeze(0).to(device),\n",
    "        }\n",
    "    \n",
    "###################### CustomSimpleDataset class test\n",
    "# train_dataloader = DataLoader(CustomDataset(train_data['Text'], train_data.iloc[:, 0],\n",
    "#                                             tokenizer=tokenizer), batch_size=2)\n",
    "train_data, test_data = load_exp_data(pred_prop='Elongation_value', fes=['embeddings'],\n",
    "                                      split_ratio=0.7, seed=666)\n",
    "print(train_data.shape, test_data.shape)\n",
    "\n",
    "val_dataloader = DataLoader(CustomSimpleDataset(test_data.iloc[:, 1:], test_data.iloc[:, 0]),\n",
    "                                    batch_size=2)\n",
    "train_dataloader = DataLoader(CustomSimpleDataset(train_data.iloc[:, 1:], train_data.iloc[:, 0]),\n",
    "                                    batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49995a06-fb67-4316-9f6a-7e8dc57f54ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:54:38.815165Z",
     "iopub.status.busy": "2023-10-07T13:54:38.815165Z",
     "iopub.status.idle": "2023-10-07T13:54:38.821149Z",
     "shell.execute_reply": "2023-10-07T13:54:38.821149Z",
     "shell.execute_reply.started": "2023-10-07T13:54:38.815165Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "for batch, s in enumerate(val_dataloader):\n",
    "    print(s['embeddings'].shape)\n",
    "    if batch == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e8597-9ec3-420a-a196-16dea6438860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a0fa677-1a68-4fff-8723-a7165df1de1c",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c1c077a-3263-4d0c-a6fe-c361cbb78341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:56:46.327941Z",
     "iopub.status.busy": "2023-10-07T13:56:46.326944Z",
     "iopub.status.idle": "2023-10-07T13:56:46.333924Z",
     "shell.execute_reply": "2023-10-07T13:56:46.333924Z",
     "shell.execute_reply.started": "2023-10-07T13:56:46.326944Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomSimpleModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.concat_linear = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # nn.Linear(256, 256),\n",
    "            # nn.ReLU(),\n",
    "            \n",
    "        \n",
    "#             nn.Linear(256, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 256),\n",
    "#             nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(16, 1),            \n",
    "        )        \n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        output = self.concat_linear(embeddings)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "786019de-92f6-4f3b-ac14-8cdfeb89da46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:56:47.033836Z",
     "iopub.status.busy": "2023-10-07T13:56:47.033836Z",
     "iopub.status.idle": "2023-10-07T13:56:47.052785Z",
     "shell.execute_reply": "2023-10-07T13:56:47.052785Z",
     "shell.execute_reply.started": "2023-10-07T13:56:47.033836Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n",
      "dict_keys(['labels', 'embeddings'])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "########### output shape test\n",
    "test_model = CustomSimpleModel(model_name).to(device)\n",
    "for batch, s in enumerate(train_dataloader):\n",
    "    print(s['embeddings'].shape)\n",
    "    print(s.keys())\n",
    "    temp_ouput = test_model(**s)\n",
    "    print(temp_ouput.shape)\n",
    "    if batch == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2df0ce40-66e0-4c29-808b-326edd1eaee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:56:29.063720Z",
     "iopub.status.busy": "2023-10-07T13:56:29.063720Z",
     "iopub.status.idle": "2023-10-07T13:56:29.081672Z",
     "shell.execute_reply": "2023-10-07T13:56:29.081672Z",
     "shell.execute_reply.started": "2023-10-07T13:56:29.063720Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([41.7300, 23.5800], device='cuda:0'),\n",
       " 'embeddings': tensor([[-0.9160, -0.1278, -0.4125,  ...,  0.0113,  0.6152,  0.5559],\n",
       "         [-0.7693, -0.1105, -0.4356,  ...,  0.0844,  0.5951,  0.5560]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "477b6ca9-25b9-4935-8cc7-c4dd03e3f84d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T13:56:48.712118Z",
     "iopub.status.busy": "2023-10-07T13:56:48.712118Z",
     "iopub.status.idle": "2023-10-07T13:56:48.727078Z",
     "shell.execute_reply": "2023-10-07T13:56:48.727078Z",
     "shell.execute_reply.started": "2023-10-07T13:56:48.712118Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2731],\n",
       "        [0.2729]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d267f-b507-4583-8436-10c43d1f113c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1141e0f6-3958-44c9-b45f-d074dcba761d",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "05d82eda-ee82-4d0d-892b-1d810a1e7e73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T14:20:07.012989Z",
     "iopub.status.busy": "2023-10-07T14:20:07.012989Z",
     "iopub.status.idle": "2023-10-07T14:20:09.070387Z",
     "shell.execute_reply": "2023-10-07T14:20:09.070387Z",
     "shell.execute_reply.started": "2023-10-07T14:20:07.012989Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 769) (20, 769)\n"
     ]
    }
   ],
   "source": [
    "predict_label = 'Yield_value'\n",
    "\n",
    "# exp data\n",
    "train_data, test_data = load_exp_data(pred_prop=predict_label, fes=['embeddings'],\n",
    "                                      split_ratio=0.7, seed=123)\n",
    "\n",
    "# # text data\n",
    "# data = load_old_data(pred_prop=predict_label)\n",
    "# train_data, test_data  = np.split(data.sample(frac=1, random_state=509, ignore_index=True), [int(0.8*len(data))])\n",
    "# test_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# train_data[predict_label] = train_data[predict_label] / 100\n",
    "# test_data[predict_label] = test_data[predict_label] / 100\n",
    "    \n",
    "train_data[predict_label] = train_data[predict_label]  / train_data[predict_label].abs().max() \n",
    "test_data[predict_label] = test_data[predict_label]  / test_data[predict_label].abs().max()\n",
    "\n",
    "# train_data[predict_label]= np.log(train_data[predict_label] + 1)\n",
    "# test_data[predict_label]= np.log(test_data[predict_label] + 1)\n",
    "\n",
    "\n",
    "print(train_data.shape, test_data.shape)\n",
    "val_dataloader = DataLoader(CustomSimpleDataset(test_data.iloc[:, 1:], test_data.iloc[:, 0]),\n",
    "                                    batch_size=len(test_data))\n",
    "train_dataloader = DataLoader(CustomSimpleDataset(train_data.iloc[:, 1:], train_data.iloc[:, 0]),\n",
    "                                    batch_size=45, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a22c7d-d3d0-4cfc-8869-60c341d67e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cbc950e1-95e7-442c-90a4-0b9aa4f5491f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T14:20:12.253983Z",
     "iopub.status.busy": "2023-10-07T14:20:12.253983Z",
     "iopub.status.idle": "2023-10-07T14:23:08.717226Z",
     "shell.execute_reply": "2023-10-07T14:23:08.717226Z",
     "shell.execute_reply.started": "2023-10-07T14:20:12.253983Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 1000/1000 [00:17<00:00, 57.72it/s]\n",
      "100%|████████████████████████████████████████████████| 1000/1000 [00:18<00:00, 55.04it/s]\n",
      "100%|████████████████████████████████████████████████| 1000/1000 [00:17<00:00, 56.55it/s]\n",
      "100%|████████████████████████████████████████████████| 1000/1000 [00:17<00:00, 56.74it/s]\n",
      "100%|████████████████████████████████████████████████| 1000/1000 [00:17<00:00, 56.90it/s]\n",
      "100%|████████████████████████████████████████████████| 1000/1000 [00:17<00:00, 57.87it/s]\n",
      "100%|████████████████████████████████████████████████| 1000/1000 [00:17<00:00, 57.20it/s]\n",
      "100%|████████████████████████████████████████████████| 1000/1000 [00:17<00:00, 57.54it/s]\n",
      "100%|████████████████████████████████████████████████| 1000/1000 [00:17<00:00, 56.96it/s]\n",
      "100%|████████████████████████████████████████████████| 1000/1000 [00:18<00:00, 54.68it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    reg_model = CustomSimpleModel(model_name).to(device)\n",
    "\n",
    "    ######################## training parameters ############################\n",
    "    epoch = 1000\n",
    "    lr = 0.1\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(reg_model.parameters(), weight_decay=0.01) # best 3e-4\n",
    "    # optimizer = torch.optim.LBFGS(params=reg_model.parameters(), lr=lr)\n",
    "\n",
    "    # optimizer = torch.optim.AdamW(reg_model.parameters(), lr=1e-54)\n",
    "    # scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=50)\n",
    "    # scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=lr,                                              \n",
    "    #                                                end_factor=1e-4, total_iters=0.8*epoch)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.8)\n",
    "\n",
    "    ######################## start training ############################\n",
    "    dt_string = datetime.now().strftime(\"%d_%m_%Y_%H.%M.%S\")\n",
    "    writer = SummaryWriter('runs', flush_secs=20)\n",
    "    test_r2_li = []\n",
    "    for epoch_num in tqdm(range(epoch)):\n",
    "        # print(f\"\\n Epoch {epoch_num+1}\\n----------------------------------\")\n",
    "        reg_model.train()\n",
    "        for batch, inputs in enumerate(train_dataloader):\n",
    "            y = inputs['labels'].unsqueeze(1)\n",
    "            size = len(y)\n",
    "\n",
    "            preds = reg_model(**inputs)\n",
    "            train_loss = loss_fn(preds, y)\n",
    "\n",
    "            result = eval_model(y, preds)\n",
    "            train_r2 = result['r2']\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()        \n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # if batch % 4 == 0:\n",
    "            #     train_loss, current = train_loss.item(), (batch + 1) * len(y)\n",
    "            #     print(f\"Train batch loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        scheduler.step()\n",
    "        # print(f\"Train avg r2: {train_r2}\")\n",
    "        # print(f\"train_loss {train_loss}\")  \n",
    "\n",
    "        reg_model.eval()\n",
    "        num_batches = len(val_dataloader)\n",
    "        val_loss, val_r2 = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch, inputs in enumerate(val_dataloader):\n",
    "\n",
    "                y = inputs['labels'].unsqueeze(1)\n",
    "                preds = reg_model(**inputs)\n",
    "                val_loss = loss_fn(preds, y).item()\n",
    "                result = eval_model(y, preds)\n",
    "                val_r2 = result['r2']\n",
    "\n",
    "        val_loss = val_loss/num_batches\n",
    "        val_r2 = val_r2/num_batches\n",
    "        # print(val_loss, val_r2)\n",
    "\n",
    "    #     reg_model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         for batch, inputs in enumerate(test_dataloader):\n",
    "    #             y = inputs['labels'].unsqueeze(1)\n",
    "    #             preds = reg_model(**inputs)\n",
    "    #             test_r2 = eval_model(y, preds)['r2']\n",
    "    #             print(f\"Test r2: {test_r2:>6f}\\n\")\n",
    "    #             print('*'*10)\n",
    "    #             print(compute_metrics_for_regression((preds, y)), test_r2)\n",
    "    #             print('*'*10)\n",
    "\n",
    "        writer.add_scalars('model_'+dt_string+'/Loss',\n",
    "                        tag_scalar_dict = {'train_loss':train_loss,\n",
    "                                            'val_loss':val_loss},\n",
    "                        global_step = epoch_num+1)\n",
    "        writer.add_scalars('model_'+dt_string+'/R2',\n",
    "                            tag_scalar_dict = {'train_r2':train_r2,\n",
    "                                               'val_r2':val_r2},\n",
    "                                               # 'test_r2':test_r2},\n",
    "                            global_step = epoch_num+1)\n",
    "\n",
    "\n",
    "        # test_r2_li.append(test_r2)\n",
    "    writer.close()\n",
    "\n",
    "    # return test_r2_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a46879-9609-4791-8809-ece0cf41c04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9ddb6-48b9-4549-9d9d-39379790a70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff3563-b367-4ee0-af4e-9bdc5046173d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2bedc1-e25d-4407-b5eb-eec2eb6392b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6df3e-e5b3-4a34-924c-52e97bf2a73a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530bfd4-07a2-4c64-91f5-2b0803cf75d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c935bc7-5c91-407f-9b28-f2c28a883807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0401d1f-2d2f-41e6-b8c7-8749cd64f35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d4d1d-d5db-43f3-a724-cd265650fea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b817922-2b96-4446-a4ce-87c7b155ae9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e482e-293d-4045-8ba2-70b48acccc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a985105-3087-48e5-b80f-951acab1a63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ffde0-eca3-430b-838c-4715b696de80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb2ea7-4b57-4b57-bee6-6976fec5f761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9595549e-6516-4082-a470-02cf5e375513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310800f-468f-4486-9e83-b6ae873a0f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20f976-38ef-4dad-b5dd-4bcf9e6a5f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62441d37-2a51-4842-871a-33d2364a4853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53cd48-eea2-43f2-b5c9-7131fa735d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49d488-58a6-4b2b-ab80-2c722adb7a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d25fee-d80f-44e3-baca-ff1fef8ebd88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399c383b-5484-4267-8bf3-a66e23dcc2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d77183-a354-4399-a0ed-5f70f1a072db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70d97a-df22-41c9-a32e-48875d08a3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79869959-ffff-4315-a528-86fccc25bad9",
   "metadata": {},
   "source": [
    "### hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92675570-d789-4bee-a2e3-fe229ac6c419",
   "metadata": {},
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b4148b-73a0-417d-ba5c-16a1446742b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-07T02:55:33.461027Z",
     "iopub.status.busy": "2023-10-07T02:55:33.460029Z",
     "iopub.status.idle": "2023-10-07T02:55:33.479976Z",
     "shell.execute_reply": "2023-10-07T02:55:33.479976Z",
     "shell.execute_reply.started": "2023-10-07T02:55:33.461027Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics_for_regression(eval_preds):\n",
    "    \n",
    "    logits, labels = eval_preds\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    print(eval_preds)\n",
    "\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    rmse = mean_squared_error(labels, logits, squared=False)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "    smape = 1/len(labels) * np.sum(2 * np.abs(logits-labels) / (np.abs(labels) + np.abs(logits))*100)\n",
    "\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"smape\": smape}\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    norm = BertNormalizer(clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=False)\n",
    "    examples['text'] = list(map(norm.normalize_str, examples['text']))\n",
    "    result = tokenizer(examples['text'], padding='max_length', \n",
    "                        max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    result['label'] = [l for l in examples['label']]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2794033-2bab-4ffd-b76c-f41997b8537b",
   "metadata": {},
   "source": [
    "#### hf DataClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92871c6a-bc66-4b71-9fe9-567ac0848d17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T08:01:43.282194Z",
     "iopub.status.busy": "2023-10-06T08:01:43.282194Z",
     "iopub.status.idle": "2023-10-06T08:01:45.113299Z",
     "shell.execute_reply": "2023-10-06T08:01:45.113299Z",
     "shell.execute_reply.started": "2023-10-06T08:01:43.282194Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propery = 'Tensile_value'\n",
    "\n",
    "train_data, test_data = load_exp_data(pred_prop=propery, fes=['text'],\n",
    "                                      split_ratio=0.7, seed=789)\n",
    "df = pd.concat([train_data, test_data])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e0a890f-8338-4aaf-b5f6-e02a7e6bfba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T08:10:35.107445Z",
     "iopub.status.busy": "2023-10-06T08:10:35.107445Z",
     "iopub.status.idle": "2023-10-06T08:10:35.560210Z",
     "shell.execute_reply": "2023-10-06T08:10:35.560210Z",
     "shell.execute_reply.started": "2023-10-06T08:10:35.107445Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1252406cfde84cc69e0235185fdd76e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "propery = 'Tensile_value'\n",
    "\n",
    "# train_data, test_data = load_exp_data(pred_prop=propery, fes=['text'],\n",
    "#                                       split_ratio=0.7, seed=789)\n",
    "# df = pd.concat([train_data, test_data])\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = load_old_data(pred_prop=propery, split_ratio=0.75, seed=666)\n",
    "\n",
    "df.rename(columns={propery:'label', 'Text':'text'}, inplace=True)\n",
    "\n",
    "dataset_df = Dataset.from_pandas(df).shuffle(seed=456)\n",
    "\n",
    "tokenized_datasets = dataset_df.map(preprocess_function, batched=True)\n",
    "dataset = tokenized_datasets.train_test_split(test_size=0.3, seed=666)\n",
    "train_dataset, val_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "740517fa-3d23-42ea-84e8-5ed7ad4975b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T08:10:35.562205Z",
     "iopub.status.busy": "2023-10-06T08:10:35.561206Z",
     "iopub.status.idle": "2023-10-06T08:10:37.059201Z",
     "shell.execute_reply": "2023-10-06T08:10:37.059201Z",
     "shell.execute_reply.started": "2023-10-06T08:10:35.562205Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./../model_saved/checkpoint-140000 were not used when initializing DebertaV2ForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ./../model_saved/checkpoint-140000 and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "SEED = 666\n",
    "\n",
    "hf_model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                            num_labels=1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdf9e2c4-4c83-4c0f-a6ff-ac6800315649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T08:10:37.060199Z",
     "iopub.status.busy": "2023-10-06T08:10:37.060199Z",
     "iopub.status.idle": "2023-10-06T08:27:20.865701Z",
     "shell.execute_reply": "2023-10-06T08:27:20.864732Z",
     "shell.execute_reply.started": "2023-10-06T08:10:37.060199Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 609\n",
      "  Num Epochs = 67\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n",
      "  Number of trainable parameters = 184422913\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='273' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [273/600 16:34 < 20:00, 0.27 it/s, Epoch 30.21/67]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>R2</th>\n",
       "      <th>Smape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>948795.625000</td>\n",
       "      <td>1123220.750000</td>\n",
       "      <td>1123220.500000</td>\n",
       "      <td>1059.820923</td>\n",
       "      <td>982.527649</td>\n",
       "      <td>-6.115284</td>\n",
       "      <td>51336.610687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1062636.125000</td>\n",
       "      <td>1060211.000000</td>\n",
       "      <td>1060211.000000</td>\n",
       "      <td>1029.665527</td>\n",
       "      <td>949.921509</td>\n",
       "      <td>-5.716137</td>\n",
       "      <td>47566.774809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1016716.937500</td>\n",
       "      <td>880272.125000</td>\n",
       "      <td>880272.125000</td>\n",
       "      <td>938.228210</td>\n",
       "      <td>849.948242</td>\n",
       "      <td>-4.576275</td>\n",
       "      <td>37719.282443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>692432.312500</td>\n",
       "      <td>518970.875000</td>\n",
       "      <td>518970.875000</td>\n",
       "      <td>720.396362</td>\n",
       "      <td>601.364746</td>\n",
       "      <td>-2.287534</td>\n",
       "      <td>20478.206107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>318337.812500</td>\n",
       "      <td>163833.312500</td>\n",
       "      <td>163833.296875</td>\n",
       "      <td>404.763275</td>\n",
       "      <td>304.144318</td>\n",
       "      <td>-0.037838</td>\n",
       "      <td>8105.380725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>154751.375000</td>\n",
       "      <td>1143660.250000</td>\n",
       "      <td>1143660.125000</td>\n",
       "      <td>1069.420410</td>\n",
       "      <td>992.874512</td>\n",
       "      <td>-6.244763</td>\n",
       "      <td>52400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1308715.500000</td>\n",
       "      <td>1123005.125000</td>\n",
       "      <td>1123005.125000</td>\n",
       "      <td>1059.719360</td>\n",
       "      <td>982.417969</td>\n",
       "      <td>-6.113920</td>\n",
       "      <td>51323.358779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>943839.750000</td>\n",
       "      <td>949395.437500</td>\n",
       "      <td>949395.437500</td>\n",
       "      <td>974.369263</td>\n",
       "      <td>889.682739</td>\n",
       "      <td>-5.014152</td>\n",
       "      <td>41361.309160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>694494.312500</td>\n",
       "      <td>504650.312500</td>\n",
       "      <td>504650.312500</td>\n",
       "      <td>710.387451</td>\n",
       "      <td>589.512512</td>\n",
       "      <td>-2.196817</td>\n",
       "      <td>19833.538168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>484001.937500</td>\n",
       "      <td>165328.859375</td>\n",
       "      <td>165328.843750</td>\n",
       "      <td>406.606506</td>\n",
       "      <td>331.439178</td>\n",
       "      <td>-0.047311</td>\n",
       "      <td>8743.654580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>212277.281200</td>\n",
       "      <td>409742.906250</td>\n",
       "      <td>409742.906250</td>\n",
       "      <td>640.111633</td>\n",
       "      <td>571.841614</td>\n",
       "      <td>-1.595605</td>\n",
       "      <td>13176.837786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>372288.625000</td>\n",
       "      <td>167270.375000</td>\n",
       "      <td>167270.375000</td>\n",
       "      <td>408.987000</td>\n",
       "      <td>306.421417</td>\n",
       "      <td>-0.059611</td>\n",
       "      <td>8170.482824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>156032.546900</td>\n",
       "      <td>165978.421875</td>\n",
       "      <td>165978.437500</td>\n",
       "      <td>407.404510</td>\n",
       "      <td>332.956116</td>\n",
       "      <td>-0.051426</td>\n",
       "      <td>8778.303435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>171969.687500</td>\n",
       "      <td>158997.609375</td>\n",
       "      <td>158997.625000</td>\n",
       "      <td>398.745056</td>\n",
       "      <td>303.304688</td>\n",
       "      <td>-0.007205</td>\n",
       "      <td>8077.385496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>140722.781200</td>\n",
       "      <td>157860.890625</td>\n",
       "      <td>157860.906250</td>\n",
       "      <td>397.317139</td>\n",
       "      <td>305.962891</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>8140.538168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>155735.703100</td>\n",
       "      <td>162211.718750</td>\n",
       "      <td>162211.750000</td>\n",
       "      <td>402.755188</td>\n",
       "      <td>303.419983</td>\n",
       "      <td>-0.027566</td>\n",
       "      <td>8084.469466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>164538.062500</td>\n",
       "      <td>167025.671875</td>\n",
       "      <td>167025.656250</td>\n",
       "      <td>408.687714</td>\n",
       "      <td>335.278992</td>\n",
       "      <td>-0.058060</td>\n",
       "      <td>8831.042939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>123287.906200</td>\n",
       "      <td>167233.843750</td>\n",
       "      <td>167233.828125</td>\n",
       "      <td>408.942322</td>\n",
       "      <td>306.395508</td>\n",
       "      <td>-0.059379</td>\n",
       "      <td>8169.742366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>159275.500000</td>\n",
       "      <td>164639.078125</td>\n",
       "      <td>164639.078125</td>\n",
       "      <td>405.757416</td>\n",
       "      <td>329.799011</td>\n",
       "      <td>-0.042942</td>\n",
       "      <td>8706.065840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>145846.531200</td>\n",
       "      <td>158715.328125</td>\n",
       "      <td>158715.312500</td>\n",
       "      <td>398.390900</td>\n",
       "      <td>312.462738</td>\n",
       "      <td>-0.005417</td>\n",
       "      <td>8299.046756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>151614.703100</td>\n",
       "      <td>161265.406250</td>\n",
       "      <td>161265.406250</td>\n",
       "      <td>401.578644</td>\n",
       "      <td>303.218384</td>\n",
       "      <td>-0.021571</td>\n",
       "      <td>8078.290076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>136475.250000</td>\n",
       "      <td>158087.140625</td>\n",
       "      <td>158087.140625</td>\n",
       "      <td>397.601746</td>\n",
       "      <td>304.264069</td>\n",
       "      <td>-0.001437</td>\n",
       "      <td>8099.276718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>171354.500000</td>\n",
       "      <td>159487.000000</td>\n",
       "      <td>159487.015625</td>\n",
       "      <td>399.358246</td>\n",
       "      <td>303.185333</td>\n",
       "      <td>-0.010305</td>\n",
       "      <td>8075.220420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>134117.781200</td>\n",
       "      <td>178806.890625</td>\n",
       "      <td>178806.906250</td>\n",
       "      <td>422.855652</td>\n",
       "      <td>357.226074</td>\n",
       "      <td>-0.132691</td>\n",
       "      <td>9314.897901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>145984.953100</td>\n",
       "      <td>172885.343750</td>\n",
       "      <td>172885.343750</td>\n",
       "      <td>415.794830</td>\n",
       "      <td>310.235443</td>\n",
       "      <td>-0.095180</td>\n",
       "      <td>8279.856870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>163427.406200</td>\n",
       "      <td>175711.093750</td>\n",
       "      <td>175711.093750</td>\n",
       "      <td>419.179077</td>\n",
       "      <td>312.410126</td>\n",
       "      <td>-0.113080</td>\n",
       "      <td>8343.059160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>129911.242200</td>\n",
       "      <td>171511.687500</td>\n",
       "      <td>171511.687500</td>\n",
       "      <td>414.139709</td>\n",
       "      <td>344.336823</td>\n",
       "      <td>-0.086478</td>\n",
       "      <td>9033.723282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>193017.250000</td>\n",
       "      <td>158309.250000</td>\n",
       "      <td>158309.250000</td>\n",
       "      <td>397.880951</td>\n",
       "      <td>303.802307</td>\n",
       "      <td>-0.002844</td>\n",
       "      <td>8088.319656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>128051.437500</td>\n",
       "      <td>165504.750000</td>\n",
       "      <td>165504.734375</td>\n",
       "      <td>406.822723</td>\n",
       "      <td>305.185547</td>\n",
       "      <td>-0.048426</td>\n",
       "      <td>8135.145992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>162094.250000</td>\n",
       "      <td>198546.093750</td>\n",
       "      <td>198546.093750</td>\n",
       "      <td>445.585114</td>\n",
       "      <td>385.395386</td>\n",
       "      <td>-0.257733</td>\n",
       "      <td>9901.838740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>169332.281200</td>\n",
       "      <td>171956.937500</td>\n",
       "      <td>171956.937500</td>\n",
       "      <td>414.676910</td>\n",
       "      <td>345.173615</td>\n",
       "      <td>-0.089299</td>\n",
       "      <td>9052.220420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>159723.953100</td>\n",
       "      <td>178827.578125</td>\n",
       "      <td>178827.578125</td>\n",
       "      <td>422.880096</td>\n",
       "      <td>314.937744</td>\n",
       "      <td>-0.132822</td>\n",
       "      <td>8417.230916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>139000.031200</td>\n",
       "      <td>159670.718750</td>\n",
       "      <td>159670.718750</td>\n",
       "      <td>399.588196</td>\n",
       "      <td>303.168427</td>\n",
       "      <td>-0.011469</td>\n",
       "      <td>8075.087786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>174948.828100</td>\n",
       "      <td>170778.921875</td>\n",
       "      <td>170778.937500</td>\n",
       "      <td>413.254089</td>\n",
       "      <td>342.929260</td>\n",
       "      <td>-0.081836</td>\n",
       "      <td>9002.512405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>139562.734400</td>\n",
       "      <td>159992.250000</td>\n",
       "      <td>159992.250000</td>\n",
       "      <td>399.990326</td>\n",
       "      <td>303.140778</td>\n",
       "      <td>-0.013506</td>\n",
       "      <td>8074.825382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>132461.156200</td>\n",
       "      <td>157979.234375</td>\n",
       "      <td>157979.265625</td>\n",
       "      <td>397.466064</td>\n",
       "      <td>308.148621</td>\n",
       "      <td>-0.000754</td>\n",
       "      <td>8194.201336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>145010.078100</td>\n",
       "      <td>158880.437500</td>\n",
       "      <td>158880.421875</td>\n",
       "      <td>398.598083</td>\n",
       "      <td>303.345551</td>\n",
       "      <td>-0.006463</td>\n",
       "      <td>8078.173664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>133923.828100</td>\n",
       "      <td>158712.203125</td>\n",
       "      <td>158712.203125</td>\n",
       "      <td>398.386993</td>\n",
       "      <td>312.448029</td>\n",
       "      <td>-0.005397</td>\n",
       "      <td>8298.692748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>165924.156200</td>\n",
       "      <td>157931.906250</td>\n",
       "      <td>157931.890625</td>\n",
       "      <td>397.406464</td>\n",
       "      <td>307.628448</td>\n",
       "      <td>-0.000454</td>\n",
       "      <td>8181.416985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>134839.390600</td>\n",
       "      <td>157926.656250</td>\n",
       "      <td>157926.671875</td>\n",
       "      <td>397.399902</td>\n",
       "      <td>304.939514</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>8115.506679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>176951.312500</td>\n",
       "      <td>157918.171875</td>\n",
       "      <td>157918.171875</td>\n",
       "      <td>397.389191</td>\n",
       "      <td>305.005463</td>\n",
       "      <td>-0.000367</td>\n",
       "      <td>8117.112595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>121753.843800</td>\n",
       "      <td>157935.281250</td>\n",
       "      <td>157935.265625</td>\n",
       "      <td>397.410706</td>\n",
       "      <td>307.669006</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>8182.415076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>145060.281200</td>\n",
       "      <td>159540.437500</td>\n",
       "      <td>159540.453125</td>\n",
       "      <td>399.425140</td>\n",
       "      <td>303.180328</td>\n",
       "      <td>-0.010644</td>\n",
       "      <td>8075.183206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>131529.031200</td>\n",
       "      <td>158028.890625</td>\n",
       "      <td>158028.890625</td>\n",
       "      <td>397.528473</td>\n",
       "      <td>308.594177</td>\n",
       "      <td>-0.001068</td>\n",
       "      <td>8205.114504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>144083.125000</td>\n",
       "      <td>161263.000000</td>\n",
       "      <td>161262.984375</td>\n",
       "      <td>401.575623</td>\n",
       "      <td>320.913300</td>\n",
       "      <td>-0.021555</td>\n",
       "      <td>8499.679389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>169756.343800</td>\n",
       "      <td>159310.390625</td>\n",
       "      <td>159310.375000</td>\n",
       "      <td>399.137024</td>\n",
       "      <td>314.891754</td>\n",
       "      <td>-0.009186</td>\n",
       "      <td>8357.422710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>129600.265600</td>\n",
       "      <td>158104.843750</td>\n",
       "      <td>158104.828125</td>\n",
       "      <td>397.623993</td>\n",
       "      <td>309.181580</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>8219.473282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>150437.218800</td>\n",
       "      <td>158187.437500</td>\n",
       "      <td>158187.421875</td>\n",
       "      <td>397.727814</td>\n",
       "      <td>304.031860</td>\n",
       "      <td>-0.002073</td>\n",
       "      <td>8093.778626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>116135.226600</td>\n",
       "      <td>159503.875000</td>\n",
       "      <td>159503.875000</td>\n",
       "      <td>399.379364</td>\n",
       "      <td>315.568176</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>8373.537214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>158389.875000</td>\n",
       "      <td>157867.046875</td>\n",
       "      <td>157867.046875</td>\n",
       "      <td>397.324860</td>\n",
       "      <td>305.686310</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>8133.759542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>153406.765600</td>\n",
       "      <td>158537.984375</td>\n",
       "      <td>158537.984375</td>\n",
       "      <td>398.168274</td>\n",
       "      <td>311.641571</td>\n",
       "      <td>-0.004293</td>\n",
       "      <td>8279.218511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>146915.671900</td>\n",
       "      <td>158531.109375</td>\n",
       "      <td>158531.109375</td>\n",
       "      <td>398.159668</td>\n",
       "      <td>303.516937</td>\n",
       "      <td>-0.004250</td>\n",
       "      <td>8081.632634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>133550.843800</td>\n",
       "      <td>164432.312500</td>\n",
       "      <td>164432.312500</td>\n",
       "      <td>405.502533</td>\n",
       "      <td>329.296722</td>\n",
       "      <td>-0.041632</td>\n",
       "      <td>8694.522901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>137799.546900</td>\n",
       "      <td>158365.703125</td>\n",
       "      <td>158365.703125</td>\n",
       "      <td>397.951874</td>\n",
       "      <td>303.713531</td>\n",
       "      <td>-0.003202</td>\n",
       "      <td>8086.213740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>156399.437500</td>\n",
       "      <td>158395.875000</td>\n",
       "      <td>158395.875000</td>\n",
       "      <td>397.989807</td>\n",
       "      <td>310.935669</td>\n",
       "      <td>-0.003393</td>\n",
       "      <td>8262.142176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>142053.265600</td>\n",
       "      <td>157868.093750</td>\n",
       "      <td>157868.093750</td>\n",
       "      <td>397.326172</td>\n",
       "      <td>306.558807</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>8155.156489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>135976.437500</td>\n",
       "      <td>157877.468750</td>\n",
       "      <td>157877.453125</td>\n",
       "      <td>397.337952</td>\n",
       "      <td>306.788239</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>8160.769084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>129926.835900</td>\n",
       "      <td>163266.437500</td>\n",
       "      <td>163266.437500</td>\n",
       "      <td>404.062408</td>\n",
       "      <td>326.390717</td>\n",
       "      <td>-0.034247</td>\n",
       "      <td>8627.468511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>173348.218800</td>\n",
       "      <td>165833.968750</td>\n",
       "      <td>165833.968750</td>\n",
       "      <td>407.227173</td>\n",
       "      <td>305.399261</td>\n",
       "      <td>-0.050511</td>\n",
       "      <td>8141.248092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>167042.578100</td>\n",
       "      <td>157861.921875</td>\n",
       "      <td>157861.937500</td>\n",
       "      <td>397.318420</td>\n",
       "      <td>305.886658</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>8138.672710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>146793.390600</td>\n",
       "      <td>158327.187500</td>\n",
       "      <td>158327.187500</td>\n",
       "      <td>397.903503</td>\n",
       "      <td>310.561890</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>8253.069656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>135960.812500</td>\n",
       "      <td>158502.015625</td>\n",
       "      <td>158502.015625</td>\n",
       "      <td>398.123108</td>\n",
       "      <td>311.470520</td>\n",
       "      <td>-0.004065</td>\n",
       "      <td>8275.088740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>127926.937500</td>\n",
       "      <td>159305.765625</td>\n",
       "      <td>159305.765625</td>\n",
       "      <td>399.131256</td>\n",
       "      <td>303.219879</td>\n",
       "      <td>-0.009157</td>\n",
       "      <td>8075.803435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>169510.093800</td>\n",
       "      <td>162473.390625</td>\n",
       "      <td>162473.406250</td>\n",
       "      <td>403.079895</td>\n",
       "      <td>324.280121</td>\n",
       "      <td>-0.029223</td>\n",
       "      <td>8578.408397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>135002.343800</td>\n",
       "      <td>160483.140625</td>\n",
       "      <td>160483.140625</td>\n",
       "      <td>400.603485</td>\n",
       "      <td>318.625122</td>\n",
       "      <td>-0.016615</td>\n",
       "      <td>8445.843511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>149786.453100</td>\n",
       "      <td>161947.015625</td>\n",
       "      <td>161947.015625</td>\n",
       "      <td>402.426422</td>\n",
       "      <td>303.344513</td>\n",
       "      <td>-0.025889</td>\n",
       "      <td>8082.229008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>136718.500000</td>\n",
       "      <td>160491.125000</td>\n",
       "      <td>160491.109375</td>\n",
       "      <td>400.613403</td>\n",
       "      <td>318.649445</td>\n",
       "      <td>-0.016666</td>\n",
       "      <td>8446.416985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>174297.062500</td>\n",
       "      <td>158542.593750</td>\n",
       "      <td>158542.578125</td>\n",
       "      <td>398.174042</td>\n",
       "      <td>311.663116</td>\n",
       "      <td>-0.004322</td>\n",
       "      <td>8279.739504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A8BA9BE50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CE7BB0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-8\n",
      "Configuration saved in ./results\\checkpoint-8\\config.json\n",
      "Model weights saved in ./results\\checkpoint-8\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-8\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-8\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CF2520>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CE7BB0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-16\n",
      "Configuration saved in ./results\\checkpoint-16\\config.json\n",
      "Model weights saved in ./results\\checkpoint-16\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-16\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-16\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A97711EE0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CEE430>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-24\n",
      "Configuration saved in ./results\\checkpoint-24\\config.json\n",
      "Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-24\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-24\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-8] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CFD340>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CF8AC0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-32\n",
      "Configuration saved in ./results\\checkpoint-32\\config.json\n",
      "Model weights saved in ./results\\checkpoint-32\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-32\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-32\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-24] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A8BABD700>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CFB040>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-40\n",
      "Configuration saved in ./results\\checkpoint-40\\config.json\n",
      "Model weights saved in ./results\\checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-16] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A8BAAEEE0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CFD340>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-48\n",
      "Configuration saved in ./results\\checkpoint-48\\config.json\n",
      "Model weights saved in ./results\\checkpoint-48\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-48\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-48\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-32] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CE6760>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CF9760>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-56\n",
      "Configuration saved in ./results\\checkpoint-56\\config.json\n",
      "Model weights saved in ./results\\checkpoint-56\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-56\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-56\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-40] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CF2F40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A97711670>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-64\n",
      "Configuration saved in ./results\\checkpoint-64\\config.json\n",
      "Model weights saved in ./results\\checkpoint-64\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-64\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-64\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-48] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF9FB56A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CF2F40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-72\n",
      "Configuration saved in ./results\\checkpoint-72\\config.json\n",
      "Model weights saved in ./results\\checkpoint-72\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-72\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-72\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-64] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A8BADB820>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CF8D00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-80\n",
      "Configuration saved in ./results\\checkpoint-80\\config.json\n",
      "Model weights saved in ./results\\checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-56] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A97710490>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF7EE8DC0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-88\n",
      "Configuration saved in ./results\\checkpoint-88\\config.json\n",
      "Model weights saved in ./results\\checkpoint-88\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-88\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-88\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-72] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CEC1C0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A8BABD820>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-96\n",
      "Configuration saved in ./results\\checkpoint-96\\config.json\n",
      "Model weights saved in ./results\\checkpoint-96\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-96\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-96\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-80] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A97749070>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A8BABD820>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-104\n",
      "Configuration saved in ./results\\checkpoint-104\\config.json\n",
      "Model weights saved in ./results\\checkpoint-104\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-104\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-104\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-96] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF86E7FD0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF7EB60D0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-112\n",
      "Configuration saved in ./results\\checkpoint-112\\config.json\n",
      "Model weights saved in ./results\\checkpoint-112\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-112\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-112\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-104] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF86E7FD0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CB09D0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-120\n",
      "Configuration saved in ./results\\checkpoint-120\\config.json\n",
      "Model weights saved in ./results\\checkpoint-120\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-120\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-120\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-112] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CF5A00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CFDFD0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-128\n",
      "Configuration saved in ./results\\checkpoint-128\\config.json\n",
      "Model weights saved in ./results\\checkpoint-128\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-128\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-128\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-120] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CED550>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF86E34F0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-136\n",
      "Configuration saved in ./results\\checkpoint-136\\config.json\n",
      "Model weights saved in ./results\\checkpoint-136\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-136\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-136\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-128] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF7EEFCD0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A977100A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-144\n",
      "Configuration saved in ./results\\checkpoint-144\\config.json\n",
      "Model weights saved in ./results\\checkpoint-144\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-144\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-144\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-88] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF7EE8DC0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CFCA90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-152\n",
      "Configuration saved in ./results\\checkpoint-152\\config.json\n",
      "Model weights saved in ./results\\checkpoint-152\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-152\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-152\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-136] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CB09D0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF09469A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-160\n",
      "Configuration saved in ./results\\checkpoint-160\\config.json\n",
      "Model weights saved in ./results\\checkpoint-160\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-160\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-160\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-144] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CECE50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CB0700>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-168\n",
      "Configuration saved in ./results\\checkpoint-168\\config.json\n",
      "Model weights saved in ./results\\checkpoint-168\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-168\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-168\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-152] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A9773C7C0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AEFBF02E0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-176\n",
      "Configuration saved in ./results\\checkpoint-176\\config.json\n",
      "Model weights saved in ./results\\checkpoint-176\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-176\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-176\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-168] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF9FB56A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF09368E0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-184\n",
      "Configuration saved in ./results\\checkpoint-184\\config.json\n",
      "Model weights saved in ./results\\checkpoint-184\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-184\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-184\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-176] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CF5A00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A8BABDCA0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-192\n",
      "Configuration saved in ./results\\checkpoint-192\\config.json\n",
      "Model weights saved in ./results\\checkpoint-192\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-192\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-192\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-184] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A97735CA0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AEFBF02E0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-200\n",
      "Configuration saved in ./results\\checkpoint-200\\config.json\n",
      "Model weights saved in ./results\\checkpoint-200\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-200\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-200\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-160] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A97742220>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF86DC850>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-208\n",
      "Configuration saved in ./results\\checkpoint-208\\config.json\n",
      "Model weights saved in ./results\\checkpoint-208\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-208\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-208\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-192] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A9773CF40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AEFBF02E0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-216\n",
      "Configuration saved in ./results\\checkpoint-216\\config.json\n",
      "Model weights saved in ./results\\checkpoint-216\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-216\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-216\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-208] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF09469A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF0946A90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-224\n",
      "Configuration saved in ./results\\checkpoint-224\\config.json\n",
      "Model weights saved in ./results\\checkpoint-224\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-224\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-224\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-216] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A8BA9B940>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF87048B0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-232\n",
      "Configuration saved in ./results\\checkpoint-232\\config.json\n",
      "Model weights saved in ./results\\checkpoint-232\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-232\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-232\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-224] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CE7B80>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CEDBE0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-240\n",
      "Configuration saved in ./results\\checkpoint-240\\config.json\n",
      "Model weights saved in ./results\\checkpoint-240\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-240\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-240\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF9FB56A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CEEC40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-248\n",
      "Configuration saved in ./results\\checkpoint-248\\config.json\n",
      "Model weights saved in ./results\\checkpoint-248\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-248\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-248\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-232] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A8BABDCA0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A87CEEC40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-256\n",
      "Configuration saved in ./results\\checkpoint-256\\config.json\n",
      "Model weights saved in ./results\\checkpoint-256\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-256\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-256\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-248] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A9773C7C0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF093A580>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-264\n",
      "Configuration saved in ./results\\checkpoint-264\\config.json\n",
      "Model weights saved in ./results\\checkpoint-264\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-264\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-264\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-256] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024AF87048B0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 262\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x0000024A8BABDCA0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-272\n",
      "Configuration saved in ./results\\checkpoint-272\\config.json\n",
      "Model weights saved in ./results\\checkpoint-272\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-272\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-272\\special_tokens_map.json\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy='steps',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_dir = './logs',\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    learning_rate=2e-2,\n",
    "    weight_decay=1e-2,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=2e-5,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=1,\n",
    "    lr_scheduler_type='linear',\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    eval_steps=4,\n",
    "    save_strategy='steps',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=4,\n",
    "    save_steps=8,\n",
    "    save_total_limit=1,\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    fp16=False,\n",
    "    optim='adamw_torch',\n",
    "    # max_steps=int(len(train_dataset)*50/64),\n",
    "    max_steps=600,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    'params': [p for n, p in model.named_parameters() if not 'deberta' in n],\n",
    "    'betas': (0.9, 0.99),\n",
    "    'eps': 2e-5,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-4\n",
    "}\n",
    "optimizer = AdamW(**optimizer_kwargs)\n",
    "\n",
    "# Call the Trainer\n",
    "trainer = Trainer(\n",
    "    model = hf_model,                         \n",
    "    args = training_args,                  \n",
    "    train_dataset = train_dataset,         \n",
    "    eval_dataset = val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = compute_metrics_for_regression,\n",
    "    # optimizers=(optimizer, None)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Call the summary\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e9593-649f-4e6b-a4e9-64f1dba3e52a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8c1c8-d3b6-4435-9823-e7b574f5b3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395127bf-b69e-4268-89d1-43f0d2486fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ff559-969f-4920-901c-281f465da262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517006b2-f30b-414f-b081-6c4f5c1e2a4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:41:56.841400Z",
     "iopub.status.busy": "2023-10-06T03:41:56.840403Z",
     "iopub.status.idle": "2023-10-06T03:41:56.862344Z",
     "shell.execute_reply": "2023-10-06T03:41:56.862344Z",
     "shell.execute_reply.started": "2023-10-06T03:41:56.841400Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train, y_val = train_data['Elongation_value'], test_data['Elongation_value']\n",
    "train_encodings = tokenizer(train_data['Text'].tolist(),\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=512,\n",
    "                            return_token_type_ids=True,\n",
    "                            truncation=True,\n",
    "                            padding='max_length',\n",
    "                            return_attention_mask=True,\n",
    "                            return_tensors='pt')\n",
    "valid_encodings = tokenizer(test_data['Text'].tolist(),\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=512,\n",
    "                            return_token_type_ids=True,\n",
    "                            truncation=True,\n",
    "                            padding='max_length',\n",
    "                            return_attention_mask=True,\n",
    "                            return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "252baea1-83f1-4cab-83a2-f890ad422b35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:41:57.317128Z",
     "iopub.status.busy": "2023-10-06T03:41:57.316131Z",
     "iopub.status.idle": "2023-10-06T03:41:57.333085Z",
     "shell.execute_reply": "2023-10-06T03:41:57.333085Z",
     "shell.execute_reply.started": "2023-10-06T03:41:57.317128Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MakeTorchData(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        item[\"labels\"] = float(item[\"labels\"])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "# convert our tokenized data into a torch Dataset\n",
    "train_dataset = MakeTorchData(train_encodings, y_train.ravel())\n",
    "valid_dataset = MakeTorchData(valid_encodings, y_val.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3371d3-f868-410e-bcc2-25a814a78b2a",
   "metadata": {},
   "source": [
    "#### hf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88233e-0020-4aef-9f39-5061a88fdd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1d260d5-2e99-474c-aefa-1a10475940ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:19:53.552296Z",
     "iopub.status.busy": "2023-10-06T03:19:53.552296Z",
     "iopub.status.idle": "2023-10-06T03:19:53.569250Z",
     "shell.execute_reply": "2023-10-06T03:19:53.569250Z",
     "shell.execute_reply.started": "2023-10-06T03:19:53.552296Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomHFModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        self.transformer= AutoModel.from_pretrained(self.name)\n",
    "        self.concat_linear = nn.Sequential(\n",
    "            nn.Linear(768, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        \n",
    "        # using [Seq]\n",
    "        output = self.transformer(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, return_dict=True).last_hidden_state[:, 0]\n",
    "        output = self.concat_linear(output)   \n",
    "        \n",
    "#         loss_fct = nn.MSELoss()\n",
    "#         loss = loss_fct(output.view(-1, 1), labels.view(-1))\n",
    "        # output = self.concat_linear(output)\n",
    "        return output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33d68676-6ae9-4609-ae6d-42f75991bb62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:17:06.773742Z",
     "iopub.status.busy": "2023-10-06T03:17:06.772744Z",
     "iopub.status.idle": "2023-10-06T03:17:06.792690Z",
     "shell.execute_reply": "2023-10-06T03:17:06.792690Z",
     "shell.execute_reply.started": "2023-10-06T03:17:06.773742Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\AppData\\Local\\Temp\\ipykernel_24536\\2473836382.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\shaohan.tian\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_24536\\\\2473836382.py'</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'float'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'shape'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\AppData\\Local\\Temp\\ipykernel_24536\\2473836382.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\shaohan.tian\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_24536\\\\2473836382.py'\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'float'\u001b[0m object has no attribute \u001b[32m'shape'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k,v in train_dataset[0].items():\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07701394-4291-43f7-b120-03de19278efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:29:00.485825Z",
     "iopub.status.busy": "2023-10-06T03:29:00.484827Z",
     "iopub.status.idle": "2023-10-06T03:29:02.074575Z",
     "shell.execute_reply": "2023-10-06T03:29:02.074575Z",
     "shell.execute_reply.started": "2023-10-06T03:29:00.485825Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./../model_saved/checkpoint-140000 were not used when initializing DebertaV2Model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'hello'\n",
    "temp_model = AutoModel.from_pretrained(model_name)\n",
    "temp_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "encoded_input = tokenizer(text, padding=True, truncation=False, return_tensors=\"pt\")\n",
    "# model_output = model(**encoded_input)\n",
    "\n",
    "test_model(**encoded_input, labels=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a3686-bd21-4afb-92e1-fe25a2b643c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fa6d396-3744-43c8-92da-20c24bbb9768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:30:03.607546Z",
     "iopub.status.busy": "2023-10-06T03:30:03.607546Z",
     "iopub.status.idle": "2023-10-06T03:30:03.622506Z",
     "shell.execute_reply": "2023-10-06T03:30:03.622506Z",
     "shell.execute_reply.started": "2023-10-06T03:30:03.607546Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b8bee74-44a6-4531-b513-9828ba8c30fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:30:39.823773Z",
     "iopub.status.busy": "2023-10-06T03:30:39.823773Z",
     "iopub.status.idle": "2023-10-06T03:30:39.838733Z",
     "shell.execute_reply": "2023-10-06T03:30:39.838733Z",
     "shell.execute_reply.started": "2023-10-06T03:30:39.823773Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f49b215-7f53-4af8-a42a-9dfd717d6e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:31:05.790938Z",
     "iopub.status.busy": "2023-10-06T03:31:05.789940Z",
     "iopub.status.idle": "2023-10-06T03:31:05.808918Z",
     "shell.execute_reply": "2023-10-06T03:31:05.808918Z",
     "shell.execute_reply.started": "2023-10-06T03:31:05.790938Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = train_dataset[0]['input_ids'].unsqueeze(0)\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c768bd1-c3d7-4745-b67e-057ab49ae2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3007aca2-a5eb-4f5c-aa4b-60a62a9b1d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97b48a14-de0e-47b8-b82e-b946f9bfc572",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:31:20.147399Z",
     "iopub.status.busy": "2023-10-06T03:31:20.146402Z",
     "iopub.status.idle": "2023-10-06T03:31:22.437275Z",
     "shell.execute_reply": "2023-10-06T03:31:22.437275Z",
     "shell.execute_reply.started": "2023-10-06T03:31:20.147399Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./../model_saved/checkpoint-140000 were not used when initializing DebertaV2Model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = train_dataset[0]['input_ids'].unsqueeze(0)\n",
    "attention_mask = train_dataset[0]['attention_mask'].unsqueeze(0)\n",
    "token_type_ids = train_dataset[0]['token_type_ids'].unsqueeze(0)\n",
    "labels = train_dataset[0]['labels']\n",
    "\n",
    "test_model = CustomHFModel(model_name)\n",
    "test_model.eval()\n",
    "test_model(input_ids, attention_mask, token_type_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa9b11-fd7f-409f-bb06-dcc69dc942e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d1e09-c16e-4e91-b5ff-605eefb0f0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2f25d-8c89-46d7-b805-5b286095a127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd6b1700-3c92-45ba-9a3d-871d1ec72db1",
   "metadata": {},
   "source": [
    "#### hf Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c33939ce-ba4a-4eb5-b5fe-8edc888e55b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:35:06.001409Z",
     "iopub.status.busy": "2023-10-06T03:35:06.001409Z",
     "iopub.status.idle": "2023-10-06T03:35:22.590256Z",
     "shell.execute_reply": "2023-10-06T03:35:22.589258Z",
     "shell.execute_reply.started": "2023-10-06T03:35:06.001409Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./../model_saved/checkpoint-140000 were not used when initializing DebertaV2ForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ./../model_saved/checkpoint-140000 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 45\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2\n",
      "  Number of trainable parameters = 184423682\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshaohantian\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928f8f71277e41a98941eb604dc08366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\shaohan.tian\\OneDrive\\code\\code_github\\SteelBERTa\\regression\\wandb\\run-20231006_113511-v1t02drv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shaohantian/huggingface/runs/v1t02drv' target=\"_blank\">charmed-universe-165</a></strong> to <a href='https://wandb.ai/shaohantian/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shaohantian/huggingface' target=\"_blank\">https://wandb.ai/shaohantian/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shaohantian/huggingface/runs/v1t02drv' target=\"_blank\">https://wandb.ai/shaohantian/huggingface/runs/v1t02drv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\AppData\\Local\\Temp\\ipykernel_36876\\3080396221.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">33</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\shaohan.tian\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_36876\\\\3080396221.py'</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\transformers\\t</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">rainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1543</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1540 │   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1541 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1542 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1543 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1544 │   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1545 │   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1546 │   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\transformers\\t</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">rainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1791</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1788 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> model.no_sync():                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1789 │   │   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1790 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1791 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1792 │   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1793 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1794 │   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\transformers\\t</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">rainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2557</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2554 │   │   │   # loss gets scaled under gradient_accumulation_steps in deepspeed</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2555 │   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed.backward(loss)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2556 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2557 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2558 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2559 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss.detach()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2560 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\torch\\_tensor.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">396</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 393 │   │   │   │   </span>retain_graph=retain_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 394 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 395 │   │   │   │   </span>inputs=inputs)                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 396 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=input  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 397 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 398 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">register_hook</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, hook):                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 399 │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">r\"\"\"Registers a backward hook.</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\torch\\autograd</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">\\__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">173</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">171 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>173 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">175 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>CUDA error: device-side assert triggered\n",
       "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be \n",
       "incorrect.\n",
       "For debugging consider passing <span style=\"color: #808000; text-decoration-color: #808000\">CUDA_LAUNCH_BLOCKING</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\AppData\\Local\\Temp\\ipykernel_36876\\3080396221.py\u001b[0m:\u001b[94m33\u001b[0m in \u001b[92m<module>\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\shaohan.tian\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_36876\\\\3080396221.py'\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\transformers\\t\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mrainer.py\u001b[0m:\u001b[94m1543\u001b[0m in \u001b[92mtrain\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1540 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1541 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1542 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1543 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1544 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1545 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1546 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\transformers\\t\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mrainer.py\u001b[0m:\u001b[94m1791\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1788 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1789 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1790 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1791 \u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1792 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1793 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1794 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\transformers\\t\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mrainer.py\u001b[0m:\u001b[94m2557\u001b[0m in \u001b[92mtraining_step\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2554 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# loss gets scaled under gradient_accumulation_steps in deepspeed\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2555 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.deepspeed.backward(loss)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2556 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2557 \u001b[2m│   │   │   \u001b[0mloss.backward()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2558 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2559 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m loss.detach()                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2560 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\torch\\_tensor.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mpy\u001b[0m:\u001b[94m396\u001b[0m in \u001b[92mbackward\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 393 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mretain_graph=retain_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 394 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 395 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs)                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 396 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=input  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 397 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 398 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mregister_hook\u001b[0m(\u001b[96mself\u001b[0m, hook):                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 399 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\"\"Registers a backward hook.\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\torch\\autograd\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33m\\__init__.py\u001b[0m:\u001b[94m173\u001b[0m in \u001b[92mbackward\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m173 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m176 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mCUDA error: device-side assert triggered\n",
       "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be \n",
       "incorrect.\n",
       "For debugging consider passing \u001b[33mCUDA_LAUNCH_BLOCKING\u001b[0m=\u001b[1;36m1\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hf_model = CustomHFModel(model_name).to(device)\n",
    "hf_model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "                                                   \n",
    "                                                   \n",
    "# Specifiy the arguments for the trainer  \n",
    "training_args = TrainingArguments(\n",
    "    output_dir ='./results',          \n",
    "    num_train_epochs = 5,     \n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    weight_decay = 0.01,               \n",
    "    learning_rate = 2e-3,\n",
    "    logging_dir = './logs',            \n",
    "    save_total_limit = 1,\n",
    "    load_best_model_at_end = True,     \n",
    "    metric_for_best_model = 'r2',    \n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_strategy = \"steps\",\n",
    "    max_steps=2\n",
    "    \n",
    ") \n",
    "\n",
    "# Call the Trainer\n",
    "trainer = Trainer(\n",
    "    model = hf_model,                         \n",
    "    args = training_args,                  \n",
    "    train_dataset = train_dataset,         \n",
    "    eval_dataset = valid_dataset,          \n",
    "    compute_metrics = compute_metrics_for_regression,     \n",
    ")\n",
    "\n",
    "# # Train the model\n",
    "trainer.train()\n",
    "\n",
    "# # Call the summary\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78b1b9-7cb9-408b-9aa4-47cdb9ad665e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fe23ef5-530b-4247-9851-cc38e1d593ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:50:44.681026Z",
     "iopub.status.busy": "2023-10-06T03:50:44.680029Z",
     "iopub.status.idle": "2023-10-06T03:50:46.450406Z",
     "shell.execute_reply": "2023-10-06T03:50:46.450406Z",
     "shell.execute_reply.started": "2023-10-06T03:50:44.681026Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb189fea-3f30-4ad6-9cfc-6f530e02e528",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:50:46.732652Z",
     "iopub.status.busy": "2023-10-06T03:50:46.732652Z",
     "iopub.status.idle": "2023-10-06T03:50:48.155847Z",
     "shell.execute_reply": "2023-10-06T03:50:48.155847Z",
     "shell.execute_reply.started": "2023-10-06T03:50:46.732652Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./../model_saved/checkpoint-140000\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"./../model_saved/checkpoint-140000\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file ./../model_saved/checkpoint-140000\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at ./../model_saved/checkpoint-140000 were not used when initializing DebertaV2ForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ./../model_saved/checkpoint-140000 and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "                                                           num_labels=1,\n",
    "                                                           ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8261e91-b0f0-4ccc-8ec8-1020c882a34d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c26043e2-775d-4a0f-955d-61cbe86e8d9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:51:07.883965Z",
     "iopub.status.busy": "2023-10-06T03:51:07.883965Z",
     "iopub.status.idle": "2023-10-06T03:51:07.906903Z",
     "shell.execute_reply": "2023-10-06T03:51:07.906903Z",
     "shell.execute_reply.started": "2023-10-06T03:51:07.883965Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=100,\n",
    "    report_to=\"none\",\n",
    "    weight_decay=0.01,\n",
    "    output_dir='./results',\n",
    "    # metric_for_best_model='accuracy'\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = train_dataset,         \n",
    "    eval_dataset = valid_dataset,          \n",
    "    compute_metrics = compute_metrics_for_regression,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef623922-a9f0-44e7-b92a-331e9b07a4c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-06T03:51:09.337232Z",
     "iopub.status.busy": "2023-10-06T03:51:09.337232Z",
     "iopub.status.idle": "2023-10-06T03:57:14.295121Z",
     "shell.execute_reply": "2023-10-06T03:57:14.294123Z",
     "shell.execute_reply.started": "2023-10-06T03:51:09.337232Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 45\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 300\n",
      "  Number of trainable parameters = 184422913\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 25/300 05:31 < 1:05:59, 0.07 it/s, Epoch 8/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>R2</th>\n",
       "      <th>Smape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>235.571777</td>\n",
       "      <td>235.571747</td>\n",
       "      <td>15.348347</td>\n",
       "      <td>11.923091</td>\n",
       "      <td>-1.519010</td>\n",
       "      <td>1850.379883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>173.332855</td>\n",
       "      <td>173.332825</td>\n",
       "      <td>13.165592</td>\n",
       "      <td>9.472361</td>\n",
       "      <td>-0.853478</td>\n",
       "      <td>1225.323828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>135.523209</td>\n",
       "      <td>135.523209</td>\n",
       "      <td>11.641443</td>\n",
       "      <td>8.628752</td>\n",
       "      <td>-0.449173</td>\n",
       "      <td>1089.562305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>109.018143</td>\n",
       "      <td>109.018150</td>\n",
       "      <td>10.441175</td>\n",
       "      <td>8.374501</td>\n",
       "      <td>-0.165750</td>\n",
       "      <td>1065.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>95.036667</td>\n",
       "      <td>95.036667</td>\n",
       "      <td>9.748675</td>\n",
       "      <td>8.366055</td>\n",
       "      <td>-0.016244</td>\n",
       "      <td>1063.997656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>95.601646</td>\n",
       "      <td>95.601662</td>\n",
       "      <td>9.777610</td>\n",
       "      <td>8.699568</td>\n",
       "      <td>-0.022285</td>\n",
       "      <td>1088.907617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>109.624001</td>\n",
       "      <td>109.624001</td>\n",
       "      <td>10.470148</td>\n",
       "      <td>9.212896</td>\n",
       "      <td>-0.172228</td>\n",
       "      <td>1121.520508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>131.328949</td>\n",
       "      <td>131.328995</td>\n",
       "      <td>11.459887</td>\n",
       "      <td>9.638322</td>\n",
       "      <td>-0.404324</td>\n",
       "      <td>1140.426563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x000001A5EC320EE0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-3\n",
      "Configuration saved in ./results\\checkpoint-3\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x000001A5F56984C0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-6\n",
      "Configuration saved in ./results\\checkpoint-6\\config.json\n",
      "Model weights saved in ./results\\checkpoint-6\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x000001A5F5683D60>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-9\n",
      "Configuration saved in ./results\\checkpoint-9\\config.json\n",
      "Model weights saved in ./results\\checkpoint-9\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x000001A5F5683910>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-12\n",
      "Configuration saved in ./results\\checkpoint-12\\config.json\n",
      "Model weights saved in ./results\\checkpoint-12\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x000001A5BB0A7730>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-15\n",
      "Configuration saved in ./results\\checkpoint-15\\config.json\n",
      "Model weights saved in ./results\\checkpoint-15\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x000001A5EC320EE0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-18\n",
      "Configuration saved in ./results\\checkpoint-18\\config.json\n",
      "Model weights saved in ./results\\checkpoint-18\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x000001A5F56E37F0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-21\n",
      "Configuration saved in ./results\\checkpoint-21\\config.json\n",
      "Model weights saved in ./results\\checkpoint-21\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x000001A5EC320EE0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-24\n",
      "Configuration saved in ./results\\checkpoint-24\\config.json\n",
      "Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\AppData\\Local\\Temp\\ipykernel_10608\\3488631611.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\shaohan.tian\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_10608\\\\3488631611.py'</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\transformers\\t</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">rainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1543</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1540 │   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1541 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1542 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1543 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1544 │   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1545 │   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1546 │   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\transformers\\t</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">rainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1791</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1788 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> model.no_sync():                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1789 │   │   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1790 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1791 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1792 │   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1793 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1794 │   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\transformers\\t</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">rainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2557</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2554 │   │   │   # loss gets scaled under gradient_accumulation_steps in deepspeed</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2555 │   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed.backward(loss)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2556 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2557 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2558 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2559 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss.detach()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2560 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\torch\\_tensor.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">396</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 393 │   │   │   │   </span>retain_graph=retain_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 394 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 395 │   │   │   │   </span>inputs=inputs)                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 396 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=input  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 397 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 398 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">register_hook</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, hook):                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 399 │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">r\"\"\"Registers a backward hook.</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\torch\\autograd</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">\\__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">173</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">171 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>173 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">175 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\AppData\\Local\\Temp\\ipykernel_10608\\3488631611.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\shaohan.tian\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_10608\\\\3488631611.py'\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\transformers\\t\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mrainer.py\u001b[0m:\u001b[94m1543\u001b[0m in \u001b[92mtrain\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1540 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1541 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1542 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1543 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1544 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1545 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1546 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\transformers\\t\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mrainer.py\u001b[0m:\u001b[94m1791\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1788 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1789 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1790 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1791 \u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1792 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1793 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1794 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\transformers\\t\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mrainer.py\u001b[0m:\u001b[94m2557\u001b[0m in \u001b[92mtraining_step\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2554 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# loss gets scaled under gradient_accumulation_steps in deepspeed\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2555 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.deepspeed.backward(loss)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2556 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2557 \u001b[2m│   │   │   \u001b[0mloss.backward()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2558 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2559 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m loss.detach()                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2560 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\torch\\_tensor.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mpy\u001b[0m:\u001b[94m396\u001b[0m in \u001b[92mbackward\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 393 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mretain_graph=retain_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 394 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 395 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs)                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 396 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=input  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 397 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 398 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mregister_hook\u001b[0m(\u001b[96mself\u001b[0m, hook):                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 399 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\"\"Registers a backward hook.\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\shaohan.tian\\scoop\\apps\\miniconda3\\current\\envs\\automl\\lib\\site-packages\\torch\\autograd\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33m\\__init__.py\u001b[0m:\u001b[94m173\u001b[0m in \u001b[92mbackward\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m173 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m176 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Call the summary\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271872f6-9e24-4f4a-ac25-725b89af880d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d9bf5-0da5-4bf0-94ed-af923c840d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97087bff-69ed-4db5-962e-e43860e16e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad998d0d-2ee4-4f86-a9c7-f844c64f9dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl",
   "language": "python",
   "name": "automl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
